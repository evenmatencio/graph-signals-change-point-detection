{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance matrix change-point detection under graph stationarity assumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "\n",
    "Let $y = (y_1, \\ldots, y_t, \\ldots, y_T), y_t \\in \\mathbb{R}^{N}$ a graph signal lying on the nodes of the graph $G = (V, E, W)$, with $N =|V|$.\n",
    "\n",
    "We aim at detecting changes of the (spatial) covariance matrix $\\Sigma_t$ of the graph signals $y_t$. We assume that there exits an unknown set of change-points $\\Tau = (t_1, \\ldots, t_K) \\subset [1, T]$ with unknown cardinality such that the covariance matrix of the graph signals is constant over any segment $[t_k, t_{k+1}]$. We do the following hypothesis:\n",
    "\n",
    "1. the signals $y_t$ follow a multivariate Gaussian distribution with fixed covariance matrix over each segment and known mean $\\mu$, i.e:\n",
    "$$\\forall k \\in [1, K] ~ \\forall t \\in [t_k, t_{k+1}] \\quad y_t \\sim \\mathcal{N}(\\mu, \\Sigma_k)$$\n",
    "\n",
    "2. over each segment, the signals $y_t$ verify the second order wide-sense graph stationarity:\n",
    "$$\\forall k \\in [1, K] \\quad \\Sigma_k = U \\text{diag}(\\gamma_k)U^T $$\n",
    "\n",
    "where the matrix $U$ contains the eigenvectors of the graph combinatorial Laplacian matrix $L = D - W$ in its columns. \n",
    "\n",
    "The Graph Fourier Transform $\\tilde{y}$ of a signal $y$ is defined by $\\tilde{y} = U^T y $.\n",
    "\n",
    "\n",
    "Based on the above assumptions, the cost derived from the maximum log-likelihood over a segment $[a, b-1]$ writes:\n",
    "\n",
    "\\begin{align*}\n",
    "    c_s(y_{a}, \\ldots, y_{b-1}) = ~ & (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + ~ \\sum_{t=a}^{b-1} \\sum_{n=1}^N \\frac{\\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2}{\\hat{\\gamma}_{a.b}^{(n)}} = ~ (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + N(b-a)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\hat{\\mu}_{T}$ is the empirical mean of the process over $[0, T]$\n",
    "- $\\hat{\\gamma}_{a..b}$ is the (empirical) biased correlogram/periodogram of the process over $[a, b-1]$: $\\hat{\\gamma}_{a..b} = \\frac{1}{(b-a)} \\sum_{t=a}^{b-1} \\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.linalg import eigh\n",
    "from ruptures.base import BaseCost\n",
    "\n",
    "class CostGraphStatioNormal(BaseCost):\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"graph_sationary_normal_cost\"\n",
    "\n",
    "    def __init__(self, laplacian_mat) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            laplacian_mat (array): the discrete Laplacian matrix of the graph: D - W\n",
    "            where D is the diagonal matrix diag(d_i) of the node degrees and W the adjacency matrix\n",
    "        \"\"\"\n",
    "        self.graph_laplacian_mat = laplacian_mat\n",
    "        self.signal = None\n",
    "        self.gft_square_cumsum = None\n",
    "        self.gft_mean = None\n",
    "        self.min_size = laplacian_mat.shape[0]\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, signal):\n",
    "        \"\"\"Performs pre-computations for per-segment approximation cost.\n",
    "\n",
    "        NOTE: the number of dimensions of the signal and their ordering\n",
    "        must match those of the nodes of the graph.\n",
    "        The function eigh used below returns the eigenvector corresponding to \n",
    "        the ith eigenvalue in the ith column eigvect[:, i]\n",
    "\n",
    "        Args:\n",
    "            signal (array): of shape [n_samples, n_dim].\n",
    "        \"\"\"\n",
    "        self.signal = signal\n",
    "        # Computation of the GFSS\n",
    "        _, eigvects = eigh(self.graph_laplacian_mat)\n",
    "        gft =  signal @ eigvects # equals signal.dot(eigvects) = eigvects.T.dot(signal.T).T\n",
    "        self.gft_mean = np.mean(gft, axis=0)\n",
    "        # Computation of the per-segment cost utils\n",
    "        self.gft_square_cumsum = np.concatenate([np.zeros((1, signal.shape[1])), np.cumsum((gft - self.gft_mean[None, :])**2, axis=0)], axis=0)\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "\n",
    "        Returns:\n",
    "            float: segment cost\n",
    "        \"\"\"\n",
    "        if end - start < self.min_size:\n",
    "            raise ValueError(f'end - start shoud be higher than {self.min_size}')\n",
    "        sub_square_sum = self.gft_square_cumsum[end] - self.gft_square_cumsum[start]\n",
    "        return (end  - start) * np.sum(np.log(sub_square_sum / (end - start)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments: synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on the minimum distance between consecutive change points\n",
    "\n",
    "We require that the different change points $(t_1, \\ldots, t_K)$ verify:\n",
    "\n",
    "$$|t_{k+1} - t_k| >= l ~ \\forall k \\in [1, K-1] $$\n",
    "\n",
    "where $l$ can be seen as the minimum segment length. In this paragraph we give a meaningful lower bound of this parameter. Such lower bound is related to the computation of the cost functions over the segments $[a, b] \\subset [0, T]$, namely the graph stationary normal cost function $c_s$ described above and the standard normal cost function $c_n$:\n",
    "\n",
    "- $ c_n(y_{a}, \\ldots, y_{b-1}) = (b - a) \\log  \\left[ \\det \\left( \\hat \\Sigma_{a..b} \\right) \\right]$\n",
    "- $ c_s(y_{a}, \\ldots, y_{b-1}) = (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} $\n",
    "\n",
    "Based on the formula of the spectrogram $\\hat{\\gamma}_{a.b}$ given in the introduction, there is no numerical constraints for the feasibility of the computation. However, the $\\log [ \\det ( \\cdot ) ]$ function used in the formula for $c_n$ should be applied to invertible matrices $\\Sigma_{a..b}$ only. Therefore, we should focus on the conditions under which the matrix:\n",
    "\n",
    "$$ \\hat \\Sigma_{a..b} = \\frac{1}{b-a} \\sum_{t=a}^{b-1} (y_t - \\mu_T) (y_t - \\mu_T)^T \\quad \\text{ with } y_t \\sim \\mathcal{N}(\\mu, \\Sigma)  $$\n",
    "\n",
    "is invertible. Actually, such conditions have already been clearly stated in different works from Random Matrix Theory (RMT). For instance, it is shown in [[Izenman2008](#Izenman2008)] that $n \\hat \\Sigma_{a..b} \\sim \\mathcal{W}(b-a, \\Sigma)$ follows the Wishart distribution. In this framework, it is possible to study the distribution of the eigenvalues of $\\hat \\Sigma_{a..b}$ and to deduce that: \n",
    "\n",
    "$$ \\text{If } b-a > N \\text{ with } N  \\text{ the dimension of } y_t, \\text{ then } \\hat \\Sigma_{a..b} \\text{ is almost surely invertible }   $$\n",
    "\n",
    "Conversely, it is possible to show that if $ b-a < N $ (the number of observations is lower then the number of variables), the matrix $\\hat \\Sigma_{a..b}$ is nalmost surely not invertible. This can be done by considering the family the first $(N+1)$ columns of $\\hat \\Sigma_{a..b}$.\n",
    "\n",
    "Thus, the right lower-bound $l$ should be $\\boxed{l = N}$, which is consistent with statement from [[Ryan2023](#Ryan2023)].\n",
    "\n",
    "Note: with segments of length $l$, one is not guaranteed to compute good estimates of the covariance matrix, but at least such computations is almost surely admissible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change comment to minimal gap constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_git_head_short_hash() -> str:\n",
    "    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).decode('ascii').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_all_list_of_dict_into_str(data:dict):\n",
    "    new_dict = {}\n",
    "    for key, val in data.items():\n",
    "        if isinstance(val, list):\n",
    "            new_dict[key] = str(val)\n",
    "        elif isinstance(val, dict):\n",
    "            new_dict[key] = turn_all_list_of_dict_into_str(val)\n",
    "        else:\n",
    "            new_dict[key] = val\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_er_graphs(params_rng, nx_graph_seed, min_n_nodes=10, max_n_nodes=30, min_edge_p=0.15, max_edge_p=0.5):\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    edge_p = min_edge_p + (max_edge_p-min_edge_p) * params_rng.random()\n",
    "    G = nx.erdos_renyi_graph(n=n_nodes, p=edge_p, seed=nx_graph_seed)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "for _ in range(5):\n",
    "    G = generate_random_er_graphs(params_rng, graph_seed)\n",
    "    coord = nx.spring_layout(G, seed=0)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaus_signal_with_cov_diag_in_basis(n_dims, n_samples, eigvects, signal_rng, diag_cov_max=1):\n",
    "    # randomly draw diagonal coef (in the fourier space)\n",
    "    diag_coefs = diag_cov_max * signal_rng.random(n_dims)\n",
    "    diag_mat = np.diag(diag_coefs)\n",
    "    # compute the corresponding covariance matrix and signal \n",
    "    cov_mat = eigvects @ diag_mat @ eigvects.T\n",
    "    signal = signal_rng.multivariate_normal(np.zeros(n_dims), cov_mat, size=n_samples)\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "seg_length = Literal[\"large\", \"minimal\"]\n",
    "\n",
    "def get_min_size_for_hyp(n_dims, hyp:seg_length = \"minimal\"):\n",
    "    # the minimal segment length for admissible computations\n",
    "    min_size = n_dims\n",
    "    if hyp == \"large\":\n",
    "        #for segment long enough for good estimates\n",
    "        min_size = n_dims * (n_dims-1) / 2\n",
    "    return min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bkps_with_gap_constraint(n_samples, bkps_gap, bkps_rng, max_tries=10000, n_bkps_max=10):\n",
    "    # randomly pick an admissible number of bkps\n",
    "    n_bkps = bkps_rng.integers(low=1, high=min(n_bkps_max, n_samples // bkps_gap))\n",
    "    bkps = []\n",
    "    n_tries = 0\n",
    "    # select admissible randomly drawn bkps\n",
    "    while n_tries < max_tries and len(bkps) < n_bkps:\n",
    "        new_bkp = bkps_rng.integers(low=bkps_gap, high=n_samples-bkps_gap)\n",
    "        to_keep = True\n",
    "        for bkp in bkps:\n",
    "            if abs(new_bkp - bkp) < bkps_gap:\n",
    "                to_keep = False\n",
    "                break\n",
    "        if to_keep:\n",
    "            bkps.append(new_bkp)\n",
    "        n_tries+=1\n",
    "    bkps.sort()\n",
    "    return bkps + [n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruptures.metrics import precision_recall\n",
    "from ruptures.metrics import hausdorff\n",
    "\n",
    "def update_results(true_bkps, pred_bkps, results_dic, prec_rec_margin):\n",
    "    preci, recall = precision_recall(true_bkps, pred_bkps, prec_rec_margin)\n",
    "    hsdrf = hausdorff(true_bkps, pred_bkps)\n",
    "    results_dic[\"precision\"]['raw'].append(round(preci, 4))\n",
    "    results_dic[\"recall\"]['raw'].append(round(recall, 4))\n",
    "    results_dic[\"hausdorff\"]['raw'].append(hsdrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_add_stat_on_results(model_results:dict):\n",
    "    for model_res in model_results.values():\n",
    "        for metric_name, res in model_res.items():\n",
    "            model_res[metric_name]['mean'] = round(np.mean(res['raw']), ndigits=4)\n",
    "            model_res[metric_name]['std'] = round(np.std(res['raw']), ndigits=4)\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_results(results_per_models, stats, dir, comment=''):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    to_save = {\"date_time\": now, 'comment': comment}\n",
    "    to_save[\"hyper-parameters\"] = stats\n",
    "    to_save[\"results\"] = results_per_models\n",
    "    to_save = turn_all_list_of_dict_into_str(to_save)\n",
    "    path = os.path.join(dir, now)\n",
    "    with open(f\"{path}.json\", 'w+') as res_f:\n",
    "        json.dump(to_save, res_f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data verifying the hypothesis of the model\n",
    "\n",
    "In this experiment, we generate data according to the hypothesis presented in the [problem formulation](#problem-formulation) and we compare our method to the cost function for standard covariance change detection in Gaussian models (that is supposed to cover our hypothesis). More precisely, we randomly generate a graph and a corresponding multivariate Gaussian signal, undergoing a (known) random number of covariance change points. The comparison between the two methods relies on the precision, recall and Hausdorff metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_in_hyp(G:nx.Graph, signal_rng:np.random.Generator, n_samples:int=500, diag_cov_max=1, n_bkps_max=10):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = get_min_size_for_hyp(n_dims=n_dims)\n",
    "    bkps = draw_bkps_with_gap_constraint(n_samples, min_size, signal_rng, n_bkps_max)\n",
    "    # generate the signal\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 3\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G = generate_random_er_graphs(graph_rng, nx_graph_seed)\n",
    "bkps, s = generate_rd_signal_in_hyp(G, signal_rng, n_samples=1000, diag_cov_max=10)\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(mosaic='ABB', figsize=(16, 3))\n",
    "nx.draw_networkx(G, ax=axes['A'])\n",
    "for i in range(6):\n",
    "    axes['B'].plot(10*i+s[:, i])\n",
    "axes['B'].set_yticks([])\n",
    "for bkp in bkps[:-1]:\n",
    "    axes['B'].axvline(x=bkp, c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "N_EXP1 = 50\n",
    "NX_GRAPH_SEED = 1\n",
    "GRAPH_SEED = 2\n",
    "SIGNAL_SEED = 3\n",
    "PRECI_RECALL_MARGIN = 2\n",
    "MAX_N_NODES = 25\n",
    "N_SAMPLES = 1000\n",
    "MAX_N_BKPS = 10\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "\n",
    "# initialization\n",
    "exp_statistics_1 = {\"commit hash\":get_git_head_short_hash(), \"n_iter\": N_EXP1, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"metrics_margin\": PRECI_RECALL_MARGIN, \"n_samples\": N_SAMPLES, \"max_n_nodes\": MAX_N_NODES, \"max_n_bkps\": MAX_N_BKPS, \"n_nodes\": [], \"bkps\":[]}\n",
    "statio_results_1 = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results_1 = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in tqdm(range(N_EXP1), desc='Running experiment'):\n",
    "    \n",
    "    # data and ground truth generation\n",
    "    G = generate_random_er_graphs(graph_rng, NX_GRAPH_SEED, max_n_nodes=MAX_N_NODES)\n",
    "    gt_bkps, signal = generate_rd_signal_in_hyp(G, signal_rng, n_samples=N_SAMPLES, n_bkps_max=MAX_N_BKPS)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes())\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results_1[\"time\"]['raw'].append(round(t2 - t1, ndigits=3))\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results_1[\"time\"]['raw'].append(round(t2 - t1, ndigits=3))\n",
    "\n",
    "    # performances evaluation\n",
    "    update_results(gt_bkps, normal_bkps, normal_results_1, PRECI_RECALL_MARGIN)\n",
    "    update_results(gt_bkps, statio_bkps, statio_results_1, PRECI_RECALL_MARGIN)\n",
    "\n",
    "    # statistics collection\n",
    "    exp_statistics_1[\"bkps\"].append((len(gt_bkps)-1, gt_bkps[:-1]))\n",
    "    exp_statistics_1[\"n_nodes\"].append(G.number_of_nodes())\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results_1 = {\"statio normal cost\": statio_results_1, \"normal cost\": normal_results_1}\n",
    "full_results_1 = compute_and_add_stat_on_results(full_results_1)\n",
    "save_results(full_results_1, exp_statistics_1, 'results/within_hypothesis', comment='1. bkps drawn with minimal gap constraints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STATISTICS:\\t\", exp_statistics_1)\n",
    "print(\"WITH STATIO:\\t\", statio_results_1)\n",
    "print(\"WITHOUT STATIO:\\t\", normal_results_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data not verifying the hypothesis of the model\n",
    "\n",
    "In what follows, we work with signals verifying hypothesis 1 from the [the problem formulation](#problem-formulation), but not respecting the second hypothesis. More precisely, we will generate covariance matrices that are diagonalizable in a basis different from the Fourier one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_from_other_basis(G:nx.Graph, signal_rng:np.random.Generator, n_samples:int=500, diag_cov_max=1, n_bkps_max=10):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = get_min_size_for_hyp(n_dims=n_dims)\n",
    "    bkps = draw_bkps_with_gap_constraint(n_samples, min_size, signal_rng, n_bkps_max)\n",
    "    # generate another graph to compute the signal covariance matrices\n",
    "    G_for_cov = generate_random_er_graphs(signal_rng, NX_GRAPH_SEED, n_dims, n_dims, min_edge_p=0.01, max_edge_p=1)\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G_for_cov).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP2 = 50\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "\n",
    "# initialization\n",
    "exp_statistics_2 = {\"commit hash\":get_git_head_short_hash(), \"n_iter\": N_EXP2, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"metrics_margin\": PRECI_RECALL_MARGIN, \"n_samples\": N_SAMPLES, \"max_n_nodes\": MAX_N_NODES, \"max_n_bkps\": MAX_N_BKPS, \"n_nodes\": [], \"bkps\":[]}\n",
    "statio_results_2 = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results_2 = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in tqdm(range(N_EXP2), desc='Running experiment'):\n",
    "    \n",
    "    # data and ground truth generation\n",
    "    G = generate_random_er_graphs(graph_rng, NX_GRAPH_SEED, max_n_nodes=MAX_N_NODES)\n",
    "    gt_bkps, signal = generate_rd_signal_from_other_basis(G, signal_rng, n_samples=N_SAMPLES, n_bkps_max=MAX_N_BKPS)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes())\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results_2[\"time\"]['raw'].append(round(t2 - t1, ndigits=3))\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results_2[\"time\"]['raw'].append(round(t2 - t1, ndigits=3))\n",
    "\n",
    "    # performances evaluation\n",
    "    update_results(gt_bkps, normal_bkps, normal_results_2, PRECI_RECALL_MARGIN)\n",
    "    update_results(gt_bkps, statio_bkps, statio_results_2, PRECI_RECALL_MARGIN)\n",
    "\n",
    "    # statistics collection\n",
    "    exp_statistics_2[\"bkps\"].append((len(gt_bkps)-1, gt_bkps[:-1]))\n",
    "    exp_statistics_2[\"n_nodes\"].append(G.number_of_nodes())\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results_2 = {\"statio normal cost\": statio_results_2, \"normal cost\": normal_results_2}\n",
    "full_results_2 = compute_and_add_stat_on_results(full_results_2)\n",
    "save_results(full_results_2, exp_statistics_2, 'results/out_of_hypothesis', comment='1. the covariance matrix is diagonal in the Fourier basis of another random Erdos-Renyi graphs \\n2. bkps drawn with  bkps drawn with minimal gap constraints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STATISTICS:\\t\", exp_statistics_2)\n",
    "print(\"WITH STATIO:\\t\", statio_results_2)\n",
    "print(\"WITHOUT STATIO:\\t\", normal_results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data verifying the hypothesis of the models, with node dropping to simulate breakdowns\n",
    "\n",
    "In what follows, we work with signals verifying the two hypothesis from the [the problem formulation](#problem-formulation). Additionally, we will randomly select a (very) small number of nodes and simulate the breakdown of the corresponding sensors by setting the value of signal lying on this node to 0 for a random time length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_signal_to_simulate_breakdown(signal, signal_rng, n_breakdown_max):\n",
    "    # initialization\n",
    "    n_samples = signal.shape[0]\n",
    "    n_breakdown = signal_rng.integers(1, n_breakdown_max+1)\n",
    "    # randomly pick the location and time length of the breakdowns\n",
    "    breakdowns = {}\n",
    "    broken_node_ids = signal_rng.integers(0, signal.shape[1], size=(n_breakdown))\n",
    "    for node_id in broken_node_ids:\n",
    "        start = signal_rng.integers(0, n_samples-1)\n",
    "        end = signal_rng.integers(start, n_samples)\n",
    "        signal[start:end, node_id] = 0\n",
    "        breakdowns[node_id] = (start, end)\n",
    "    return signal, breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G = generate_random_er_graphs(graph_rng, nx_graph_seed)\n",
    "bkps, s = generate_rd_signal_in_hyp(G, signal_rng, n_samples=G.number_of_nodes()**2, diag_cov_max=1)\n",
    "s, breakdowns = modify_signal_to_simulate_breakdown(s, signal_rng, G.number_of_nodes()//10)\n",
    "\n",
    "print(\"The generated breakdowns are:\", breakdowns)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,3))\n",
    "for i in range(5):\n",
    "    ax.plot(10*i+s[:, i])\n",
    "for i, node_id in enumerate(breakdowns.keys()):\n",
    "    ax.plot(10*(i+5)+s[:, node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP3 = 50\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "\n",
    "# initialization\n",
    "exp_statistics_3 = {\"commit hash\":get_git_head_short_hash(), \"n_iter\": N_EXP3, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"metrics_margin\": PRECI_RECALL_MARGIN, \"n_samples\": N_SAMPLES, \"max_n_nodes\": MAX_N_NODES, \"max_n_bkps\": MAX_N_BKPS, \"n_nodes\": [], \"bkps\":[], \"breakdowns\": []}\n",
    "statio_results_3 = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results_3 = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in tqdm(range(N_EXP3), desc='Running experiment'):\n",
    "    \n",
    "    # data and ground truth generation\n",
    "    G = generate_random_er_graphs(graph_rng, NX_GRAPH_SEED, max_n_nodes=MAX_N_NODES)\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    gt_bkps, signal = generate_rd_signal_in_hyp(G, signal_rng, n_samples=N_SAMPLES, n_bkps_max=MAX_N_BKPS)\n",
    "    signal, breakdowns = modify_signal_to_simulate_breakdown(signal, signal_rng, n_nodes//10)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes())\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results_3[\"time\"]['raw'].append(round(t2 - t1, ndigits=3))\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results_3[\"time\"]['raw'].append(round(t2 - t1, ndigits=3))\n",
    "    \n",
    "\n",
    "    # performances evaluation\n",
    "    update_results(gt_bkps, normal_bkps, normal_results_3, PRECI_RECALL_MARGIN)\n",
    "    update_results(gt_bkps, statio_bkps, statio_results_3, PRECI_RECALL_MARGIN)\n",
    "\n",
    "    # statistics collection\n",
    "    exp_statistics_3[\"bkps\"].append((len(gt_bkps)-1, gt_bkps[:-1]))\n",
    "    exp_statistics_3[\"n_nodes\"].append(G.number_of_nodes())\n",
    "    exp_statistics_3[\"breakdowns\"].append(breakdowns)\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results_3 = {\"statio normal cost\": statio_results_3, \"normal cost\": normal_results_3}\n",
    "full_results_3 = compute_and_add_stat_on_results(full_results_3)\n",
    "save_results(full_results_3, exp_statistics_3, 'results/breakdowns_in_hyp', comment='1. bkps drawn with  bkps drawn with minimal gap constraints \\n2. sensor breakdowns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STATISTICS:\\t\", exp_statistics_3)\n",
    "print(\"WITH STATIO:\\t\", statio_results_3)\n",
    "print(\"WITHOUT STATIO:\\t\", normal_results_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"Izenman2008\">[Izenman2008]</a>\n",
    "Izenman Alan J. (2008). Introduction to Random-Matrix Theory [asc.ohio-state.edu]\n",
    "\n",
    "<a id=\"Ryan2023\">[Ryan2023]</a>\n",
    "Sean Ryan and Rebecca Killick. Detecting Changes in Covariance via Random Matrix Theory. Technometrics, 65(4):480â€“491, October 2023. Publisher: Taylor & Francis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
