{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance matrix change-point detection under graph stationarity assumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "\n",
    "Let $y = (y_1, \\ldots, y_t, \\ldots, y_T), y_t \\in \\mathbb{R}^{N}$ a graph signal lying on the nodes of the graph $G = (V, E, W)$, with $N =|V|$.\n",
    "\n",
    "We aim at detecting changes of the (spatial) covariance matrix $\\Sigma_t$ of the graph signals $y_t$. We assume that there exits an unknown set of change-points $\\Tau = (t_1, \\ldots, t_K) \\subset [1, T]$ with unknown cardinality such that the covariance matrix of the graph signals is constant over any segment $[t_k, t_{k+1}]$. We do the following hypothesis:\n",
    "\n",
    "1. the signals $y_t$ follow a multivariate Gaussian distribution with fixed covariance matrix over each segment and known mean $\\mu$, i.e:\n",
    "$$\\forall k \\in [1, K] ~ \\forall t \\in [t_k, t_{k+1}] \\quad y_t \\sim \\mathcal{N}(\\mu, \\Sigma_k)$$\n",
    "\n",
    "2. over each segment, the signals $y_t$ verify the second order wide-sense graph stationarity:\n",
    "$$\\forall k \\in [1, K] \\quad \\Sigma_k = U \\text{diag}(\\gamma_k)U^T $$\n",
    "\n",
    "where the matrix $U$ contains the eigenvectors of the graph combinatorial Laplacian matrix $L = D - W$ in its columns. \n",
    "\n",
    "The Graph Fourier Transform $\\tilde{y}$ of a signal $y$ is defined by $\\tilde{y} = U^T y $.\n",
    "\n",
    "\n",
    "Based on the above assumptions, the cost derived from the maximum log-likelihood over a segment $[a, b-1]$ writes:\n",
    "\n",
    "\\begin{align*}\n",
    "    c_s(y_{a}, \\ldots, y_{b-1}) = ~ & (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + ~ \\sum_{t=a}^{b-1} \\sum_{n=1}^N \\frac{\\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2}{\\hat{\\gamma}_{a.b}^{(n)}} = ~ (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + N(b-a)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\hat{\\mu}_{T}$ is the empirical mean of the process over $[0, T]$\n",
    "- $\\hat{\\gamma}_{a..b}$ is the (empirical) biased correlogram/periodogram of the process over $[a, b-1]$: $\\hat{\\gamma}_{a..b} = \\frac{1}{(b-a)} \\sum_{t=a}^{b-1} \\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.covariance import GraphicalLasso, log_likelihood\n",
    "from math import floor\n",
    "from ruptures.base import BaseCost\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from numba import njit\n",
    "from typing import List, Callable, Literal, Dict\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from matplotlib.lines import Line2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostGraphStatioNormal(BaseCost):\n",
    "\n",
    "    \"\"\"\n",
    "    DISCLAIMER: in the current model, the mean in supposed to be known and constant over different segments,\n",
    "    so we compute its estimate over the whole available samples.\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"graph_sationary_normal_cost\"\n",
    "\n",
    "    def __init__(self, laplacian_mat) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            laplacian_mat (array): the discrete Laplacian matrix of the graph: D - W\n",
    "            where D is the diagonal matrix diag(d_i) of the node degrees and W the adjacency matrix\n",
    "        \"\"\"\n",
    "        self.graph_laplacian_mat = laplacian_mat\n",
    "        self.signal = None\n",
    "        self.gft_square_cumsum = None\n",
    "        self.gft_mean = None\n",
    "        self.min_size = laplacian_mat.shape[0]\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, signal):\n",
    "        \"\"\"Performs pre-computations for per-segment approximation cost.\n",
    "\n",
    "        NOTE: the number of dimensions of the signal and their ordering\n",
    "        must match those of the nodes of the graph.\n",
    "        The function eigh used below returns the eigenvector corresponding to \n",
    "        the ith eigenvalue in the ith column eigvect[:, i]\n",
    "\n",
    "        Args:\n",
    "            signal (array): of shape [n_samples, n_dim].\n",
    "        \"\"\"\n",
    "        self.signal = signal\n",
    "        # computation of the GFSS\n",
    "        _, eigvects = eigh(self.graph_laplacian_mat)\n",
    "        gft =  signal @ eigvects # equals signal.dot(eigvects) = eigvects.T.dot(signal.T).T\n",
    "        self.gft_mean = np.mean(gft, axis=0)\n",
    "        # computation of the per-segment cost utils\n",
    "        self.gft_square_cumsum = np.concatenate([np.zeros((1, signal.shape[1])), np.cumsum((gft - self.gft_mean[None, :])**2, axis=0)], axis=0)\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "\n",
    "        Returns:\n",
    "            float: segment cost\n",
    "        \"\"\"\n",
    "        if end - start < self.min_size:\n",
    "            raise ValueError(f'end - start shoud be higher than {self.min_size}')\n",
    "        sub_square_sum = self.gft_square_cumsum[end] - self.gft_square_cumsum[start]\n",
    "        return (end  - start) * np.sum(np.log(sub_square_sum / (end - start)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setting v.0: synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on the minimum distance between consecutive change points (minimum segment length)\n",
    "\n",
    "We require that the different change points $(t_1, \\ldots, t_K)$ verify:\n",
    "\n",
    "$$|t_{k+1} - t_k| >= l ~ \\forall k \\in [1, K-1] $$\n",
    "\n",
    "where $l$ can be seen as the minimum admissible segment length. In this paragraph we give a meaningful lower bound of this parameter. Such lower bound is related to the computation of the cost functions over the segments $[a, b] \\subset [0, T]$, namely the graph stationary normal cost function $c_s$ described above and the standard normal cost function $c_n$:\n",
    "\n",
    "- $ c_n(y_{a}, \\ldots, y_{b-1}) = (b - a) \\log  \\left[ \\det \\left( \\hat \\Sigma_{a..b} \\right) \\right]$\n",
    "- $ c_s(y_{a}, \\ldots, y_{b-1}) = (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} $\n",
    "\n",
    "Based on the formula of the spectrogram $\\hat{\\gamma}_{a.b}$ given in the introduction, there is no numerical constraints for the feasibility of the computation. However, the $\\log [ \\det ( \\cdot ) ]$ function used in the formula for $c_n$ should be applied to invertible matrices $\\Sigma_{a..b}$ only. Therefore, we should focus on the conditions under which the matrix:\n",
    "\n",
    "$$ \\hat \\Sigma_{a..b} = \\frac{1}{b-a} \\sum_{t=a}^{b-1} (y_t - \\mu_T) (y_t - \\mu_T)^T \\quad \\text{ with } y_t \\sim \\mathcal{N}(\\mu, \\Sigma)  $$\n",
    "\n",
    "is invertible. Actually, such conditions have already been clearly stated in different works from Random Matrix Theory (RMT). For instance, it is shown in [[Izenman2008](#Izenman2008)] that $n \\hat \\Sigma_{a..b} \\sim \\mathcal{W}(b-a, \\Sigma)$ follows the Wishart distribution. In this framework, it is possible to study the distribution of the eigenvalues of $\\hat \\Sigma_{a..b}$ and to deduce that: \n",
    "\n",
    "$$ \\text{If } b-a > N \\text{ with } N  \\text{ the dimension of } y_t, \\text{ then } \\hat \\Sigma_{a..b} \\text{ is almost surely invertible }   $$\n",
    "\n",
    "Conversely, it is possible to show that if $ b-a < N $ (the number of observations is lower then the number of variables), the matrix $\\hat \\Sigma_{a..b}$ is nalmost surely not invertible. This can be done by considering the family the first $(N+1)$ columns of $\\hat \\Sigma_{a..b}$.\n",
    "\n",
    "Thus, the right lower-bound $l$ should be $\\boxed{l = N}$, which is consistent with statement from [[Ryan2023](#Ryan2023)].\n",
    "\n",
    "Note: with segments of length $l$, one is not guaranteed to compute good estimates of the covariance matrix, but at least such computations is almost surely admissible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data generation\n",
    "\n",
    "We design two graph generation scenarios.\n",
    "\n",
    "1. Erdős–Rényi (ER) graphs with random parameters\n",
    "\n",
    "In this scenario, we pick random number of nodes $N$ uniformly drawn in $[N_{\\min}, N_{\\max}]$, that may vary according to the experiment, and a random edge probability $p$ uniformly drawn in $[0.15, 0.5]$. The latter was chosen so that the generated graphs empirically have a realistic connectivity.\n",
    "\n",
    "2. Geographic-like graphs \n",
    "\n",
    "We pick a random number $N$ of nodes uniformly drawn in $[N_{\\min}, N_{\\max}]$. The nodes are randomly located in $[0, 1]^2$ using the uniform law $\\mathcal{U}([0, 1]^2)$. Eventually, we build the adjacency matrix of the graph by applying a threshold $\\rho$ to the distance separating the nodes. More formally, if we denote $(W_{ij})_{1 \\leq i, j \\leq N}$ the coefficients of the adjacency matrix, we explore the following two formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "    W_{ij} = \\mathbb{I} \\left( \\|p_i - p_j \\|_2 \\leq \\rho \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    W_{ij} = \\frac{\\rho}{\\|p_i - p_j \\|_2}  \\mathbb{I} \\left( \\|p_i - p_j \\|_2 \\leq \\rho \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $p_i$ denotes the 2D coordinates of node $i$. In the above formulas, the value of $\\rho$ is chosen empirically so that the resulting graphs visually exhibit connectivity patterns that are consistent to what one may expect when using a graph structure for signal analysis. More precisely, we use the following values:\n",
    "\n",
    "- for $N$ drawn in $[10, 50]$: $~$ $\\rho = 0.3$\n",
    "- for $N$ drawn in $[80, 110]$: $\\rho = 0.2$\n",
    "\n",
    "This scenario simulates 2D geographic graphs and is more likely to match realistic use-cases.\n",
    "\n",
    "\n",
    "### Signal generation\n",
    "\n",
    "Let $G$ be a graph randomly generated using one of the above scenarios. We recall that the laplacian matrix $L$ of the graph verifies $L = U \\Lambda U^T$, where the columns of $U$ are the eigenvectors of $L$. We now describe how to generate a signal $(y_t)_{1 \\leq t \\leq T}$ that verifies the hypothesis presented in the [problem formulation](#problem-formulation).\n",
    "\n",
    "We first pick an admissible number of change points $K$ depending on the signal length $T=1000$ and the minimum segment length $l = N$ by sampling $~  \\mathcal{U}([1, \\min(\\frac{T}{10}, \\frac{T}{l})])$. The change points $(t_k)_{1 \\leq k \\leq K}$ are uniformly drawn in $[l, T-l]$, by checking that a newly selected change point does not break the minimum segment length criteria. Therefore, it may happen that after a limit number of iterations, not $K$ change points were drawn, but we still use the same notations (SHOULD BE CHANGED).\n",
    "\n",
    "Finally, we apply the following formula to generate the signal $(y_t)_{1 \\leq t \\leq T}$:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\forall ~ k \\in [1, K-1] ~ \\forall  t \\in [t_k, t_{k+1}] \\quad  y_t \\sim \\mathcal{N}_N(0, \\Sigma_k) \\quad \\text{ with } \\quad \\Sigma_k = U \\text{diag}(\\gamma_k) U^T ~  \\text{ and } ~ \\gamma_k \\sim \\mathcal{U}niform_N([0, 1]) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_head_short_hash() -> str:\n",
    "    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).decode('ascii').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_all_list_of_dict_into_str(data:dict):\n",
    "    new_dict = {}\n",
    "    for key, val in data.items():\n",
    "        if isinstance(val, list):\n",
    "            new_dict[key] = str(val)\n",
    "        elif isinstance(val, dict):\n",
    "            new_dict[key] = turn_all_list_of_dict_into_str(val)\n",
    "        else:\n",
    "            new_dict[key] = val\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_str_of_list_into_list_of_int(list_str):\n",
    "    assert list_str[0] == '[' and list_str[-1] == ']'\n",
    "    list_of_str = list_str[1:-1].split(',')\n",
    "    return [int(val_str) for val_str in list_of_str]\n",
    "\n",
    "def turn_str_of_list_into_list_of_float(list_str):\n",
    "    assert list_str[0] == '[' and list_str[-1] == ']'\n",
    "    list_of_str = list_str[1:-1].split(',')\n",
    "    return [float(val_str) for val_str in list_of_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_and_dump_json(parent_dir, name, data, indent=None):\n",
    "    if os.path.exists(os.path.join(parent_dir, name)):\n",
    "        raise FileExistsError\n",
    "    if not os.path.exists(parent_dir):\n",
    "        Path(parent_dir).mkdir(parents=True, exist_ok=False)\n",
    "    with open(os.path.join(parent_dir, name), 'w+') as f:\n",
    "        json.dump(data, f, indent=indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(path):\n",
    "    with open(path, 'r+') as f:\n",
    "        content = json.load(f)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_er_graphs(params_rng, nx_graph_seed, max_n_nodes, min_n_nodes=10, min_edge_p=0.15, max_edge_p=0.5):\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    edge_p = min_edge_p + (max_edge_p-min_edge_p) * params_rng.random()\n",
    "    G = nx.erdos_renyi_graph(n=n_nodes, p=edge_p, seed=nx_graph_seed)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "for _ in range(5):\n",
    "    G = generate_random_er_graphs(params_rng, graph_seed, max_n_nodes=30)\n",
    "    coord = nx.spring_layout(G, seed=0)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_ba_graphs(params_rng, nx_graph_seed, min_n_nodes, max_n_nodes):\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    m = int(params_rng.normal(loc=n_nodes//10, scale=1))\n",
    "    G = nx.barabasi_albert_graph(n=n_nodes, m=m, seed=nx_graph_seed)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "for _ in range(5):\n",
    "    G = generate_random_ba_graphs(params_rng, graph_seed, min_n_nodes=10, max_n_nodes=30)\n",
    "    coord = nx.spring_layout(G, seed=0)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_weighted_geographic_graph(params_rng, min_n_nodes, max_n_nodes, dist_threshold):\n",
    "    # generation of the nodes coordinates\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "    # computation of the adjacency matrix based on distance threshold \n",
    "    dist_mat_condensed = pdist(nodes_coord, metric='euclidean')\n",
    "    adj_mat = squareform((dist_threshold/dist_mat_condensed)*(dist_mat_condensed < dist_threshold).astype(int))\n",
    "    G = nx.Graph(adj_mat)\n",
    "    return G, nodes_coord\n",
    "\n",
    "def generate_random_geographic_graph(params_rng, min_n_nodes, max_n_nodes, dist_threshold):\n",
    "    # generation of the nodes coordinates\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "    # computation of the adjacency matrix based on distance threshold \n",
    "    dist_mat_condensed = pdist(nodes_coord, metric='euclidean')\n",
    "    adj_mat = squareform((dist_mat_condensed < dist_threshold).astype(int))\n",
    "    G = nx.Graph(adj_mat)\n",
    "    return G, nodes_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(5*4, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "for _ in range(5):\n",
    "    G, coord = generate_random_geographic_graph(params_rng, min_n_nodes=80, max_n_nodes=110, dist_threshold=0.20)\n",
    "    coord_dic = {i: coord[i, :] for i in range(coord.shape[0])}\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord_dic, node_size=50, ax=axes[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "\n",
    "n_nodes = params_rng.integers(low=80, high=110+1)\n",
    "nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "# computation of the adjacency matrix based on distance threshold \n",
    "dist_mat_condensed = pdist(nodes_coord, metric='euclidean')\n",
    "# --> to check that there is no 0 value in the condensed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaus_signal_with_cov_diag_in_basis(n_dims, n_samples, basis, signal_rng, diag_cov_max=1):\n",
    "    # randomly draw diagonal coef (in the fourier space)\n",
    "    diag_coefs = diag_cov_max * signal_rng.random(n_dims)\n",
    "    diag_mat = np.diag(diag_coefs)\n",
    "    # compute the corresponding covariance matrix and signal \n",
    "    cov_mat = basis @ diag_mat @ basis.T\n",
    "    signal = signal_rng.multivariate_normal(np.zeros(n_dims), cov_mat, size=n_samples)\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_length = Literal[\"large\", \"minimal\"]\n",
    "\n",
    "def get_min_size_for_hyp(n_dims, hyp:seg_length):\n",
    "    # the minimal segment length for admissible computations\n",
    "    min_size = n_dims\n",
    "    if hyp == \"large\":\n",
    "        #for segment long enough for good estimates\n",
    "        min_size = n_dims * (n_dims-1) / 2\n",
    "    return min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bkps_with_gap_constraint(n_samples, bkps_gap, bkps_rng, n_bkps_max, max_tries=10000):\n",
    "    # randomly pick an admissible number of bkps\n",
    "    n_bkps = bkps_rng.integers(low=1, high=min(n_bkps_max, n_samples // bkps_gap))\n",
    "    bkps = []\n",
    "    n_tries = 0\n",
    "    # select admissible randomly drawn bkps\n",
    "    while n_tries < max_tries and len(bkps) < n_bkps:\n",
    "        new_bkp = bkps_rng.integers(low=bkps_gap, high=n_samples-bkps_gap)\n",
    "        to_keep = True\n",
    "        for bkp in bkps:\n",
    "            if abs(new_bkp - bkp) < bkps_gap:\n",
    "                to_keep = False\n",
    "                break\n",
    "        if to_keep:\n",
    "            bkps.append(new_bkp)\n",
    "        n_tries+=1\n",
    "    bkps.sort()\n",
    "    return bkps + [n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_fixed_nb_bkps_with_gap_constraint(n_samples, bkps_gap, bkps_rng, max_tries=10000):\n",
    "    # randomly pick an admissible number of bkps\n",
    "    n_bkps = bkps_rng.integers(low=n_samples // bkps_gap - 1, high= n_samples // bkps_gap)\n",
    "    bkps = []\n",
    "    n_tries = 0\n",
    "    # select admissible randomly drawn bkps\n",
    "    while n_tries < max_tries and len(bkps) < n_bkps:\n",
    "        new_bkp = bkps_rng.integers(low=bkps_gap, high=n_samples-bkps_gap)\n",
    "        to_keep = True\n",
    "        for bkp in bkps:\n",
    "            if abs(new_bkp - bkp) < bkps_gap:\n",
    "                to_keep = False\n",
    "                break\n",
    "        if to_keep:\n",
    "            bkps.append(new_bkp)\n",
    "        n_tries+=1\n",
    "    bkps.sort()\n",
    "    return bkps + [n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data writting and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(G:nx.Graph, signal:np.ndarray, bkps:List[int], dir:str):\n",
    "    # create subfolder\n",
    "    Path(dir).mkdir(parents=True, exist_ok=False)\n",
    "    # save graph\n",
    "    adj_mat = nx.to_numpy_array(G)\n",
    "    with open(f'{dir}/graph_adj_mat.npy', 'wb+') as nx_f:\n",
    "        np.save(nx_f, adj_mat, allow_pickle=False)\n",
    "    # save signal\n",
    "    with open(f'{dir}/signal.npy', 'wb+') as np_f:\n",
    "        np.save(np_f, signal, allow_pickle=False)\n",
    "    # save bkps\n",
    "    bkps_str = [int(bkp) for bkp in bkps]\n",
    "    create_parent_and_dump_json(dir, \"bkps.json\", bkps_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str):\n",
    "    adj_mat = np.load(f\"{path}/graph_adj_mat.npy\", allow_pickle=False)\n",
    "    G = nx.from_numpy_array(adj_mat)\n",
    "    signal = np.load(f\"{path}/signal.npy\", allow_pickle=False)\n",
    "    bkps = open_json(f\"{path}/bkps.json\")\n",
    "    return G, signal, bkps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results computation and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruptures.metrics import precision_recall\n",
    "from ruptures.metrics import hausdorff\n",
    "\n",
    "def compute_f1_score(preci, recall):\n",
    "    if preci + recall > 0:\n",
    "        return 2 * (preci * recall) / (preci + recall)\n",
    "    return 0\n",
    "\n",
    "def compute_and_update_metrics(true_bkps, pred_bkps, metrics_dic, prec_rec_margin):\n",
    "    preci, recall = precision_recall(true_bkps, pred_bkps, prec_rec_margin)\n",
    "    f1_score = compute_f1_score(preci, recall)\n",
    "    hsdrf = hausdorff(true_bkps, pred_bkps)\n",
    "    metrics_dic[\"precision\"]['raw'].append(round(preci, 4))\n",
    "    metrics_dic[\"recall\"]['raw'].append(round(recall, 4))\n",
    "    metrics_dic[\"f1_score\"]['raw'].append(round(f1_score, 4))\n",
    "    metrics_dic[\"hausdorff\"]['raw'].append(int(hsdrf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_add_stat_on_metrics(model_metrics:dict):\n",
    "    for model_res in model_metrics.values():\n",
    "        for metric_name, res in model_res.items():\n",
    "            model_res[metric_name]['mean'] = round(np.mean(res['raw']), ndigits=4)\n",
    "            model_res[metric_name]['std'] = round(np.std(res['raw']), ndigits=4)\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(metrics_per_models, stats, dir, comment=''):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    to_save = {\"date_time\": now, 'comment': comment}\n",
    "    to_save[\"hyper-parameters\"] = stats\n",
    "    to_save[\"results\"] = metrics_per_models\n",
    "    to_save = turn_all_list_of_dict_into_str(to_save)\n",
    "    create_parent_and_dump_json(dir, now + '.json', to_save, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin):\n",
    "    # initialization\n",
    "    stat_per_metric_per_cost_func = {}\n",
    "    for cost_func in cost_func_keys:\n",
    "        stat_per_metric_per_cost_func[cost_func] = {}\n",
    "        for metric in metrics_keys:\n",
    "            stat_per_metric_per_cost_func[cost_func][metric] = {\"mean\": [], \"std\": [], \"raw\": []}\n",
    "    # parsing metrics file to store results adequately\n",
    "    for folder_name in res_folder_list:\n",
    "        file_metric = open_json(os.path.join(folder_name, f\"metrics_{preci_margin}.json\"))\n",
    "        for cost_func in cost_func_keys:\n",
    "            for metric in metrics_keys:\n",
    "                mean = file_metric[cost_func][metric][\"mean\"]\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"mean\"].append(mean)\n",
    "                std = file_metric[cost_func][metric][\"std\"]\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"std\"].append(std)\n",
    "                raw_str = file_metric[cost_func][metric][\"raw\"]\n",
    "                raw = turn_str_of_list_into_list_of_float(raw_str)\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"raw\"].append(raw)\n",
    "    return stat_per_metric_per_cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_raw_scatter_to_plot(x, res_dic, met_name, cost_func, ax):\n",
    "    # scatter plot based on the raw values\n",
    "    raw = np.array(res_dic[cost_func][met_name][\"raw\"])\n",
    "    raw.flatten()\n",
    "    x_scat_val = []\n",
    "    for x_id in range(len(x)):\n",
    "        x_scat_val = x_scat_val + [x[x_id]] * len(raw[x_id])\n",
    "    ax.scatter(x=x_scat_val, y=raw, alpha=0.3, c='k', s=10)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.4\n",
    "error_kw = {\"capsize\": 5, \"elinewidth\": 0.8, \"capthick\": 2}\n",
    "\n",
    "def plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, met_name, ax, shift= 0.1, to_label=False):\n",
    "    # plot_x_coord = np.linspace(0, 1, num=(len(abscissa)+1))[:-1]\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        label = None\n",
    "        if to_label:\n",
    "            label = cost_func\n",
    "        color = colors_per_cost_func[cost_func]\n",
    "        # bar plot based on the statistics\n",
    "        x = np.asarray(abscissa) + shift*i\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.bar(x = x, height=y, yerr=y_err, label=label, width=bar_width, error_kw=error_kw, color=color)\n",
    "        # scatter plot based on the raw values\n",
    "        add_raw_scatter_to_plot(x, res_dic, met_name, cost_func, ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.02\n",
    "error_kw = {\"capsize\": 5, \"elinewidth\": 0.8, \"capthick\": 2}\n",
    "\n",
    "def plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, abscissa, plot_x_coord, colors_per_cost_func, met_name, ax, shift= 0.1, to_label=False):\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        label = None\n",
    "        if to_label:\n",
    "            label = cost_func\n",
    "        color = colors_per_cost_func[cost_func]\n",
    "        # bar plot based on the statistics\n",
    "        x = [plot_x_coord + shift*i]\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.bar(x = x, height=y, yerr=y_err, label=label, width=bar_width, error_kw=error_kw, color=color)\n",
    "        # scatter plot based on the raw values\n",
    "        add_raw_scatter_to_plot(x, res_dic, met_name, cost_func, ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_errorbar_manually(abscissa, y, yerr, marker, linestyle, linewidth, markersize, color, ax):\n",
    "    # create top and bottom lim\n",
    "    y_err_top = [y_pos + yerr_val for y_pos, yerr_val in zip(y, yerr)]\n",
    "    y_err_bottom = [y_pos - yerr_val for y_pos, yerr_val in zip(y, yerr)]\n",
    "    # add the top and bottom markers\n",
    "    ax.scatter(abscissa, y_err_top, s=markersize, c=color, marker=marker)\n",
    "    ax.scatter(abscissa, y_err_bottom, s=markersize, c=color, marker=marker)\n",
    "    # add the vertical lines\n",
    "    ax.vlines(x=abscissa, ymin=y_err_bottom, ymax=y_err_top, color=color, linewidth=linewidth, linestyle=linestyle)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_errorbar(abscissa_pos, cost_func_keys, color, markers, linestyles, met_name, res_dic, ax):\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.plot(abscissa_pos, y, c=color, marker=markers[i], markersize=7, linewidth=2, linestyle=linestyles[i])\n",
    "        add_errorbar_manually(abscissa_pos, y, y_err, marker=markers[i], linestyle=linestyles[i], linewidth=1, markersize=20, color=color, ax=ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data verifying the hypothesis of the model\n",
    "\n",
    "In this experiment, we generate data according to the hypothesis presented in the [problem formulation](#problem-formulation) and we compare our method to the cost function for standard covariance change detection in Gaussian models (that is supposed to cover our hypothesis). More precisely, we randomly generate a graph and a corresponding multivariate Gaussian signal, undergoing a (known) random number of covariance change points. The comparison between the two methods relies on the precision / recall and Hausdorff metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_in_hyp_with_max_tries(G:nx.Graph, signal_rng:np.random.Generator, n_bkps_max, hyp:seg_length, n_samples:int, diag_cov_max=1):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = get_min_size_for_hyp(n_dims=n_dims, hyp=hyp)\n",
    "    bkps = draw_bkps_with_gap_constraint(n_samples, min_size, signal_rng, n_bkps_max)\n",
    "    # generate the signal\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 3\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "# G = generate_random_er_graphs(graph_rng, nx_graph_seed, max_n_nodes=30)\n",
    "G, _ = generate_random_weighted_geographic_graph(graph_rng, min_n_nodes=40, max_n_nodes=50, dist_threshold=0.30)\n",
    "bkps, s = generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_samples=1000, diag_cov_max=10, hyp='min', n_bkps_max=10)\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(mosaic='ABB', figsize=(14, 3))\n",
    "nx.draw_networkx(G, ax=axes['A'])\n",
    "for i in range(6):\n",
    "    axes['B'].plot(10*i+s[:, i])\n",
    "axes['B'].set_yticks([])\n",
    "for bkp in bkps[:-1]:\n",
    "    axes['B'].axvline(x=bkp, c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data not verifying the hypothesis of the model at all\n",
    "\n",
    "In what follows, we work with signals verifying hypothesis 1 from the [the problem formulation](#problem-formulation), but not respecting the second hypothesis. More precisely, we will generate covariance matrices that are diagonalizable in a basis different from the Fourier basis $U$. To do so, we generate a graph $G'$  different from the graph $G$ used to compute the cost function. Then, we apply the same process and formula as previously described to generate the signal but using the eigenvectors $U'$ of the laplacian matrix $L'$ of $G'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_from_other_basis(G:nx.Graph, signal_rng:np.random.Generator, hyp:seg_length, n_bkps_max, n_samples, diag_cov_max=1):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = get_min_size_for_hyp(n_dims=n_dims, hyp=hyp)\n",
    "    bkps = draw_bkps_with_gap_constraint(n_samples, min_size, signal_rng, n_bkps_max)\n",
    "    # generate another graph to compute the signal covariance matrices\n",
    "    G_for_cov = generate_random_er_graphs(signal_rng, NX_GRAPH_SEED, max_n_nodes=n_dims, min_n_nodes=n_dims, min_edge_p=0.01, max_edge_p=1)\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G_for_cov).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data verifying the hypothesis of the models, with node dropping to simulate breakdowns\n",
    "\n",
    "In what follows, we work with signals verifying the two hypothesis from the [the problem formulation](#problem-formulation). Additionally, we will randomly select a (very) small number of nodes and simulate the breakdown of the corresponding sensors by setting the value of the signal lying on this node to 0 for a random time length. \n",
    "\n",
    "More formally, let denote $\\eta_{max}$ the hyper-parameter corresponding to the maximal proportion of nodes that undergo a breakdown. We denote $N^{max}_{broken} = \\lfloor \\eta_{max} * N \\rfloor$. For each signal $(y_t)_{1 \\leq t \\leq T} \\in \\mathbb{R}^{T \\times N}$ , we draw $N_{broken}$ number of nodes undergoing a breakdown in $ ~  \\mathcal{U}niform([0, N^{max}_{broken}])$. Then, for a node $i$ undergoing a breakdown we apply:\n",
    "\n",
    "\\begin{equation}\n",
    "    t_{start} ~ \\sim ~ \\mathcal{U}niform([0, T-1]), ~ t_{end} ~ \\sim ~ \\mathcal{U}niform([t_{start}, T]) \\qquad \\forall ~  t \\in [t_{start}, t_{end}] ~ y_t^i = 0\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, by increasing the value of $\\eta_{max}$ we evaluate the robustness of our cost function with respect to brutal, isolated and uncorrelated mean changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_signal_to_simulate_breakdown(signal, signal_rng, n_breakdown_max):\n",
    "    # initialization\n",
    "    n_samples = signal.shape[0]\n",
    "    n_breakdown = signal_rng.integers(1, n_breakdown_max+1)\n",
    "    # randomly pick the location and time length of the breakdowns\n",
    "    breakdowns = {}\n",
    "    broken_node_ids = signal_rng.integers(0, signal.shape[1], size=(n_breakdown))\n",
    "    for node_id in broken_node_ids:\n",
    "        start = int(signal_rng.integers(0, n_samples-1))\n",
    "        end = int(signal_rng.integers(start, n_samples))\n",
    "        signal[start:end, node_id] = 0\n",
    "        breakdowns[int(node_id)] = (start, end)\n",
    "    return signal, breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G = generate_random_er_graphs(graph_rng, nx_graph_seed, max_n_nodes=30)\n",
    "bkps, s = generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "s, breakdowns = modify_signal_to_simulate_breakdown(s, signal_rng, n_breakdown_max=G.number_of_nodes()//10)\n",
    "\n",
    "print(\"The generated breakdowns are:\", breakdowns)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,3))\n",
    "for i in range(5):\n",
    "    ax.plot(10*i+s[:, i])\n",
    "for i, node_id in enumerate(breakdowns.keys()):\n",
    "    ax.plot(10*(i+5)+s[:, node_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Robustness with respect to (spatially and temporaly) independent additive white noise\n",
    "\n",
    "In this experiment, we keep using signals that verify our two hypothesis. Though we add a temporally independent white noise with scalar covariance matrix to such signal. More formally, we apply the change point detection algorithm to the signal $(y'_t)_{1 \\leq t \\leq T}$ defined by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\forall ~  t \\in [0, T] \\quad y'_t = y_t + e_t \\quad \\text{ with } \\quad e_t \\sim \\mathcal{N}_N(0, \\sigma)\n",
    "\\end{equation}\n",
    "\n",
    "We evaluate the performance of our cost function against increasing value of $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_diagonal_white_noise(signal_rng:np.random.Generator, signal, sigma):\n",
    "    n_samples , n_dims = signal.shape\n",
    "    cov_mat = sigma * np.eye(n_dims)\n",
    "    white_noise = signal_rng.multivariate_normal(np.zeros(n_dims), cov_mat, size=n_samples)\n",
    "    return signal + white_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G, _ = generate_random_weighted_geographic_graph(graph_rng, min_n_nodes=10, max_n_nodes=30, dist_threshold=0.3)\n",
    "bkps, s = generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "s_noise = add_diagonal_white_noise(signal_rng, s, sigma=3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,2))\n",
    "for i in range(5):\n",
    "    axes[0].plot(10*i+s[:, i])\n",
    "for i in range(5):\n",
    "    axes[1].plot(10*i+s_noise[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Robustness with respect to (temporaly) independent additive plain white noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Robustness with respect to the graph structure: modification of the connectivity\n",
    "\n",
    "In this experiment, we do not generate the signal $(y_t)_{1 \\leq t \\leq T}$ with the graph $G$ that is used to compute the cost function. Instead, we rather utilize the laplacian matrix $L_{noisy}$ of a noisy version $G_{noisy}$ of the original graph $G$.\n",
    "\n",
    "Let denote $M = |E|$ the number of edges in $G$ and $\\eta_{edge}$ the proportion of edges that we modify. We randomly remove $M_{remove} = \\lfloor M * \\eta_{edge} / 2 \\rfloor$ from the set E and we randomly add $M_{add} = \\lfloor M * \\eta_{edge} / 2 \\rfloor$ to $E$. In both cases, we select the edges randomly, but we always check that the resulting noisy graph $G_{noisy}$ still has the same number of nodes as $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_graph_connectivity_from_binary_adj_mat_2(G:nx.Graph, edge_prop, graph_rng:np.random.Generator):\n",
    "    # initialization\n",
    "    n_edges = G.number_of_edges()\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    n_modif_edges = int(n_edges * edge_prop)\n",
    "    adj_mat = nx.adjacency_matrix(G).toarray()\n",
    "    # retrieving the actual graph edges\n",
    "    directed_edges = [(i, j) for i, j in zip(np.nonzero(adj_mat)[0], np.nonzero(adj_mat)[1])]\n",
    "    non_directed_edges = set([(min(e), max(e)) for e in directed_edges])\n",
    "    # listing all possible edges to select those to be add\n",
    "    all_possible_edges_list = []\n",
    "    for i in range(n_nodes-1):\n",
    "        for j in range(i+1, n_nodes):\n",
    "            all_possible_edges_list.append((i, j))\n",
    "    all_possible_edges_set = set(all_possible_edges_list)\n",
    "    # removing some random edges\n",
    "    n_edges_to_remove = n_modif_edges // 2\n",
    "    edges_id_to_remove = graph_rng.choice(len(non_directed_edges), min(n_edges_to_remove, len(non_directed_edges)), replace=False)\n",
    "    for edge_id in edges_id_to_remove:\n",
    "        edge_to_remove = list(non_directed_edges)[edge_id]\n",
    "        adj_mat[edge_to_remove[0], edge_to_remove[1]] = 0\n",
    "        adj_mat[edge_to_remove[1], edge_to_remove[0]] = 0\n",
    "    # adding some edges\n",
    "    possibles_edges_to_add = all_possible_edges_set.difference(non_directed_edges)\n",
    "    possibles_edges_to_add_list = list(possibles_edges_to_add)\n",
    "    n_edges_to_add = n_edges_to_remove\n",
    "    new_edges_ids = graph_rng.choice(len(possibles_edges_to_add), min(n_edges_to_add, len(possibles_edges_to_add)), replace=False)\n",
    "    new_edges_list = [possibles_edges_to_add_list[edge_id] for edge_id in new_edges_ids]\n",
    "    for new_edge in new_edges_list:\n",
    "        adj_mat[new_edge[1], new_edge[0]] = 1\n",
    "        adj_mat[new_edge[0], new_edge[1]] = 1\n",
    "    new_G = nx.from_numpy_array(adj_mat)\n",
    "    return new_G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed+3)\n",
    "\n",
    "G, coord = generate_random_weighted_geographic_graph(graph_rng, min_n_nodes=19, max_n_nodes=20, dist_threshold=0.5)\n",
    "# G_modif = modify_graph_connectivity(G, 0.05, graph_rng)\n",
    "G_modif = modify_graph_connectivity_from_binary_adj_mat_2(G, 1, graph_rng)\n",
    "bkps, s = generate_rd_signal_in_hyp_with_max_tries(G_modif, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "\n",
    "original_edges = set([(min(e), max(e)) for e in G.edges()])\n",
    "new_edges = set([(min(e), max(e)) for e in G_modif.edges()])\n",
    "\n",
    "\n",
    "added = new_edges.difference(original_edges)\n",
    "withdrawn  = original_edges.difference(new_edges)\n",
    "same = new_edges.intersection(original_edges)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# original graph\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=same, edge_color='k', ax=axes[0])\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=withdrawn, edge_color='r', width=3, ax=axes[0])\n",
    "nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[0], edgelist = [])\n",
    "# modified graph\n",
    "nx.draw_networkx_edges(G_modif, pos=coord, edgelist=same, edge_color='k', ax=axes[1])\n",
    "nx.draw_networkx_edges(G_modif, pos=coord, edgelist=added, width=3, edge_color='g', ax=axes[1])\n",
    "nx.draw_networkx(G_modif, with_labels=False, pos=coord, node_size=50, ax=axes[1], edgelist = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed+3)\n",
    "\n",
    "# G, coord = generate_random_weighted_geographic_graph(graph_rng, min_n_nodes=19, max_n_nodes=20, dist_threshold=0.5)\n",
    "# # G_modif = modify_graph_connectivity(G, 0.05, graph_rng)\n",
    "# G_modif = modify_graph_connectivity_from_binary_adj_mat_2(G, 1, graph_rng)\n",
    "# bkps, s = generate_rd_signal_in_hyp_with_max_tries(G_modif, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "\n",
    "exp_id = 1\n",
    "\n",
    "ori_path = \"data_1/graphs/clean_ER_with_bandwidth/ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "adj_mat = np.load(f\"{ori_path}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "G = nx.from_numpy_array(adj_mat)\n",
    "coord = nx.spring_layout(G, seed=0)\n",
    "modif_path = \"data_1/graphs/ER_with_bd_edge_changed/ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.2\"\n",
    "modif_adj_mat = np.load(f\"{modif_path}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "modif_G = nx.from_numpy_array(modif_adj_mat)\n",
    "\n",
    "\n",
    "original_edges = set([(min(e), max(e)) for e in G.edges()])\n",
    "new_edges = set([(min(e), max(e)) for e in modif_G.edges()])\n",
    "\n",
    "\n",
    "added = new_edges.difference(original_edges)\n",
    "withdrawn  = original_edges.difference(new_edges)\n",
    "same = new_edges.intersection(original_edges)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# original graph\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=same, edge_color='k', ax=axes[0])\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=withdrawn, edge_color='r', width=3, ax=axes[0])\n",
    "nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[0], edgelist = [])\n",
    "# modified graph\n",
    "nx.draw_networkx_edges(modif_G, pos=coord, edgelist=same, edge_color='k', ax=axes[1])\n",
    "nx.draw_networkx_edges(modif_G, pos=coord, edgelist=added, width=3, edge_color='g', ax=axes[1])\n",
    "nx.draw_networkx(modif_G, with_labels=False, pos=coord, node_size=50, ax=axes[1], edgelist = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setting v.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data writing and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph(G:nx.Graph, path):\n",
    "    '''note: nx.to_numpy_array(G) returns the same object as nx.adjacency_matrix(G).toarray() '''\n",
    "    if os.path.exists(path):\n",
    "        raise FileExistsError\n",
    "    # save graph\n",
    "    adj_mat = nx.to_numpy_array(G)\n",
    "    with open(path, 'wb+') as nx_f:\n",
    "        np.save(nx_f, adj_mat, allow_pickle=False)\n",
    "\n",
    "def save_signal_and_bkps(signal:np.ndarray, bkps:List[int], dir, name):\n",
    "    path = os.path.join(dir, name)\n",
    "    if os.path.exists(f\"{path}_signal.npy\"):\n",
    "        raise FileExistsError\n",
    "    # save signal\n",
    "    with open(f\"{path}_signal.npy\", 'wb+') as np_f:\n",
    "        np.save(np_f, signal, allow_pickle=False)\n",
    "    # save bkps\n",
    "    bkps_str = [int(bkp) for bkp in bkps]\n",
    "    with open(f\"{path}_bkps.json\", 'w+') as f:\n",
    "        json.dump(bkps_str, f, indent=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_1(graph_path: str, signal_path: str, exp_id: int):\n",
    "    adj_mat = np.load(f\"{graph_path}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "    G = nx.from_numpy_array(adj_mat)\n",
    "    signal = np.load(f\"{signal_path}/{exp_id}_signal.npy\", allow_pickle=False)\n",
    "    bkps = open_json(f\"{signal_path}/{exp_id}_bkps.json\")\n",
    "    return G, signal, bkps\n",
    "\n",
    "def load_data(graph_path: str, signal_path: str, exp_id: int, hyp:seg_length):\n",
    "    G, signal, bkps = read_data_1(graph_path, signal_path, exp_id)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes(), hyp=hyp)\n",
    "    return G, signal, bkps, min_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation and vizualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph generation\n",
    "\n",
    "DISCLAIMER: ALL ER GRAPHS WHERE GENERATED USING THE NX SEED SET TO EXP_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_another_graph(graph_path: str, previous_exp_id: int, max_id: int, graph_rng: np.random.Generator):\n",
    "    new_exp_id = previous_exp_id\n",
    "    while new_exp_id == previous_exp_id:\n",
    "        new_exp_id = graph_rng.integers(low=0, high=max_id)\n",
    "    new_exp_id = str(new_exp_id)\n",
    "    new_adj_mat = np.load(f\"{graph_path}/{new_exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "    new_G = nx.from_numpy_array(new_adj_mat)\n",
    "    return new_G, new_exp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_er_graphs_fixed_nodes_nb(params_rng, nx_graph_seed, n_nodes, target_deg, bandwidth_coef):\n",
    "    min_edge_p = (1- bandwidth_coef) * target_deg / (n_nodes -1)\n",
    "    max_edge_p = (1+ bandwidth_coef) * target_deg / (n_nodes -1)\n",
    "    edge_p = min_edge_p + (max_edge_p-min_edge_p) * params_rng.random()\n",
    "    G = nx.erdos_renyi_graph(n=n_nodes, p=edge_p, seed=nx_graph_seed)\n",
    "    params = {\"edge_prob\": edge_p}\n",
    "    return G, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "n_nodes = 40\n",
    "target_deg = 10\n",
    "bandwidth = 0.4\n",
    "\n",
    "av_deg = []\n",
    "for _ in range(5):\n",
    "    G, a = generate_random_er_graphs_fixed_nodes_nb(params_rng, _, n_nodes, target_deg, bandwidth)\n",
    "    coord = nx.spring_layout(G, seed=0)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])\n",
    "    degrees = [val for (node, val) in G.degree()]\n",
    "    av_deg.append(sum(degrees)/n_nodes)\n",
    "print(av_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_geographic_graph_with_gauss_kernel(params_rng, n_nodes, target_degree):\n",
    "    # generation of the nodes coordinates\n",
    "    nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "    # computation of the threshold for the exponential adjacency matrix\n",
    "    dist_mat_condensed = pdist(nodes_coord, metric='euclidean')\n",
    "    sigma = np.median(dist_mat_condensed)  # median heuristic\n",
    "    expsim_condensed = np.exp(-(dist_mat_condensed**2) / (sigma**2))\n",
    "    # ordering the values to find the right threshold\n",
    "    ordered_exp_sim = np.sort(expsim_condensed)\n",
    "    n_edge_for_target_deg = target_degree*n_nodes//2\n",
    "    threshold = ordered_exp_sim[-n_edge_for_target_deg+1]\n",
    "    # creating the corresponding adjacency matrix and graph\n",
    "    adj_mat_condensed = np.where(expsim_condensed > threshold, expsim_condensed, 0.0)\n",
    "    adj_mat = squareform(adj_mat_condensed)\n",
    "    G = nx.Graph(adj_mat)\n",
    "    return G, nodes_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "n_nodes = 80\n",
    "target_deg = 10\n",
    "av_deg = []\n",
    "to_plot_coords = []\n",
    "for _ in range(5):\n",
    "    G, coord = generate_random_geographic_graph_with_gauss_kernel(params_rng,  n_nodes=n_nodes, target_degree=target_deg)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])\n",
    "    degrees = [val for (node, val) in G.degree()]\n",
    "    av_deg.append(sum(degrees)/n_nodes)\n",
    "    to_plot_coords.append(coord)\n",
    "print(av_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libpysal.weights import KNN\n",
    "\n",
    "def generate_KNN_random_geographic_graph(params_rng, n_nodes, K):\n",
    "    # generation of the nodes coordinates \n",
    "    nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "    # computation of the KNN adjacency matrix\n",
    "    knn_weights = KNN.from_array(nodes_coord, K)\n",
    "    # build the graph\n",
    "    G_directed = knn_weights.to_networkx()\n",
    "    G = G_directed.to_undirected()\n",
    "    return G, nodes_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "n_nodes = 80\n",
    "K = 8\n",
    "av_deg = []\n",
    "for _ in range(5):\n",
    "    G, coord = generate_KNN_random_geographic_graph(params_rng,  n_nodes=n_nodes, K=K)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])\n",
    "    degrees = [val for (node, val) in G.degree()]\n",
    "    av_deg.append(sum(degrees)/n_nodes)\n",
    "print(av_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "def load_modify_connec_and_store_graph(original_graph_path:int, exp_id:int, edge_prop:float, rng:np.random.Generator, target_dir:str):\n",
    "    adj_mat = np.load(f\"{original_graph_path}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "    G = nx.from_numpy_array(adj_mat)\n",
    "    G_modif = modify_graph_connectivity_from_binary_adj_mat_2(G, edge_prop, rng)\n",
    "    save_graph(G_modif, f\"{target_dir}/{exp_id}_mat_adj.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 1000\n",
    "GRAPH_SEED = 1\n",
    "N_NODES = 20\n",
    "TARGET_DEGREE = 10\n",
    "K_NEIGHBOUR = 8\n",
    "ER_BANDWIDTH = 0.4\n",
    "EDGE_PROP_TO_MODIF = 0.6\n",
    "INITIAL_GRAPH_PATH = \"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "INIT_NAME = \"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "to_modify_graph_path = os.path.join(INITIAL_GRAPH_PATH, INIT_NAME)\n",
    "\n",
    "# logging\n",
    "NAME =  INIT_NAME + '_' + f\"edge_prop_{EDGE_PROP_TO_MODIF}\"  #f\"ER_{N_NODES}_nodes_deg_{TARGET_DEGREE}_bandwidth_{ER_BANDWIDTH}_edge_prop_{EDGE_PROP_TO_MODIF}\"\n",
    "data_dir = \"data_1/graphs/ER_with_bd_edge_changed/\" + NAME\n",
    "graphs_desc = f\"Graphs fetched from {to_modify_graph_path}. ER graphs with fixed number of nodes. The edge probability is randomly drawn based on the target degree but also using a bandwidth parameter to allow for more diversity. Otherwise, for a given number of nodes and edge probability, the generated graphs would always be the same based on the networkx implementation. The connectivity of the graphs is modified: we randomly remove half of the edges obtained when randomly selecting some with the given edge proportion, and we randomly add the same amount of edges.\"\n",
    "graph_gen_func = lambda rng : generate_random_er_graphs_fixed_nodes_nb(rng, GRAPH_SEED, n_nodes=N_NODES, target_deg=TARGET_DEGREE, bandwidth_coef=ER_BANDWIDTH)\n",
    "graph_modif_func = lambda exp_id : load_modify_connec_and_store_graph(to_modify_graph_path, exp_id, EDGE_PROP_TO_MODIF, graph_rng, data_dir)\n",
    "# graphs_metadata = {\"datetime\": now, \"description\": graphs_desc, \"commit hash\": get_git_head_short_hash(), \"graph_func\": generate_random_er_graphs_fixed_nodes_nb.__name__, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": \"index\", \"n_nodes\": N_NODES, \"target_degree\": TARGET_DEGREE, \"bandwidth coefficient\": ER_BANDWIDTH}\n",
    "graphs_metadata = {\"datetime\": now, \"description\": graphs_desc, \"commit hash\": get_git_head_short_hash(), \"graph_modif_func\": load_modify_connec_and_store_graph.__name__, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": \"index\", \"n_nodes\": N_NODES, \"target_degree\": TARGET_DEGREE, \"bandwidth coefficient\": ER_BANDWIDTH, \"modified edge proportion\": EDGE_PROP_TO_MODIF}\n",
    "\n",
    "# output formatting\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=False)\n",
    "graphs_metadata = turn_all_list_of_dict_into_str(graphs_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"00_graphs_metadata.json\", graphs_metadata, indent=4)\n",
    "\n",
    "# graph generation\n",
    "for exp_id in range(N_EXP):\n",
    "    graph_modif_func(exp_id)\n",
    "    # G, coords = graph_gen_func(graph_rng)\n",
    "    # save_graph(G, f\"{data_dir}/{exp_id}_mat_adj.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal and bkps generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_in_hyp(G:nx.Graph, signal_rng:np.random.Generator, hyp:seg_length, n_samples:int, diag_cov_max):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = get_min_size_for_hyp(n_dims=n_dims, hyp=hyp)\n",
    "    bkps = draw_bkps_with_gap_constraint(n_samples=n_samples, bkps_gap=min_size, bkps_rng=signal_rng, n_bkps_max=n_samples)\n",
    "    # generate the signal\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_in_hyp_with_fixed_min_size(G:nx.Graph, signal_rng:np.random.Generator, hyp:seg_length, n_samples:int, min_size_coef:int, diag_cov_max):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = int(min_size_coef * get_min_size_for_hyp(n_dims=n_dims, hyp=hyp))\n",
    "    bkps = draw_fixed_nb_bkps_with_gap_constraint(n_samples=n_samples, bkps_gap=min_size, bkps_rng=signal_rng)\n",
    "    # generate the signal\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_FOLDER = \"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "GRAPH_FOLDER_NAME = \"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "GRAPH_PATH = os.path.join(GRAPH_FOLDER, GRAPH_FOLDER_NAME)\n",
    "SIGNAL_SEED = 3\n",
    "DIAG_COV_MAX = 1\n",
    "MIN_SEGMENT_LENGTH_COEF = 0.1\n",
    "SNR = 10\n",
    "N_SAMPLES = 1000\n",
    "BKPS_GAP_CONSTRAINT: seg_length = \"large\" \n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "sigma_noise = DIAG_COV_MAX / ( 10**(SNR / 10) )\n",
    "\n",
    "\n",
    "# logging\n",
    "NAME = f\"{BKPS_GAP_CONSTRAINT}_x{MIN_SEGMENT_LENGTH_COEF}_SNR_{SNR}\" + \"_\" + GRAPH_FOLDER_NAME \n",
    "data_dir = f\"data_1/signal/within_hyp/noisy_varying_segment_length/\" + NAME\n",
    "signal_desc = \"Data verifying the two hypothesis, with a given number of bkps fixed by a coefficient applied to the large segment length. We add a white noise to the observed signal.\"\n",
    "signal_gen_func = lambda G : generate_rd_signal_in_hyp_with_fixed_min_size(G, signal_rng, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, min_size_coef=MIN_SEGMENT_LENGTH_COEF, diag_cov_max=DIAG_COV_MAX)\n",
    "signal_modif_func = lambda s : add_diagonal_white_noise(signal_rng, s, sigma=sigma_noise)\n",
    "signal_metadata = {\"datetime\": now, \"description\": signal_desc, \"commit hash\": get_git_head_short_hash(), \"graph_folder\": GRAPH_PATH, \"signal_seed\": SIGNAL_SEED, \"signal_gen_function\": generate_rd_signal_in_hyp.__name__, \"signal_modif_func\": add_diagonal_white_noise.__name__, \"n_samples\": N_SAMPLES, \"diag_cov_max\": DIAG_COV_MAX, \"min_size_coef\": MIN_SEGMENT_LENGTH_COEF, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"SNR\": SNR}\n",
    "\n",
    "# output formatting\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=False)\n",
    "signal_metadata = turn_all_list_of_dict_into_str(signal_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"00_signal_metadata.json\", signal_metadata, indent=4)\n",
    "\n",
    "# signal generation\n",
    "for exp_id in range(len(os.listdir(GRAPH_PATH)) - 1):\n",
    "    adj_mat = np.load(f\"{GRAPH_PATH}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "    G = nx.from_numpy_array(adj_mat)\n",
    "    bkps, signal = signal_gen_func(G)\n",
    "    modif_signal = signal_modif_func(signal)\n",
    "    save_signal_and_bkps(signal, bkps, data_dir, str(exp_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostGraphLasso(BaseCost):\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"graph_lasso_mle_cost\"\n",
    "\n",
    "    def __init__(self, alpha, add_small_diag=True):\n",
    "        \"\"\"Initialize the object.\n",
    "\n",
    "        Args:\n",
    "            add_small_diag (bool, optional): For signals with truly constant\n",
    "                segments, the covariance matrix is badly conditioned, so we add\n",
    "                a small diagonal matrix. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.signal = None\n",
    "        self.min_size = 2\n",
    "        self.n_samples = None\n",
    "        self.alpha = alpha\n",
    "        self.add_small_diag = add_small_diag\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, signal) :\n",
    "        \"\"\"Set parameters of the instance.\n",
    "        Args:\n",
    "            signal (array): signal of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        if signal.ndim == 1:\n",
    "            self.signal = signal.reshape(-1, 1)\n",
    "        else:\n",
    "            self.signal = signal\n",
    "        self.n_samples, self.n_dims = self.signal.shape\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "\n",
    "        Returns:\n",
    "            float: segment cost\n",
    "        \"\"\"\n",
    "        sub_signal = self.signal[start:end, :]\n",
    "        emp_cov_mat= np.cov(sub_signal.T)\n",
    "        gl_estimator = GraphicalLasso(alpha=self.alpha, assume_centered=True, covariance='precomputed').fit(emp_cov_mat)\n",
    "        return log_likelihood(emp_cov_mat, gl_estimator.get_precision())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_station_normal_cost(signal, graph_laplacian_mat):\n",
    "    '''signal (array): of shape [n_samples, n_dim]'''\n",
    "    # computation of the graph fourier transform\n",
    "    _, eigvects = eigh(graph_laplacian_mat)\n",
    "    gft =  signal @ eigvects # equals signal.dot(eigvects) = eigvects.T.dot(signal.T).T\n",
    "    gft_mean = np.mean(gft, axis=0)\n",
    "    # computation of the per-segment cost utils\n",
    "    gft_square_cumsum = np.concatenate([np.zeros((1, signal.shape[1])), np.cumsum((gft - gft_mean[None, :])**2, axis=0)], axis=0)\n",
    "    return gft_square_cumsum.astype(np.float64)\n",
    "\n",
    "@njit\n",
    "def numba_statio_cost_func(start, end, gft_square_cumsum):\n",
    "    '''\n",
    "    Computes the cost over signal[start:end, :] where end is excluded\n",
    "\n",
    "    gft_square_cumsum (array): of shape [n_samples + 1, n_dim] \n",
    "    '''\n",
    "    sub_square_sum = gft_square_cumsum[end, :] - gft_square_cumsum[start, :]\n",
    "    return np.float64(end  - start) * np.sum(np.log(sub_square_sum / (end - start)), dtype=np.float64)\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_statio_cost(n_bkps:int, min_size:int, data: np.ndarray):\n",
    "    n_samples = data.shape[0]\n",
    "    # if no bkp to find\n",
    "    if n_bkps == 0:\n",
    "        return np.array([1000], dtype=np.int64)\n",
    "    # full partitions costs\n",
    "    full_part_cost = np.inf * np.ones((n_bkps, n_samples, n_samples), dtype=np.float64)\n",
    "    # compute the segment cost with no bpk, for admissible segment only\n",
    "    for start in range(0, n_samples-min_size):\n",
    "        # until n_samples + 1 because the call to cost_function(start, end, data) computes over [y_0, ... y_{n-1}] (remember data.shape[0] = n_samples + 1)\n",
    "        for end in range(start+min_size, n_samples):  \n",
    "            full_part_cost[0, start, end] = numba_statio_cost_func(start, end, data)\n",
    "    # compute the cost of the possible higher order partitions \n",
    "    for bkp_order in range(1, n_bkps):\n",
    "        min_multi_seg_length = (bkp_order + 1) * min_size\n",
    "        for start in range(0, n_samples-min_multi_seg_length):\n",
    "            for end in range(start + min_multi_seg_length, n_samples):\n",
    "                min_size_left_seg = min_multi_seg_length - min_size\n",
    "                full_part_cost[bkp_order, start, end] = np.min(full_part_cost[bkp_order-1, start, start+min_size_left_seg:end-min_size+1] + full_part_cost[0, start+min_size_left_seg:end-min_size+1, end])\n",
    "    # successively pick the bkps from the right-end of the whole signal\n",
    "    bkps = np.int64(n_samples-1) * np.ones(n_bkps+1, dtype=np.int64)\n",
    "    for bkp_id in range(n_bkps, 0, -1):\n",
    "        min_multi_seg_length = np.int64(bkp_id * min_size) \n",
    "        bkp_right = bkps[bkp_id]\n",
    "        bkp_left = min_multi_seg_length + np.argmin(full_part_cost[bkp_id-1, 0, min_multi_seg_length:bkp_right-min_size+1] + full_part_cost[0, min_multi_seg_length:bkp_right-min_size+1, bkp_right])\n",
    "        bkps[bkp_id-1] = bkp_left\n",
    "    return bkps\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_statio_cost_2_optim(n_bkps:int, min_size:int, data: np.ndarray):\n",
    "    # path_mat[n, K] avec n --> [y_0, ... y_{n-1}], K : n_bkps\n",
    "\n",
    "    # initialization \n",
    "    n_samples = data.shape[0] - 1\n",
    "    path_mat = np.empty((n_samples+1, n_bkps+1), dtype=np.int32)\n",
    "    path_mat[:, 0] = 0\n",
    "    path_mat[0, :] = -1\n",
    "    sum_of_cost_mat = np.full((n_samples+1, n_bkps+1),  fill_value=np.inf, dtype=np.float64)\n",
    "    sum_of_cost_mat[0, :] = 0\n",
    "\n",
    "    # forward computation\n",
    "    for end in range(min_size, n_samples+1):\n",
    "        sum_of_cost_mat[end, 0] = numba_statio_cost_func(0, end, data)\n",
    "        # consistent because our cost functions compute the costs over [start, end[\n",
    "        max_admissible_n_bkp = floor(end/min_size) - 1\n",
    "        for k_bkps in range(1, min(max_admissible_n_bkp+1, n_bkps+1)):\n",
    "            soc_optim = np.inf\n",
    "            soc_argmin = -1\n",
    "            for mid in range(min_size*k_bkps, end - min_size + 1): \n",
    "                soc = sum_of_cost_mat[mid, k_bkps-1] + numba_statio_cost_func(mid, end, data)\n",
    "                if soc < soc_optim:\n",
    "                    soc_argmin = mid\n",
    "                    soc_optim = soc\n",
    "            sum_of_cost_mat[end, k_bkps] = soc_optim\n",
    "            path_mat[end, k_bkps] = soc_argmin\n",
    "\n",
    "    # backtracking\n",
    "    bkps = np.full((n_bkps+1), fill_value=n_samples)\n",
    "    for k_bkps in range(n_bkps, 0, -1):\n",
    "        bkps[k_bkps-1] = path_mat[bkps[k_bkps], k_bkps]\n",
    "    \n",
    "    return bkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import slogdet\n",
    "\n",
    "@njit\n",
    "def standard_normal_cost_func(start, end, signal):\n",
    "    '''signal (array): of shape [n_samples, n_dim]'''\n",
    "    sub = signal[start:end, :]\n",
    "    cov = np.cov(sub.T)\n",
    "    cov += 1e-6 * np.eye(signal.shape[1])\n",
    "    _, val = slogdet(cov)\n",
    "    return np.float64(val * (end - start))\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_mle_standard_cost(n_bkps:int, min_size:int, signal: np.ndarray):\n",
    "    n_samples = signal.shape[0]\n",
    "    # if no bkp to find\n",
    "    if n_bkps == 0:\n",
    "        return np.array([1000], dtype=np.int64)\n",
    "    # full partitions costs\n",
    "    full_part_cost = np.inf * np.ones((n_bkps, n_samples, n_samples), dtype=np.float64)\n",
    "    # compute the segment cost with no bpk, for admissible segment only\n",
    "    for start in range(0, n_samples-min_size+1):\n",
    "        # until n_samples + 1 because the call to cost_function(start, end, data) computes over [y_0, ... y_{n-1}] \n",
    "        for end in range(start+min_size, n_samples+1):  \n",
    "            full_part_cost[0, start, end] = standard_normal_cost_func(start, end, signal)\n",
    "    # compute the cost of the possible higher order partitions \n",
    "    for bkp_order in range(1, n_bkps):\n",
    "        min_multi_seg_length = (bkp_order + 1) * min_size\n",
    "        for start in range(0, n_samples-min_multi_seg_length):\n",
    "            for end in range(start + min_multi_seg_length, n_samples):\n",
    "                min_size_left_seg = min_multi_seg_length - min_size\n",
    "                full_part_cost[bkp_order, start, end] = np.min(full_part_cost[bkp_order-1, start, start+min_size_left_seg:end-min_size+1] + full_part_cost[0, start+min_size_left_seg:end-min_size+1, end])\n",
    "    # successively pick the bkps from the right-end of the whole signal\n",
    "    bkps = np.int64(n_samples) * np.ones(n_bkps+1, dtype=np.int64)\n",
    "    for bkp_id in range(n_bkps, 0, -1):\n",
    "        min_multi_seg_length = np.int64(bkp_id * min_size) \n",
    "        bkp_right = bkps[bkp_id]\n",
    "        bkp_left = min_multi_seg_length + np.argmin(full_part_cost[bkp_id-1, 0, min_multi_seg_length:bkp_right-min_size+1] + full_part_cost[0, min_multi_seg_length:bkp_right-min_size+1, bkp_right])\n",
    "        bkps[bkp_id-1] = bkp_left\n",
    "    return bkps\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_mle_standard_cost_2_optim(n_bkps:int, min_size:int, signal):\n",
    "    # path_mat[n, K] avec n --> [y_0, ... y_{n-1}] (very important to understand indexing) , K : n_bkps\n",
    "    # sum_of_cost_mat[n, K]: best cost for signal until sample n with K bkps\n",
    "\n",
    "    # initialization \n",
    "    n_samples = signal.shape[0]\n",
    "    path_mat = np.empty((n_samples+1, n_bkps+1), dtype=np.int32)\n",
    "    path_mat[:, 0] = 0\n",
    "    path_mat[0, :] = -1\n",
    "    sum_of_cost_mat = np.full((n_samples+1, n_bkps+1),  fill_value=np.inf, dtype=np.float64)\n",
    "    sum_of_cost_mat[0, :] = 0\n",
    "\n",
    "    # pre-computation, to optimize jit processing\n",
    "    statio_segment_cost = np.full((n_samples+1, n_samples+1), fill_value=np.inf, dtype=np.float64)\n",
    "    for start in range(0, n_samples-min_size+1):\n",
    "        for end in range(start+min_size, n_samples+1):  \n",
    "            statio_segment_cost[start, end] = standard_normal_cost_func(start, end, signal)\n",
    "\n",
    "    # forward computation\n",
    "    for end in range(min_size, n_samples+1):\n",
    "        sum_of_cost_mat[end, 0] = statio_segment_cost[0, end]\n",
    "        # consistent because our cost functions compute the costs over [start, end[\n",
    "        max_admissible_n_bkp = floor(end/min_size) - 1\n",
    "        for k_bkps in range(1, min(max_admissible_n_bkp+1, n_bkps+1)):\n",
    "            soc_optim = np.inf\n",
    "            soc_argmin = -1\n",
    "            for mid in range(min_size*k_bkps, end - min_size + 1):\n",
    "                soc = sum_of_cost_mat[mid, k_bkps-1] + statio_segment_cost[mid, end]\n",
    "                if soc < soc_optim:\n",
    "                    soc_argmin = mid\n",
    "                    soc_optim = soc\n",
    "            sum_of_cost_mat[end, k_bkps] = soc_optim\n",
    "            path_mat[end, k_bkps] = soc_argmin\n",
    "\n",
    "    # backtracking\n",
    "    bkps = np.full((n_bkps+1), fill_value=n_samples)\n",
    "    for k_bkps in range(n_bkps, 0, -1):\n",
    "        bkps[k_bkps-1] = path_mat[bkps[k_bkps], k_bkps]\n",
    "    \n",
    "    return bkps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search algorithms: running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_numba_statio_normal_cost(G: nx.Graph, signal: np.ndarray, gt_bkps: List[int], statio_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    graph_lapl_mat = nx.laplacian_matrix(G).toarray().astype(np.float64)\n",
    "    ###############################################################\n",
    "    # graph_lapl_mat = np.eye(signal.shape[1])\n",
    "    ###############################################################\n",
    "    gft_square_cumsum = init_station_normal_cost(signal, graph_lapl_mat)\n",
    "    statio_bkps = numba_cpd_dynprog_statio_cost_2_optim(len(gt_bkps)-1, signal.shape[1], gft_square_cumsum)\n",
    "    statio_bkps = [int(bkp) for bkp in statio_bkps]\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "    statio_results[exp_id][\"gt\"] = gt_bkps\n",
    "    statio_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_numba_standard_mle_normal_cost(signal: np.ndarray, gt_bkps: List[int], normal_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    normal_bkps = numba_cpd_dynprog_mle_standard_cost_2_optim(len(gt_bkps) - 1, signal.shape[1], signal)\n",
    "    normal_bkps = [int(bkp) for bkp in normal_bkps]\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "    normal_results[exp_id][\"gt\"] = gt_bkps\n",
    "    normal_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statio_normal_cost(G: nx.Graph, signal: np.ndarray, gt_bkps: List[int], statio_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "    statio_results[exp_id][\"gt\"] = gt_bkps\n",
    "    statio_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_standard_mle_normal_cost(signal: np.ndarray, gt_bkps: List[int], normal_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "    normal_results[exp_id][\"gt\"] = gt_bkps\n",
    "    normal_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph_lasso_mle_cost(signal: np.ndarray, gt_bkps: List[int], alpha:float, graph_lasso_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        lasso_cost = CostGraphLasso(alpha=alpha)\n",
    "        algo_lasso = rpt.Dynp(custom_cost=lasso_cost, jump=1, min_size=signal.shape[1]).fit(signal)\n",
    "        lasso_bkps = algo_lasso.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    graph_lasso_results[exp_id] = {}\n",
    "    graph_lasso_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    graph_lasso_results[exp_id][\"pred\"] = lasso_bkps\n",
    "    graph_lasso_results[exp_id][\"gt\"] = gt_bkps\n",
    "    graph_lasso_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME =  \"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "GRAPH_PATH = \"data_1/graphs/ER_with_bd_edge_changed\"\n",
    "SIGNAL_PATH = \"data_1/signal/within_hyp/noisy_varying_segment_length\"\n",
    "SIGNAL_NAME = \"large_x0.1_SNR_10\" + '_' + NAME\n",
    "MAX_ID_SUBSET = 1000\n",
    "RESULT_DIR = \"results_1/synthetic/within_hypo_graph_connec_modif/large_x0.1\"\n",
    "LASSO_ALPHA = 0.1\n",
    "EDGE_PROP_MODIF_LIST = [0.03, 0.05, 0.08, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "OTHER_GRAPH_SEED = 1\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# logging\n",
    "signal_path = os.path.join(SIGNAL_PATH, SIGNAL_NAME)\n",
    "signal_metadata = open_json(f\"{signal_path}/00_signal_metadata.json\")\n",
    "seg_length_hyp = \"minimal\"\n",
    "graph_rng = np.random.default_rng(OTHER_GRAPH_SEED)\n",
    "\n",
    "for EDGE_PROP in EDGE_PROP_MODIF_LIST:\n",
    "    \n",
    "    GRAPH_NAME =  NAME + f\"_edge_prop_{EDGE_PROP}\" #\"exp_geo_20_nodes_av_deg_10\" #\"ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.05\" \n",
    "    graph_path = os.path.join(GRAPH_PATH, GRAPH_NAME)\n",
    "    graph_metadata = open_json(f\"{graph_path}/00_graphs_metadata.json\")\n",
    "    \n",
    "    RESULT_NAME = f\"edge_prop_{EDGE_PROP}\"\n",
    "    final_name = SIGNAL_NAME + \"_\" + RESULT_NAME\n",
    "    results_dir = os.path.join(RESULT_DIR, final_name)\n",
    "\n",
    "    exp_desc = \"Study of the robustness with respect to noisy graph observation. More precisely, the binary connectivity of the graph is modified, and a proportion of edges is added and removed.\"\n",
    "    experiment_metadata = {\"datetime\": now, \"description\": exp_desc, \"commit hash\": get_git_head_short_hash(), \"graph folder\": graph_path, \"graph metadata\": graph_metadata, \"signal folder\": SIGNAL_PATH + '/' + SIGNAL_NAME, \"signal metadata\": signal_metadata, \"min segment length hypothesis\": seg_length_hyp, \"max id experiment subset\": MAX_ID_SUBSET, \"edge_prop\": EDGE_PROP}\n",
    "\n",
    "    # output formatting\n",
    "    statio_results = {}\n",
    "    # normal_results = {}\n",
    "    # lasso_results = {}\n",
    "\n",
    "    # running CPD algorithms\n",
    "    for exp_id in tqdm(range(MAX_ID_SUBSET), desc='Running experiment...'):\n",
    "        exp_id = str(exp_id)\n",
    "        G, signal, gt_bkps, min_size = load_data(graph_path, signal_path, exp_id, seg_length_hyp)\n",
    "        run_numba_statio_normal_cost(G, signal, gt_bkps, statio_results)\n",
    "        # run_numba_standard_mle_normal_cost(signal, gt_bkps, normal_results)\n",
    "        # run_graph_lasso_mle_cost(signal, gt_bkps, LASSO_ALPHA, lasso_results)\n",
    "\n",
    "    create_parent_and_dump_json(results_dir, \"experiment_metadata.json\", turn_all_list_of_dict_into_str(experiment_metadata), indent=4)\n",
    "    create_parent_and_dump_json(results_dir, F\"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "# create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n",
    "# create_parent_and_dump_json(results_dir, F\"lasso_pred_alpha_{LASSO_ALPHA}.json\", turn_all_list_of_dict_into_str(lasso_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME =  \"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "GRAPH_NAME =  NAME #\"exp_geo_20_nodes_av_deg_10\" #\"ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.05\" \n",
    "GRAPH_PATH = \"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "SIGNAL_PATH = \"data_1/signal/within_hyp/noisy_varying_segment_length\"\n",
    "SIGNAL_NAME = \"large_x0.1_SNR_10\" + '_' + NAME\n",
    "MAX_ID_SUBSET = 1000\n",
    "RESULT_DIR = \"results_1/synthetic/ablation_study/large_x0.1\"\n",
    "RESULT_NAME = \"er_other_graph_same_folder_2\"\n",
    "LASSO_ALPHA = 0.1\n",
    "OTHER_GRAPH_SEED = 1\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "final_name = SIGNAL_NAME + \"_\" + RESULT_NAME\n",
    "results_dir = os.path.join(RESULT_DIR, final_name)\n",
    "\n",
    "# logging\n",
    "graph_path = os.path.join(GRAPH_PATH, GRAPH_NAME)\n",
    "signal_path = os.path.join(SIGNAL_PATH, SIGNAL_NAME)\n",
    "graph_metadata = open_json(f\"{graph_path}/00_graphs_metadata.json\")\n",
    "signal_metadata = open_json(f\"{signal_path}/00_signal_metadata.json\")\n",
    "seg_length_hyp = \"minimal\"\n",
    "graph_rng = np.random.default_rng(OTHER_GRAPH_SEED)\n",
    "\n",
    "\n",
    "exp_desc = \"Ablation study: another ER graph from the same folder is used to instantiate the cost function.\"\n",
    "experiment_metadata = {\"datetime\": now, \"description\": exp_desc, \"commit hash\": get_git_head_short_hash(), \"graph folder\": graph_path, \"graph metadata\": graph_metadata, \"signal folder\": SIGNAL_PATH + '/' + SIGNAL_NAME, \"signal metadata\": signal_metadata, \"min segment length hypothesis\": seg_length_hyp, \"max id experiment subset\": MAX_ID_SUBSET}\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "# normal_results = {}\n",
    "# lasso_results = {}\n",
    "\n",
    "# running CPD algorithms\n",
    "for exp_id in tqdm(range(MAX_ID_SUBSET), desc='Running experiment...'):\n",
    "    exp_id = str(exp_id)\n",
    "    G, signal, gt_bkps, min_size = load_data(graph_path, signal_path, exp_id, seg_length_hyp)\n",
    "    run_numba_statio_normal_cost(G, signal, gt_bkps, statio_results)\n",
    "    # run_numba_standard_mle_normal_cost(signal, gt_bkps, normal_results)\n",
    "    # run_graph_lasso_mle_cost(signal, gt_bkps, LASSO_ALPHA, lasso_results)\n",
    "\n",
    "create_parent_and_dump_json(results_dir, \"experiment_metadata.json\", turn_all_list_of_dict_into_str(experiment_metadata), indent=4)\n",
    "create_parent_and_dump_json(results_dir, F\"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "# create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n",
    "# create_parent_and_dump_json(results_dir, F\"lasso_pred_alpha_{LASSO_ALPHA}.json\", turn_all_list_of_dict_into_str(lasso_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 5\n",
    "res_folder_root = \"results_1/synthetic/within_hypo_graph_connec_modif/large_x0.1\"\n",
    "file_names = os.listdir(res_folder_root)\n",
    "PRED_FOLDER = [os.path.join(res_folder_root, file_name) for file_name in file_names]\n",
    "# PRED_FOLDER = [res_folder_root]\n",
    "\n",
    "for pred_dir in PRED_FOLDER:\n",
    "\n",
    "    # fetching predictions\n",
    "    data_stats = open_json(f\"{pred_dir}/experiment_metadata.json\")\n",
    "    statio_pred_dic = open_json(f\"{pred_dir}/statio_pred.json\")\n",
    "    # normal_pred_dic = open_json(f\"{pred_dir}/normal_pred.json\")\n",
    "    # assert list(normal_pred_dic.keys()) == list(statio_pred_dic.keys())\n",
    "\n",
    "    # output formatting\n",
    "    metrics_dic = {}\n",
    "    metrics_dic[\"pred_path\"] = pred_dir\n",
    "    metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "    metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "    statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "    # normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "    for exp_id in statio_pred_dic.keys():\n",
    "        # compute metrics\n",
    "        statio_pred_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "        # normal_pred_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "        gt_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"gt\"])\n",
    "        compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "        # compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "        # add time values\n",
    "        statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "        # normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "    # results post-precessing and saving\n",
    "    full_results = {\"statio normal cost\": statio_results}# , \"normal cost\": normal_results}\n",
    "    full_results = compute_and_add_stat_on_metrics(full_results)\n",
    "    full_results[\"metadata\"] = metrics_dic\n",
    "    full_results = turn_all_list_of_dict_into_str(full_results)\n",
    "    create_parent_and_dump_json(pred_dir, f'metrics_{PRECI_RECALL_MARGIN}.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION 1\n",
    "####################\n",
    "preci_margin = 2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "legend_elements = []\n",
    "\n",
    "\n",
    "res_fold_root = \"results_1/synthetic/within_hypothesis_noisy/varying_segment_length/large_x0.1_SNR_10ER_20_nodes_deg_10_bandwidth_0.4_80exp\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\", \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "\n",
    "cost_func_keys = [\"normal cost\"]\n",
    "res_dic = get_res_per_metric_per_cost_func([res_fold_root], cost_func_keys, metrics_keys, preci_margin)\n",
    "x_coords = [0.1]\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "legend_elements.append(Line2D([0], [0], color=colors_per_cost_func[\"normal cost\"], lw=3, label=\"A. normal mle cost\"))\n",
    "\n",
    "cost_func_keys = [\"statio normal cost\"]\n",
    "res_dic = get_res_per_metric_per_cost_func([res_fold_root], cost_func_keys, metrics_keys, preci_margin)\n",
    "x_coords = [0.2]\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "legend_elements.append(Line2D([0], [0], color=colors_per_cost_func[\"statio normal cost\"], lw=3, label=\"B. statio cost\"))\n",
    "\n",
    "\n",
    "##### INITIALIZATION 2\n",
    "####################\n",
    "\n",
    "res_folder_root = \"results_1/synthetic/ablation_study/large_x0.1\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "res_file_name_list.sort()\n",
    "res_folder_list = [os.path.join(res_folder_root, file_name) for file_name in res_file_name_list]\n",
    "cost_func_keys = [\"statio normal cost\"]\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "colors_per_file = [\"lightcoral\", \"darkorchid\", \"peru\", \"teal\"]\n",
    "x_coords_top = [0.3, 0.4, 0.5, 0.6]\n",
    "labels = [\"C. connectivity modified 0.05\", \"D. other ER graph\", \"E. exp sim geographical graphs\", \"F. diagonal in canonical basis\"]\n",
    "\n",
    "for i, file_name in enumerate(res_folder_list):\n",
    "\n",
    "    colors_per_cost_func = {\"statio normal cost\": colors_per_file[i]}\n",
    "    res_dic = get_res_per_metric_per_cost_func([file_name], cost_func_keys, metrics_keys, preci_margin)\n",
    "    x_coords = [x_coords_top[i]]\n",
    "    # precision\n",
    "    plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "    # haussdorff\n",
    "    plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "    legend_elements.append(Line2D([0], [0], color=colors_per_file[i], lw=3, label=labels[i]))\n",
    "\n",
    "\n",
    "axes[0].set_title(F'F1 Score (margin = {preci_margin})')\n",
    "axes[0].set_xticks([i/10 for i in range(1, 7)], [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n",
    "axes[0].set_xlabel(\"Different settings\")\n",
    "axes[1].set_title('Haussdorff')\n",
    "axes[1].set_xticks([i/10 for i in range(1, 7)], [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n",
    "axes[1].set_xlabel(\"Different settings\")\n",
    "axes[1].set_ylim(bottom=0, top=400)\n",
    "\n",
    "fig.suptitle(\"ABLATION STUDY \\nER graphs; N_nodes=N=20; SNR=10; segment_length=0.1*N(N-1)/2; N_exp=1000\")\n",
    "fig.subplots_adjust(bottom=0.17, top=0.85)\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncols=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "####################\n",
    "PRECI_RECALL_MARGIN = 5\n",
    "pred_path = \"results_1/synthetic/within_hypothesis/varying_segment_length\"\n",
    "PRED_FOLDER = ['results_1/synthetic/within_hypothesis/varying_segment_length/minimal_x1_ER_20_nodes_deg_10_bandwidth_0.4_80_exp',\n",
    "                'results_1/synthetic/within_hypothesis/varying_segment_length/minimal_x1.5_ER_20_nodes_deg_10_bandwidth_0.4_80_exp',\n",
    "                'results_1/synthetic/within_hypothesis/varying_segment_length/minimal_x2_ER_20_nodes_deg_10_bandwidth_0.4_80_exp',\n",
    "                'results_1/synthetic/within_hypothesis/varying_segment_length/minimal_x2.5_ER_20_nodes_deg_10_bandwidth_0.4_80_exp',\n",
    "                'results_1/synthetic/within_hypothesis/varying_segment_length/minimal_x4_ER_20_nodes_deg_10_bandwidth_0.4_80_exp'\n",
    "            ]\n",
    "# file_names = os.listdir(pred_path)\n",
    "# PRED_FOLDER = [os.path.join(pred_path, file_name) for file_name in file_names]\n",
    "\n",
    "\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "markers = ['o', \"s\"]\n",
    "linestyles = [\"dashed\", \"dotted\"]\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "abscissa_pos =  [\"x1\", \"x1.5\", \"x2\", \"x2.5\", \"x4\"]\n",
    "graph_types = [\"ER\"] # \"exp_geo\", \"KNN_geo\", \n",
    "graph_colors = ['darkorange'] #, 'dodgerblue', 'darkorchid']\n",
    "shift = 3\n",
    "\n",
    "\n",
    "##### PLOTTING\n",
    "##############\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(7*len(metrics_keys), 5)) #, layout='constrained')\n",
    "\n",
    "for i in range(len(graph_types)):\n",
    "\n",
    "    # re-arrange the folder name so they have the right position for meaningful plotting\n",
    "    graph_name = graph_types[i]\n",
    "    res_folder_list = [folder_name for folder_name in PRED_FOLDER if graph_name in folder_name]\n",
    "    print(res_folder_list)\n",
    "    res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, PRECI_RECALL_MARGIN)\n",
    "\n",
    "    x_coords = [float(x[1:]) + i*shift for x in abscissa_pos]\n",
    "    plot_scatter_errorbar(x_coords, cost_func_keys, graph_colors[i], markers, linestyles, \"f1_score\", res_dic, ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Number of nodes\")\n",
    "    axes[0].set_title(F'F1 SCORE (margin = {PRECI_RECALL_MARGIN})')\n",
    "    plot_scatter_errorbar(x_coords, cost_func_keys, graph_colors[i], markers, linestyles, \"hausdorff\", res_dic, ax=axes[1])\n",
    "    axes[1].set_xlabel(\"Number of nodes\")\n",
    "    axes[1].set_title('Haussdorff')\n",
    "    axes[1].set_ylim(bottom=0)\n",
    "\n",
    "x_tick_loc = [float(x[1:]) + 0* shift for x in abscissa_pos]\n",
    "axes[0].set_xticks(x_tick_loc, abscissa_pos)\n",
    "axes[1].set_xticks(x_tick_loc, abscissa_pos)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='k', lw=0.01, marker=markers[0], linestyle=linestyles[0], label=cost_func_keys[0]),\n",
    "    Line2D([0], [0], color='k', lw=0.01, marker=markers[1], linestyle=linestyles[1], label=cost_func_keys[1]),\n",
    "    Line2D([0], [0], color=graph_colors[0], lw=3, label=graph_types[0]),\n",
    "    # Line2D([0], [0], color=graph_colors[1], lw=3, label=graph_types[1]),\n",
    "    # Line2D([0], [0], color=graph_colors[2], lw=3, label=graph_types[2]),\n",
    "]               \n",
    "fig.suptitle(\"Comparison of cost functions with respect to the minimum segment length\")\n",
    "fig.subplots_adjust(bottom=0.17)\n",
    "fig.legend(handles=legend_elements, loc=\"lower center\", ncols=len(legend_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "####################\n",
    "preci_margin = 5\n",
    "\n",
    "res_folder_root = \"results_1/synthetic/within_hypo_graph_connec_modif/large_x0.1\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "res_file_name_list.sort()\n",
    "res_folder_list = [os.path.join(res_folder_root, file_name) for file_name in res_file_name_list]\n",
    "\n",
    "abscissa_pos = range(len(res_folder_list))  #[\"x\" + str(i/10) for i in range(1, 11)]\n",
    "x_tick_labels = [file_name[61:] for file_name in res_file_name_list]\n",
    "cost_func_keys = [\"statio normal cost\"] #, \"normal cost\"]\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\"} #, \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "\n",
    "res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin)\n",
    "\n",
    "\n",
    "\n",
    "##### PLOTTING\n",
    "##############\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(8*len(metrics_keys), 6))\n",
    "\n",
    "x_coords = abscissa_pos  #[float(x[1:]) for x in abscissa_pos]\n",
    "# precision\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "axes[0].set_title(F'F1 SCORE (margin = {preci_margin})')\n",
    "axes[0].set_xlabel(\"Proportion of edges modified\")\n",
    "axes[0].set_xticks(x_coords, x_tick_labels)\n",
    "# haussdorff\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "axes[1].set_title('Haussdorff')\n",
    "axes[1].set_xlabel(\"Proportion of edges modified\")\n",
    "axes[1].set_ylim(bottom=0)\n",
    "axes[1].set_xticks(x_coords, x_tick_labels)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=list(colors_per_cost_func.values())[0], lw=3, label=list(colors_per_cost_func.keys())[0]),\n",
    "    # Line2D([0], [0], color=list(colors_per_cost_func.values())[1], lw=3, label=list(colors_per_cost_func.keys())[1]),\n",
    "    # Line2D([0], [0], color=graph_colors[1], lw=3, label=graph_types[1]),\n",
    "    # Line2D([0], [0], color=graph_colors[2], lw=3, label=graph_types[2]),\n",
    "] \n",
    "\n",
    "fig.suptitle(\"Evaluation of the robustness with respect to the quality of the graph observation \\n graphs:ER, N=nb_nodes=20, N_exp=1000, SNR=10, min segment length= 0.1 N(N-1)/2\")\n",
    "fig.subplots_adjust(bottom=0.17, top=0.85)\n",
    "fig.legend(handles=legend_elements, loc=\"lower center\", ncols=len(legend_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"Izenman2008\">[Izenman2008]</a>\n",
    "Izenman Alan J. (2008). Introduction to Random-Matrix Theory [asc.ohio-state.edu]\n",
    "\n",
    "<a id=\"Ryan2023\">[Ryan2023]</a>\n",
    "Sean Ryan and Rebecca Killick. Detecting Changes in Covariance via Random Matrix Theory. Technometrics, 65(4):480–491, October 2023. Publisher: Taylor & Francis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
