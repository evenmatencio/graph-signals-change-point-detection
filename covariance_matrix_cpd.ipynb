{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance matrix change-point detection under graph stationarity assumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "\n",
    "Let $y = (y_1, \\ldots, y_t, \\ldots, y_T), y_t \\in \\mathbb{R}^{N}$ a graph signal lying on the nodes of the graph $G = (V, E, W)$, with $N =|V|$.\n",
    "\n",
    "We aim at detecting changes of the (spatial) covariance matrix $\\Sigma_t$ of the graph signals $y_t$. We assume that there exits an unknown set of change-points $\\Tau = (t_1, \\ldots, t_K) \\subset [1, T]$ with unknown cardinality such that the covariance matrix of the graph signals is constant over any segment $[t_k, t_{k+1}]$. We do the following hypothesis:\n",
    "\n",
    "1. the signals $y_t$ follow a multivariate Gaussian distribution with fixed covariance matrix over each segment and known mean $\\mu$, i.e:\n",
    "$$\\forall k \\in [1, K] ~ \\forall t \\in [t_k, t_{k+1}] \\quad y_t \\sim \\mathcal{N}(\\mu, \\Sigma_k)$$\n",
    "\n",
    "2. over each segment, the signals $y_t$ verify the second order wide-sense graph stationarity:\n",
    "$$\\forall k \\in [1, K] \\quad \\Sigma_k = U \\text{diag}(\\gamma_k)U^T $$\n",
    "\n",
    "where the matrix $U$ contains the eigenvectors of the graph combinatorial Laplacian matrix $L = D - W$ in its columns. \n",
    "\n",
    "The Graph Fourier Transform $\\tilde{y}$ of a signal $y$ is defined by $\\tilde{y} = U^T y $.\n",
    "\n",
    "\n",
    "Based on the above assumptions, the cost derived from the maximum log-likelihood over a segment $[a, b-1]$ writes:\n",
    "\n",
    "\\begin{align*}\n",
    "    c_s(y_{a}, \\ldots, y_{b-1}) = ~ & (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + ~ \\sum_{t=a}^{b-1} \\sum_{n=1}^N \\frac{\\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2}{\\hat{\\gamma}_{a.b}^{(n)}} = ~ (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + N(b-a)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\hat{\\mu}_{T}$ is the empirical mean of the process over $[0, T]$\n",
    "- $\\hat{\\gamma}^{(n)}_{a..b}$ is the (empirical) biased correlogram/periodogram of the process over $[a, b-1]$: $\\hat{\\gamma}_{a..b} = \\frac{1}{(b-a)} \\sum_{t=a}^{b-1} \\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on the minimum distance between consecutive change points (minimum segment length)\n",
    "\n",
    "We require that the different change points $(t_1, \\ldots, t_K)$ verify:\n",
    "\n",
    "$$|t_{k+1} - t_k| >= l ~ \\forall k \\in [1, K-1] $$\n",
    "\n",
    "where $l$ can be seen as the minimum admissible segment length. In this paragraph we give a meaningful lower bound of this parameter. Such lower bound is related to the computation of the cost functions over the segments $[a, b] \\subset [0, T]$, namely the graph stationary normal cost function $c_s$ described above and the standard normal cost function $c_n$:\n",
    "\n",
    "- $ c_n(y_{a}, \\ldots, y_{b-1}) = (b - a) \\log  \\left[ \\det \\left( \\hat \\Sigma_{a..b} \\right) \\right]$\n",
    "- $ c_s(y_{a}, \\ldots, y_{b-1}) = (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} $\n",
    "\n",
    "Based on the formula of the spectrogram $\\hat{\\gamma}_{a.b}$ given in the introduction, there is no numerical constraints for the feasibility of the computation. However, the $\\log [ \\det ( \\cdot ) ]$ function used in the formula for $c_n$ should be applied to invertible matrices $\\Sigma_{a..b}$ only. Therefore, we should focus on the conditions under which the matrix:\n",
    "\n",
    "$$ \\hat \\Sigma_{a..b} = \\frac{1}{b-a} \\sum_{t=a}^{b-1} (y_t - \\mu_T) (y_t - \\mu_T)^T \\quad \\text{ with } y_t \\sim \\mathcal{N}(\\mu, \\Sigma)  $$\n",
    "\n",
    "is invertible. Actually, such conditions have already been clearly stated in different works from Random Matrix Theory (RMT). For instance, it is shown in [[Izenman2008](#Izenman2008)] that $n \\hat \\Sigma_{a..b} \\sim \\mathcal{W}(b-a, \\Sigma)$ follows the Wishart distribution. In this framework, it is possible to study the distribution of the eigenvalues of $\\hat \\Sigma_{a..b}$ and to deduce that: \n",
    "\n",
    "$$ \\text{If } b-a > N \\text{ with } N  \\text{ the dimension of } y_t, \\text{ then } \\hat \\Sigma_{a..b} \\text{ is almost surely invertible }   $$\n",
    "\n",
    "Conversely, it is possible to show that if $ b-a < N $ (the number of observations is lower then the number of variables), the matrix $\\hat \\Sigma_{a..b}$ is nalmost surely not invertible. This can be done by considering the family the first $(N+1)$ columns of $\\hat \\Sigma_{a..b}$.\n",
    "\n",
    "Thus, the right lower-bound $l$ should be $\\boxed{l = N}$, which is consistent with statement from [[Ryan2023](#Ryan2023)].\n",
    "\n",
    "Note: with segments of length $l$, one is not guaranteed to compute good estimates of the covariance matrix, but at least such computations is almost surely admissible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data generation\n",
    "\n",
    "We design two graph generation scenarios.\n",
    "\n",
    "1. Erdős–Rényi (ER) graphs with random parameters\n",
    "\n",
    "In this scenario, we pick random number of nodes $N$ uniformly drawn in $[N_{\\min}, N_{\\max}]$, that may vary according to the experiment, and a random edge probability $p$ uniformly drawn in $[0.15, 0.5]$. The latter was chosen so that the generated graphs empirically have a realistic connectivity.\n",
    "\n",
    "2. Geographic-like graphs \n",
    "\n",
    "We pick a random number $N$ of nodes uniformly drawn in $[N_{\\min}, N_{\\max}]$. The nodes are randomly located in $[0, 1]^2$ using the uniform law $\\mathcal{U}([0, 1]^2)$. Eventually, we build the adjacency matrix of the graph by applying a threshold $\\rho$ to the distance separating the nodes. More formally, if we denote $(W_{ij})_{1 \\leq i, j \\leq N}$ the coefficients of the adjacency matrix, we explore the following two formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "    W_{ij} = \\mathbb{I} \\left( \\|p_i - p_j \\|_2 \\leq \\rho \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    W_{ij} = \\frac{\\rho}{\\|p_i - p_j \\|_2}  \\mathbb{I} \\left( \\|p_i - p_j \\|_2 \\leq \\rho \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $p_i$ denotes the 2D coordinates of node $i$. In the above formulas, the value of $\\rho$ is chosen empirically so that the resulting graphs visually exhibit connectivity patterns that are consistent to what one may expect when using a graph structure for signal analysis. More precisely, we use the following values:\n",
    "\n",
    "- for $N$ drawn in $[10, 50]$: $~$ $\\rho = 0.3$\n",
    "- for $N$ drawn in $[80, 110]$: $\\rho = 0.2$\n",
    "\n",
    "This scenario simulates 2D geographic graphs and is more likely to match realistic use-cases.\n",
    "\n",
    "\n",
    "### Signal generation\n",
    "\n",
    "Let $G$ be a graph randomly generated using one of the above scenarios. We recall that the laplacian matrix $L$ of the graph verifies $L = U \\Lambda U^T$, where the columns of $U$ are the eigenvectors of $L$. We now describe how to generate a signal $(y_t)_{1 \\leq t \\leq T}$ that verifies the hypothesis presented in the [problem formulation](#problem-formulation).\n",
    "\n",
    "We first pick an admissible number of change points $K$ depending on the signal length $T=1000$ and the minimum segment length $l = N$ by sampling $~  \\mathcal{U}([1, \\min(\\frac{T}{10}, \\frac{T}{l})])$. The change points $(t_k)_{1 \\leq k \\leq K}$ are uniformly drawn in $[l, T-l]$, by checking that a newly selected change point does not break the minimum segment length criteria. Therefore, it may happen that after a limit number of iterations, not $K$ change points were drawn, but we still use the same notations (SHOULD BE CHANGED).\n",
    "\n",
    "Finally, we apply the following formula to generate the signal $(y_t)_{1 \\leq t \\leq T}$:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\forall ~ k \\in [1, K-1] ~ \\forall  t \\in [t_k, t_{k+1}] \\quad  y_t \\sim \\mathcal{N}_N(0, \\Sigma_k) \\quad \\text{ with } \\quad \\Sigma_k = U \\text{diag}(\\gamma_k) U^T ~  \\text{ and } ~ \\gamma_k \\sim \\mathcal{U}niform_N([0, 1]) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setting v.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import math\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ruptures as rpt\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from numba import njit\n",
    "from math import floor\n",
    "from scipy.linalg import eigh\n",
    "from typing import List\n",
    "from ruptures.base import BaseCost\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import utils as my_ut\n",
    "import graph_related as my_gr\n",
    "import signal_related as my_sgn\n",
    "import result_related as my_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments description and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A. Data verifying the hypothesis of the model\n",
    "\n",
    "In this experiment, we generate data according to the hypothesis presented in the [problem formulation](#problem-formulation) and we compare our method to the cost function for standard covariance change detection in Gaussian models (that is supposed to cover our hypothesis). More precisely, we randomly generate a graph and a corresponding multivariate Gaussian signal, undergoing a (known) random number of covariance change points. The comparison between the two methods relies on the precision / recall and Hausdorff metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 3\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G, _ = my_gr.generate_random_geographic_graph_with_gauss_kernel(graph_rng, n_nodes=20, target_degree=10)\n",
    "bkps, s = my_sgn.generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_samples=1000, diag_cov_max=10, hyp='min', n_bkps_max=10)\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(mosaic='ABB', figsize=(14, 3))\n",
    "nx.draw_networkx(G, ax=axes['A'])\n",
    "for i in range(6):\n",
    "    axes['B'].plot(10*i+s[:, i])\n",
    "axes['B'].set_yticks([])\n",
    "for bkp in bkps[:-1]:\n",
    "    axes['B'].axvline(x=bkp, c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C. Data verifying the hypothesis of the models, with node dropping to simulate breakdowns\n",
    "\n",
    "In what follows, we work with signals verifying the two hypothesis from the [the problem formulation](#problem-formulation). Additionally, we will randomly select a (very) small number of nodes and simulate the breakdown of the corresponding sensors by setting the value of the signal lying on this node to 0 for a random time length. \n",
    "\n",
    "More formally, let denote $\\eta_{max}$ the hyper-parameter corresponding to the maximal proportion of nodes that undergo a breakdown. We denote $N^{max}_{broken} = \\lfloor \\eta_{max} * N \\rfloor$. For each signal $(y_t)_{1 \\leq t \\leq T} \\in \\mathbb{R}^{T \\times N}$ , we draw $N_{broken}$ number of nodes undergoing a breakdown in $ ~  \\mathcal{U}niform([0, N^{max}_{broken}])$. Then, for a node $i$ undergoing a breakdown we apply:\n",
    "\n",
    "\\begin{equation}\n",
    "    t_{start} ~ \\sim ~ \\mathcal{U}niform([0, T-1]), ~ t_{end} ~ \\sim ~ \\mathcal{U}niform([t_{start}, T]) \\qquad \\forall ~  t \\in [t_{start}, t_{end}] ~ y_t^i = 0\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, by increasing the value of $\\eta_{max}$ we evaluate the robustness of our cost function with respect to brutal, isolated and uncorrelated mean changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G, _ = my_gr.generate_random_er_graphs_fixed_nodes_nb(graph_rng, nx_graph_seed, n_nodes=30, target_deg=10, bandwidth_coef=0.4)\n",
    "bkps, s = my_sgn.generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "s, breakdowns = my_sgn.modify_signal_to_simulate_breakdown(s, signal_rng, n_breakdown=6, breakdown_length=300)\n",
    "\n",
    "print(\"The generated breakdowns are:\", breakdowns)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,3))\n",
    "for i in range(5):\n",
    "    ax.plot(10*i+s[:, i])\n",
    "for i, node_id in enumerate(breakdowns.keys()):\n",
    "    ax.plot(10*(i+5)+s[:, node_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D. Robustness with respect to (spatially and temporaly) independent additive white noise\n",
    "\n",
    "In this experiment, we keep using signals that verify our two hypothesis. Though we add a temporally independent white noise with scalar covariance matrix to such signal. More formally, we apply the change point detection algorithm to the signal $(y'_t)_{1 \\leq t \\leq T}$ defined by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\forall ~  t \\in [0, T] \\quad y'_t = y_t + e_t \\quad \\text{ with } \\quad e_t \\sim \\mathcal{N}_N(0, \\sigma)\n",
    "\\end{equation}\n",
    "\n",
    "We evaluate the performance of our cost function against increasing value of $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "# generating noisy signal\n",
    "G, _ = my_gr.generate_random_geographic_graph_with_gauss_kernel(graph_rng, n_nodes=20, target_degree=10)\n",
    "bkps, s = my_sgn.generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "s_noise = my_sgn.add_diagonal_white_noise(signal_rng, s, sigma=3)\n",
    "\n",
    "# loading noisy signal\n",
    "signal_path_no_noise = 'data_1/signal/within_hyp/segment_length_minimal/ER_20_nodes_deg_10_bandwidth_0.4'\n",
    "signal_no_noise = np.load(f\"{signal_path_no_noise}/{0}_signal.npy\", allow_pickle=False)\n",
    "signal_path_noise = 'data_1/signal/diago_noisy_within_hyp/large_x0.4_SNR_0_ER_20_nodes_deg_10_bandwidth_0.4'\n",
    "signal_noise = np.load(f\"{signal_path_noise}/{0}_signal.npy\", allow_pickle=False)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12,4))\n",
    "for i in range(5):\n",
    "    axes[0, 0].plot(10*i+s[:, i])\n",
    "for i in range(5):\n",
    "    axes[0, 1].plot(10*i+s_noise[:, i])\n",
    "for i in range(5):\n",
    "    axes[1, 0].plot(10*i+signal_no_noise[:400, i])\n",
    "for i in range(5):\n",
    "    axes[1, 1].plot(10*i+signal_noise[:400, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F. Robustness with respect to the graph structure: modification of the connectivity\n",
    "\n",
    "In this experiment, we do not generate the signal $(y_t)_{1 \\leq t \\leq T}$ with the graph $G$ that is used to compute the cost function. Instead, we rather utilize the laplacian matrix $L_{noisy}$ of a noisy version $G_{noisy}$ of the original graph $G$.\n",
    "\n",
    "Let denote $M = |E|$ the number of edges in $G$ and $\\eta_{edge}$ the proportion of edges that we modify. We randomly remove $M_{remove} = \\lfloor M * \\eta_{edge} / 2 \\rfloor$ from the set E and we randomly add $M_{add} = \\lfloor M * \\eta_{edge} / 2 \\rfloor$ to $E$. In both cases, we select the edges randomly, but we always check that the resulting noisy graph $G_{noisy}$ still has the same number of nodes as $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed+3)\n",
    "\n",
    "G, coord = my_gr.generate_random_geographic_graph_with_gauss_kernel(graph_rng, n_nodes=20, target_degree=10)\n",
    "G_modif = my_gr.modify_graph_connectivity_from_binary_adj_mat_2(G, 0.2, graph_rng)\n",
    "bkps, s = my_sgn.generate_rd_signal_in_hyp_with_max_tries(G_modif, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "\n",
    "original_edges = set([(min(e), max(e)) for e in G.edges()])\n",
    "new_edges = set([(min(e), max(e)) for e in G_modif.edges()])\n",
    "\n",
    "\n",
    "added = new_edges.difference(original_edges)\n",
    "withdrawn  = original_edges.difference(new_edges)\n",
    "same = new_edges.intersection(original_edges)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# original graph\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=same, edge_color='k', ax=axes[0])\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=withdrawn, edge_color='r', width=3, ax=axes[0])\n",
    "nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[0], edgelist = [])\n",
    "# modified graph\n",
    "nx.draw_networkx_edges(G_modif, pos=coord, edgelist=same, edge_color='k', ax=axes[1])\n",
    "nx.draw_networkx_edges(G_modif, pos=coord, edgelist=added, width=3, edge_color='g', ax=axes[1])\n",
    "nx.draw_networkx(G_modif, with_labels=False, pos=coord, node_size=50, ax=axes[1], edgelist = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed+3)\n",
    "\n",
    "# G, coord = generate_random_weighted_geographic_graph(graph_rng, min_n_nodes=19, max_n_nodes=20, dist_threshold=0.5)\n",
    "# # G_modif = modify_graph_connectivity(G, 0.05, graph_rng)\n",
    "# G_modif = modify_graph_connectivity_from_binary_adj_mat_2(G, 1, graph_rng)\n",
    "# bkps, s = generate_rd_signal_in_hyp_with_max_tries(G_modif, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "\n",
    "exp_id = 1\n",
    "\n",
    "ori_path = \"data_1/graphs/clean_ER_with_bandwidth/ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "adj_mat = np.load(f\"{ori_path}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "G = nx.from_numpy_array(adj_mat)\n",
    "coord = nx.spring_layout(G, seed=0)\n",
    "modif_path = \"data_1/graphs/ER_with_bd_edge_changed/ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.2\"\n",
    "modif_adj_mat = np.load(f\"{modif_path}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "modif_G = nx.from_numpy_array(modif_adj_mat)\n",
    "\n",
    "\n",
    "original_edges = set([(min(e), max(e)) for e in G.edges()])\n",
    "new_edges = set([(min(e), max(e)) for e in modif_G.edges()])\n",
    "\n",
    "\n",
    "added = new_edges.difference(original_edges)\n",
    "withdrawn  = original_edges.difference(new_edges)\n",
    "same = new_edges.intersection(original_edges)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# original graph\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=same, edge_color='k', ax=axes[0])\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=withdrawn, edge_color='r', width=3, ax=axes[0])\n",
    "nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[0], edgelist = [])\n",
    "# modified graph\n",
    "nx.draw_networkx_edges(modif_G, pos=coord, edgelist=same, edge_color='k', ax=axes[1])\n",
    "nx.draw_networkx_edges(modif_G, pos=coord, edgelist=added, width=3, edge_color='g', ax=axes[1])\n",
    "nx.draw_networkx(modif_G, with_labels=False, pos=coord, node_size=50, ax=axes[1], edgelist = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COST_FUNC_NAME_TO_PLOT_LABEL = {\n",
    "    \"statio normal cost\": \"GSN cost\",\n",
    "    \"normal cost\": \"MLE cost\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_for_plot(res_path_list):\n",
    "    metadata = {}\n",
    "    for serie_id, file_path in enumerate(res_path_list):\n",
    "        file_information = {}\n",
    "        exp_metadata = my_ut.open_json(file_path + '/experiment_metadata.json')\n",
    "        file_information[\"result folder\"] = file_path\n",
    "        file_information[\"graph folder\"] = exp_metadata[\"graph folder\"]\n",
    "        file_information[\"signal folder\"] = exp_metadata[\"signal folder\"]\n",
    "        # graph information\n",
    "        file_information[\"graph_type\"] = exp_metadata[\"graph folder\"].split('/')[-1].split('_')[0]\n",
    "        file_information[\"N=nb_nodes\"] = exp_metadata[\"graph metadata\"][\"n_nodes\"]\n",
    "        # signal information\n",
    "        file_information[\"SNR\"] = exp_metadata[\"signal metadata\"][\"SNR\"]\n",
    "        if exp_metadata[\"signal metadata\"][\"bkps_gap_hyp\"] == \"large\":\n",
    "            file_information[\"min segment length\"] = f'{exp_metadata[\"signal metadata\"][\"min_size_coef\"]}xN(N-1)/2'\n",
    "        else:\n",
    "            file_information[\"min_segment_length\"] = f'{exp_metadata[\"signal metadata\"][\"min_size_coef\"]} N'\n",
    "        # experiment information\n",
    "        file_information[\"N_exp\"] = exp_metadata[\"max id experiment subset\"]\n",
    "        metadata[serie_id] = file_information\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_subtitle_based_on_exp_metadata(exp_metadata):\n",
    "    subtitle = ''\n",
    "    for data_key in exp_metadata[0].keys():\n",
    "        if 'folder' not in data_key:\n",
    "            all_values = []\n",
    "            for serie_id in exp_metadata.keys():\n",
    "                all_values.append(exp_metadata[serie_id][data_key])\n",
    "            if len(set(all_values)) == 1:\n",
    "                subtitle = subtitle + data_key + '=' + str(all_values[0]) + ' ~ '\n",
    "    return subtitle[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin):\n",
    "    # initialization\n",
    "    stat_per_metric_per_cost_func = {}\n",
    "    for cost_func in cost_func_keys:\n",
    "        stat_per_metric_per_cost_func[cost_func] = {}\n",
    "        for metric in metrics_keys:\n",
    "            stat_per_metric_per_cost_func[cost_func][metric] = {\"mean\": [], \"std\": [], \"raw\": []}\n",
    "    # parsing metrics file to store results adequately\n",
    "    for folder_name in res_folder_list:\n",
    "        file_metric = my_ut.open_json(os.path.join(folder_name, f\"metrics_{preci_margin}.json\"))\n",
    "        for cost_func in cost_func_keys:\n",
    "            for metric in metrics_keys:\n",
    "                mean = file_metric[cost_func][metric][\"mean\"]\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"mean\"].append(mean)\n",
    "                std = file_metric[cost_func][metric][\"std\"]\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"std\"].append(std)\n",
    "                raw_str = file_metric[cost_func][metric][\"raw\"]\n",
    "                raw = my_ut.turn_str_of_list_into_list_of_float(raw_str)\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"raw\"].append(raw)\n",
    "    return stat_per_metric_per_cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_raw_scatter_to_plot(x, res_dic, met_name, cost_func, ax):\n",
    "    # scatter plot based on the raw values\n",
    "    raw = np.array(res_dic[cost_func][met_name][\"raw\"])\n",
    "    raw.flatten()\n",
    "    x_scat_val = []\n",
    "    for x_id in range(len(x)):\n",
    "        x_scat_val = x_scat_val + [x[x_id]] * len(raw[x_id])\n",
    "    ax.scatter(x=x_scat_val, y=raw, alpha=0.3, c='k', s=10)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.21\n",
    "error_kw = {\"capsize\": 5, \"elinewidth\": 0.8, \"capthick\": 2}\n",
    "\n",
    "def plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, met_name, ax, shift= 0.1, to_label=False):\n",
    "    # plot_x_coord = np.linspace(0, 1, num=(len(abscissa)+1))[:-1]\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        label = None\n",
    "        if to_label:\n",
    "            label = cost_func\n",
    "        color = colors_per_cost_func[cost_func]\n",
    "        # bar plot based on the statistics\n",
    "        x = np.asarray(abscissa) + shift*i\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.bar(x = x, height=y, yerr=y_err, label=label, width=bar_width, error_kw=error_kw, color=color)\n",
    "        # scatter plot based on the raw values\n",
    "        add_raw_scatter_to_plot(x, res_dic, met_name, cost_func, ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_plot_plus_fill_between(cost_func_keys, res_dic, abscissa, colors_per_cost_func, met_name, ax, to_label=False):\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        label_mean = None\n",
    "        label_std = None\n",
    "        if to_label:\n",
    "            label_mean = 'mean ' + COST_FUNC_NAME_TO_PLOT_LABEL[cost_func] \n",
    "            label_std = 'std ' + COST_FUNC_NAME_TO_PLOT_LABEL[cost_func]\n",
    "        color = colors_per_cost_func[cost_func]\n",
    "        x = np.asarray(abscissa)\n",
    "        y = np.asarray(res_dic[cost_func][met_name][\"mean\"])\n",
    "        y_err = np.asarray(res_dic[cost_func][met_name][\"std\"])\n",
    "        ax.plot(x, y, c=color, label=label_mean, marker='x')\n",
    "        ax.fill_between(x, y1=y-y_err, y2=y+y_err, color=color, alpha=0.4, label=label_std)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_plot_plus_fill_between_wrt_margin(cost_func_keys, per_margin_res_dic, abscissa, colors_per_cost_func, met_name, preci_margin_list, ax, to_label=False, alpha=0.4):\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        label_mean = None\n",
    "        label_std = None\n",
    "        if to_label:\n",
    "            label_mean = 'mean ' + COST_FUNC_NAME_TO_PLOT_LABEL[cost_func] \n",
    "            label_std = 'std ' + COST_FUNC_NAME_TO_PLOT_LABEL[cost_func]\n",
    "        x = np.asarray(abscissa)\n",
    "        color = colors_per_cost_func[cost_func]\n",
    "        y_arr = np.empty( (len(per_margin_res_dic[preci_margin_list[0]][cost_func][met_name][\"mean\"]), len(preci_margin_list)) )\n",
    "        y_err_arr = np.empty( (len(per_margin_res_dic[preci_margin_list[0]][cost_func][met_name][\"mean\"]), len(preci_margin_list)) )\n",
    "        for i, preci_margin in enumerate(preci_margin_list):\n",
    "            y_arr[:, i] = np.asarray(per_margin_res_dic[preci_margin][cost_func][met_name][\"mean\"])\n",
    "            y_err_arr[:, i] = np.asarray(per_margin_res_dic[preci_margin][cost_func][met_name][\"std\"])\n",
    "        y = np.mean(y_arr, axis=0)\n",
    "        y_err = np.mean(y_err_arr, axis=0)\n",
    "        ax.plot(x, y, c=color, label=label_mean, marker='x')\n",
    "        ax.fill_between(x, y1=y-y_err, y2=y+y_err, color=color, alpha=alpha, label=label_std, linewidth=0)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.02\n",
    "error_kw = {\"capsize\": 5, \"elinewidth\": 0.8, \"capthick\": 2}\n",
    "\n",
    "def plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, abscissa, plot_x_coord, colors_per_cost_func, met_name, ax, shift= 0.1, to_label=False):\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        label = None\n",
    "        if to_label:\n",
    "            label = cost_func\n",
    "        color = colors_per_cost_func[cost_func]\n",
    "        # bar plot based on the statistics\n",
    "        x = [plot_x_coord + shift*i]\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.bar(x = x, height=y, yerr=y_err, label=label, width=bar_width, error_kw=error_kw, color=color)\n",
    "        # scatter plot based on the raw values\n",
    "        add_raw_scatter_to_plot(x, res_dic, met_name, cost_func, ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_errorbar_manually(abscissa, y, yerr, marker, linestyle, linewidth, markersize, color, ax):\n",
    "    # create top and bottom lim\n",
    "    y_err_top = [y_pos + yerr_val for y_pos, yerr_val in zip(y, yerr)]\n",
    "    y_err_bottom = [y_pos - yerr_val for y_pos, yerr_val in zip(y, yerr)]\n",
    "    # add the top and bottom markers\n",
    "    ax.scatter(abscissa, y_err_top, s=markersize, c=color, marker=marker)\n",
    "    ax.scatter(abscissa, y_err_bottom, s=markersize, c=color, marker=marker)\n",
    "    # add the vertical lines\n",
    "    ax.vlines(x=abscissa, ymin=y_err_bottom, ymax=y_err_top, color=color, linewidth=linewidth, linestyle=linestyle)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_errorbar(abscissa_pos, cost_func_keys, color, markers, linestyles, met_name, res_dic, ax):\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.plot(abscissa_pos, y, c=color, marker=markers[i], markersize=7, linewidth=2, linestyle=linestyles[i])\n",
    "        add_errorbar_manually(abscissa_pos, y, y_err, marker=markers[i], linestyle=linestyles[i], linewidth=1, markersize=20, color=color, ax=ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostGraphStatioNormal(BaseCost):\n",
    "\n",
    "    \"\"\"\n",
    "    DISCLAIMER: in the current model, the mean in supposed to be known and constant over different segments,\n",
    "    so we compute its estimate over the whole available samples.\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"graph_sationary_normal_cost\"\n",
    "\n",
    "    def __init__(self, laplacian_mat) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            laplacian_mat (array): the discrete Laplacian matrix of the graph: D - W\n",
    "            where D is the diagonal matrix diag(d_i) of the node degrees and W the adjacency matrix\n",
    "        \"\"\"\n",
    "        self.graph_laplacian_mat = laplacian_mat\n",
    "        self.signal = None\n",
    "        self.gft_square_cumsum = None\n",
    "        self.gft_mean = None\n",
    "        self.min_size = laplacian_mat.shape[0]\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, signal):\n",
    "        \"\"\"Performs pre-computations for per-segment approximation cost.\n",
    "\n",
    "        NOTE: the number of dimensions of the signal and their ordering\n",
    "        must match those of the nodes of the graph.\n",
    "        The function eigh used below returns the eigenvector corresponding to \n",
    "        the ith eigenvalue in the ith column eigvect[:, i]\n",
    "\n",
    "        Args:\n",
    "            signal (array): of shape [n_samples, n_dim].\n",
    "        \"\"\"\n",
    "        self.signal = signal\n",
    "        # computation of the GFSS\n",
    "        _, eigvects = eigh(self.graph_laplacian_mat)\n",
    "        gft =  signal @ eigvects # equals signal.dot(eigvects) = eigvects.T.dot(signal.T).T\n",
    "        self.gft_mean = np.mean(gft, axis=0)\n",
    "        # computation of the per-segment cost utils\n",
    "        self.gft_square_cumsum = np.concatenate([np.zeros((1, signal.shape[1])), np.cumsum((gft - self.gft_mean[None, :])**2, axis=0)], axis=0)\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "\n",
    "        Returns:\n",
    "            float: segment cost\n",
    "        \"\"\"\n",
    "        if end - start < self.min_size:\n",
    "            raise ValueError(f'end - start shoud be higher than {self.min_size}')\n",
    "        sub_square_sum = self.gft_square_cumsum[end] - self.gft_square_cumsum[start]\n",
    "        return (end  - start) * np.sum(np.log(sub_square_sum / (end - start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import GraphicalLasso, log_likelihood\n",
    "\n",
    "class CostGraphLasso(BaseCost):\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"graph_lasso_mle_cost\"\n",
    "\n",
    "    def __init__(self, pen_mult_coef, add_small_diag=True):\n",
    "        \"\"\"Initialize the object.\n",
    "\n",
    "        Args:\n",
    "            add_small_diag (bool, optional): For signals with truly constant\n",
    "                segments, the covariance matrix is badly conditioned, so we add\n",
    "                a small diagonal matrix. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.signal = None\n",
    "        self.min_size = 2\n",
    "        self.n_samples = None\n",
    "        self.alpha_mult_coef = pen_mult_coef\n",
    "        # self.pen_coef = pen_coef\n",
    "        self.add_small_diag = add_small_diag\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, signal) :\n",
    "        \"\"\"Set parameters of the instance.\n",
    "        Args:\n",
    "            signal (array): signal of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        if signal.ndim == 1:\n",
    "            self.signal = signal.reshape(-1, 1)\n",
    "        else:\n",
    "            self.signal = signal\n",
    "        self.n_samples, self.n_dims = self.signal.shape\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "\n",
    "        Returns:\n",
    "            float: segment cost\n",
    "        \"\"\"\n",
    "        sub_signal = self.signal[start:end, :]\n",
    "        emp_cov_mat= np.cov(sub_signal.T)\n",
    "        alpha = self.alpha_mult_coef * np.sqrt(np.log(sub_signal.shape[1]) / sub_signal.shape[0])\n",
    "        gl_estimator = GraphicalLasso(alpha=alpha, assume_centered=True, covariance='precomputed').fit(emp_cov_mat)\n",
    "        return - log_likelihood(emp_cov_mat, gl_estimator.get_precision())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_station_normal_cost(signal, graph_laplacian_mat):\n",
    "    '''signal (array): of shape [n_samples, n_dim]'''\n",
    "    # computation of the graph fourier transform\n",
    "    _, eigvects = eigh(graph_laplacian_mat)\n",
    "    gft =  signal @ eigvects # equals signal.dot(eigvects) = eigvects.T.dot(signal.T).T\n",
    "    gft_mean = np.mean(gft, axis=0)\n",
    "    # computation of the per-segment cost utils\n",
    "    gft_square_cumsum = np.concatenate([np.zeros((1, signal.shape[1])), np.cumsum((gft - gft_mean[None, :])**2, axis=0)], axis=0)\n",
    "    return gft_square_cumsum.astype(np.float64)\n",
    "\n",
    "@njit\n",
    "def numba_statio_cost_func(start, end, gft_square_cumsum):\n",
    "    '''\n",
    "    Computes the cost over signal[start:end, :] where end is excluded\n",
    "\n",
    "    gft_square_cumsum (array): of shape [n_samples + 1, n_dim] \n",
    "    '''\n",
    "    sub_square_sum = gft_square_cumsum[end, :] - gft_square_cumsum[start, :]\n",
    "    return np.float64(end  - start) * np.sum(np.log(sub_square_sum / (end - start)), dtype=np.float64)\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_statio_cost(n_bkps:int, min_size:int, data: np.ndarray):\n",
    "    n_samples = data.shape[0]\n",
    "    # if no bkp to find\n",
    "    if n_bkps == 0:\n",
    "        return np.array([1000], dtype=np.int64)\n",
    "    # full partitions costs\n",
    "    full_part_cost = np.inf * np.ones((n_bkps, n_samples, n_samples), dtype=np.float64)\n",
    "    # compute the segment cost with no bpk, for admissible segment only\n",
    "    for start in range(0, n_samples-min_size):\n",
    "        # until n_samples + 1 because the call to cost_function(start, end, data) computes over [y_0, ... y_{n-1}] (remember data.shape[0] = n_samples + 1)\n",
    "        for end in range(start+min_size, n_samples):  \n",
    "            full_part_cost[0, start, end] = numba_statio_cost_func(start, end, data)\n",
    "    # compute the cost of the possible higher order partitions \n",
    "    for bkp_order in range(1, n_bkps):\n",
    "        min_multi_seg_length = (bkp_order + 1) * min_size\n",
    "        for start in range(0, n_samples-min_multi_seg_length):\n",
    "            for end in range(start + min_multi_seg_length, n_samples):\n",
    "                min_size_left_seg = min_multi_seg_length - min_size\n",
    "                full_part_cost[bkp_order, start, end] = np.min(full_part_cost[bkp_order-1, start, start+min_size_left_seg:end-min_size+1] + full_part_cost[0, start+min_size_left_seg:end-min_size+1, end])\n",
    "    # successively pick the bkps from the right-end of the whole signal\n",
    "    bkps = np.int64(n_samples-1) * np.ones(n_bkps+1, dtype=np.int64)\n",
    "    for bkp_id in range(n_bkps, 0, -1):\n",
    "        min_multi_seg_length = np.int64(bkp_id * min_size) \n",
    "        bkp_right = bkps[bkp_id]\n",
    "        bkp_left = min_multi_seg_length + np.argmin(full_part_cost[bkp_id-1, 0, min_multi_seg_length:bkp_right-min_size+1] + full_part_cost[0, min_multi_seg_length:bkp_right-min_size+1, bkp_right])\n",
    "        bkps[bkp_id-1] = bkp_left\n",
    "    return bkps\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_statio_cost_2_optim(n_bkps:int, min_size:int, data: np.ndarray):\n",
    "    # path_mat[n, K] avec n --> [y_0, ... y_{n-1}], K : n_bkps\n",
    "\n",
    "    # initialization \n",
    "    n_samples = data.shape[0] - 1\n",
    "    path_mat = np.empty((n_samples+1, n_bkps+1), dtype=np.int32)\n",
    "    path_mat[:, 0] = 0\n",
    "    path_mat[0, :] = -1\n",
    "    sum_of_cost_mat = np.full((n_samples+1, n_bkps+1),  fill_value=np.inf, dtype=np.float64)\n",
    "    sum_of_cost_mat[0, :] = 0\n",
    "\n",
    "    # forward computation\n",
    "    for end in range(min_size, n_samples+1):\n",
    "        sum_of_cost_mat[end, 0] = numba_statio_cost_func(0, end, data)\n",
    "        # consistent because our cost functions compute the costs over [start, end[\n",
    "        max_admissible_n_bkp = floor(end/min_size) - 1\n",
    "        for k_bkps in range(1, min(max_admissible_n_bkp+1, n_bkps+1)):\n",
    "            soc_optim = np.inf\n",
    "            soc_argmin = -1\n",
    "            for mid in range(min_size*k_bkps, end - min_size + 1): \n",
    "                soc = sum_of_cost_mat[mid, k_bkps-1] + numba_statio_cost_func(mid, end, data)\n",
    "                if soc < soc_optim:\n",
    "                    soc_argmin = mid\n",
    "                    soc_optim = soc\n",
    "            sum_of_cost_mat[end, k_bkps] = soc_optim\n",
    "            path_mat[end, k_bkps] = soc_argmin\n",
    "\n",
    "    # backtracking\n",
    "    bkps = np.full((n_bkps+1), fill_value=n_samples)\n",
    "    for k_bkps in range(n_bkps, 0, -1):\n",
    "        bkps[k_bkps-1] = path_mat[bkps[k_bkps], k_bkps]\n",
    "    \n",
    "    return bkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import slogdet\n",
    "\n",
    "@njit\n",
    "def standard_normal_cost_func(start, end, signal):\n",
    "    '''signal (array): of shape [n_samples, n_dim]'''\n",
    "    sub = signal[start:end, :]\n",
    "    cov = np.cov(sub.T)\n",
    "    cov += 1e-6 * np.eye(signal.shape[1])\n",
    "    _, val = slogdet(cov)\n",
    "    return np.float64(val * (end - start))\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_mle_standard_cost(n_bkps:int, min_size:int, signal: np.ndarray):\n",
    "    n_samples = signal.shape[0]\n",
    "    # if no bkp to find\n",
    "    if n_bkps == 0:\n",
    "        return np.array([1000], dtype=np.int64)\n",
    "    # full partitions costs\n",
    "    full_part_cost = np.inf * np.ones((n_bkps, n_samples, n_samples), dtype=np.float64)\n",
    "    # compute the segment cost with no bpk, for admissible segment only\n",
    "    for start in range(0, n_samples-min_size+1):\n",
    "        # until n_samples + 1 because the call to cost_function(start, end, data) computes over [y_0, ... y_{n-1}] \n",
    "        for end in range(start+min_size, n_samples+1):  \n",
    "            full_part_cost[0, start, end] = standard_normal_cost_func(start, end, signal)\n",
    "    # compute the cost of the possible higher order partitions \n",
    "    for bkp_order in range(1, n_bkps):\n",
    "        min_multi_seg_length = (bkp_order + 1) * min_size\n",
    "        for start in range(0, n_samples-min_multi_seg_length):\n",
    "            for end in range(start + min_multi_seg_length, n_samples):\n",
    "                min_size_left_seg = min_multi_seg_length - min_size\n",
    "                full_part_cost[bkp_order, start, end] = np.min(full_part_cost[bkp_order-1, start, start+min_size_left_seg:end-min_size+1] + full_part_cost[0, start+min_size_left_seg:end-min_size+1, end])\n",
    "    # successively pick the bkps from the right-end of the whole signal\n",
    "    bkps = np.int64(n_samples) * np.ones(n_bkps+1, dtype=np.int64)\n",
    "    for bkp_id in range(n_bkps, 0, -1):\n",
    "        min_multi_seg_length = np.int64(bkp_id * min_size) \n",
    "        bkp_right = bkps[bkp_id]\n",
    "        bkp_left = min_multi_seg_length + np.argmin(full_part_cost[bkp_id-1, 0, min_multi_seg_length:bkp_right-min_size+1] + full_part_cost[0, min_multi_seg_length:bkp_right-min_size+1, bkp_right])\n",
    "        bkps[bkp_id-1] = bkp_left\n",
    "    return bkps\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_mle_standard_cost_2_optim(n_bkps:int, min_size:int, signal):\n",
    "    # path_mat[n, K] avec n --> [y_0, ... y_{n-1}] (very important to understand indexing) , K : n_bkps\n",
    "    # sum_of_cost_mat[n, K]: best cost for signal until sample n with K bkps\n",
    "\n",
    "    # initialization \n",
    "    n_samples = signal.shape[0]\n",
    "    path_mat = np.empty((n_samples+1, n_bkps+1), dtype=np.int32)\n",
    "    path_mat[:, 0] = 0\n",
    "    path_mat[0, :] = -1\n",
    "    sum_of_cost_mat = np.full((n_samples+1, n_bkps+1),  fill_value=np.inf, dtype=np.float64)\n",
    "    sum_of_cost_mat[0, :] = 0\n",
    "\n",
    "    # pre-computation, to optimize jit processing\n",
    "    statio_segment_cost = np.full((n_samples+1, n_samples+1), fill_value=np.inf, dtype=np.float64)\n",
    "    for start in range(0, n_samples-min_size+1):\n",
    "        for end in range(start+min_size, n_samples+1):  \n",
    "            statio_segment_cost[start, end] = standard_normal_cost_func(start, end, signal)\n",
    "\n",
    "    # forward computation\n",
    "    for end in range(min_size, n_samples+1):\n",
    "        sum_of_cost_mat[end, 0] = statio_segment_cost[0, end]\n",
    "        # consistent because our cost functions compute the costs over [start, end[\n",
    "        max_admissible_n_bkp = floor(end/min_size) - 1\n",
    "        for k_bkps in range(1, min(max_admissible_n_bkp+1, n_bkps+1)):\n",
    "            soc_optim = np.inf\n",
    "            soc_argmin = -1\n",
    "            for mid in range(min_size*k_bkps, end - min_size + 1):\n",
    "                soc = sum_of_cost_mat[mid, k_bkps-1] + statio_segment_cost[mid, end]\n",
    "                if soc < soc_optim:\n",
    "                    soc_argmin = mid\n",
    "                    soc_optim = soc\n",
    "            sum_of_cost_mat[end, k_bkps] = soc_optim\n",
    "            path_mat[end, k_bkps] = soc_argmin\n",
    "\n",
    "    # backtracking\n",
    "    bkps = np.full((n_bkps+1), fill_value=n_samples)\n",
    "    for k_bkps in range(n_bkps, 0, -1):\n",
    "        bkps[k_bkps-1] = path_mat[bkps[k_bkps], k_bkps]\n",
    "    \n",
    "    return bkps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph and signals generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 10 # 1000\n",
    "GRAPH_SEED = 1\n",
    "N_NODES = 20\n",
    "TARGET_DEGREE = 10\n",
    "K_NEIGHBOUR = 8\n",
    "ER_BANDWIDTH = 0.4\n",
    "EDGE_PROP_TO_MODIF = 0.6\n",
    "INITIAL_GRAPH_PATH =  'data_1/test/graphs/clean_exp_geo'  # \"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "INIT_NAME = 'exp_geo_20_nodes_av_deg_10_3' # \"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "to_modify_graph_path = os.path.join(INITIAL_GRAPH_PATH, INIT_NAME)\n",
    "\n",
    "# logging\n",
    "NAME =  INIT_NAME  #+ '_' + f\"edge_prop_{EDGE_PROP_TO_MODIF}\"  #f\"ER_{N_NODES}_nodes_deg_{TARGET_DEGREE}_bandwidth_{ER_BANDWIDTH}_edge_prop_{EDGE_PROP_TO_MODIF}\"\n",
    "data_dir = os.path.join(INITIAL_GRAPH_PATH, NAME) #  \"data_1/graphs/ER_with_bd_edge_changed/\" + NAME\n",
    "graphs_desc = f\"Standard exp geo graphs for testing purpose before rectorization.\"\n",
    "# graphs_desc = f\"Graphs fetched from {to_modify_graph_path}. ER graphs with fixed number of nodes. The edge probability is randomly drawn based on the target degree but also using a bandwidth parameter to allow for more diversity. Otherwise, for a given number of nodes and edge probability, the generated graphs would always be the same based on the networkx implementation. The connectivity of the graphs is modified: we randomly remove half of the edges obtained when randomly selecting some with the given edge proportion, and we randomly add the same amount of edges.\"\n",
    "graph_gen_func = lambda rng : my_gr.generate_random_geographic_graph_with_gauss_kernel(rng, n_nodes=N_NODES, target_degree=TARGET_DEGREE)\n",
    "# graph_modif_func = lambda exp_id : load_modify_connec_and_store_graph(to_modify_graph_path, exp_id, EDGE_PROP_TO_MODIF, graph_rng, data_dir)\n",
    "graphs_metadata = {\"datetime\": now, \"description\": graphs_desc, \"commit hash\": my_ut.get_git_head_short_hash(), \"graph_func\": my_gr.generate_random_geographic_graph_with_gauss_kernel.__name__, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": \"index\", \"n_nodes\": N_NODES, \"target_degree\": TARGET_DEGREE} #, \"bandwidth coefficient\": ER_BANDWIDTH}\n",
    "# graphs_metadata = {\"datetime\": now, \"description\": graphs_desc, \"commit hash\": get_git_head_short_hash(), \"graph_modif_func\": load_modify_connec_and_store_graph.__name__, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": \"index\", \"n_nodes\": N_NODES, \"target_degree\": TARGET_DEGREE, \"bandwidth coefficient\": ER_BANDWIDTH, \"modified edge proportion\": EDGE_PROP_TO_MODIF}\n",
    "\n",
    "# output formatting\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=False)\n",
    "graphs_metadata = my_ut.turn_all_list_of_dict_into_str(graphs_metadata)\n",
    "my_ut.create_parent_and_dump_json(data_dir, \"00_graphs_metadata.json\", graphs_metadata, indent=4)\n",
    "\n",
    "# graph generation\n",
    "for exp_id in range(N_EXP):\n",
    "    # graph_modif_func(exp_id)\n",
    "    G, coords = graph_gen_func(graph_rng)\n",
    "    my_ut.save_graph(G, f\"{data_dir}/{exp_id}_mat_adj.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal and bkps generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_FOLDER =  \"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "GRAPH_FOLDER_NAME =  \"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "GRAPH_PATH = os.path.join(GRAPH_FOLDER, GRAPH_FOLDER_NAME)\n",
    "SIGNAL_SEED = 3\n",
    "DIAG_COV_MAX = 1\n",
    "N_SAMPLES = 1000\n",
    "MIN_SEGMENT_LENGTH_COEF = 0.4\n",
    "BKPS_GAP_CONSTRAINT: my_ut.seg_length = \"large\" \n",
    "N_BKPS = 4\n",
    "# NB_BREAKDOWN = 6\n",
    "# BREAKDOWN_LENGTH_LIST = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "SNR_LIST = [20]\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "\n",
    "breakdowns_dic = {}\n",
    "\n",
    "for SNR in SNR_LIST:\n",
    "\n",
    "    sigma_noise = DIAG_COV_MAX / ( 10**(SNR / 10) )\n",
    "    \n",
    "    # logging\n",
    "    NAME = f\"{N_BKPS}_bkps_{BKPS_GAP_CONSTRAINT}_x{MIN_SEGMENT_LENGTH_COEF}_SNR_{round(SNR, 4)}_{N_SAMPLES}_samples\" + \"_\" + GRAPH_FOLDER_NAME     #  _NBbd_{NB_BREAKDOWN}_bklength_{BREAKDOWN_LENGTH}\"    #\n",
    "    data_dir =  f\"data_1/signal/within_hyp/SNR_20_graph_lasso_calibration/{NAME}\" \n",
    "    signal_desc = \"Data verifying the two hypothesis, with a given number of bkps. The purpose of these signals is to check graph lasso calibration in cases where bkps are not evenly spaced.\"\n",
    "    signal_gen_func = lambda G : my_sgn.generate_rd_signal_in_hyp_with_fixed_min_size(G, signal_rng, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, min_size_coef=MIN_SEGMENT_LENGTH_COEF, diag_cov_max=DIAG_COV_MAX, n_bkps=N_BKPS)\n",
    "    signal_modif_func = lambda s : my_sgn.add_diagonal_white_noise(signal_rng, s, sigma=sigma_noise)\n",
    "    # signal_modif_func2 = lambda s : my_sgn.modify_signal_to_simulate_breakdown(s, signal_rng, n_breakdown=NB_BREAKDOWN, breakdown_length=BREAKDOWN_LENGTH)\n",
    "    signal_metadata = {\"datetime\": now, \"description\": signal_desc, \"commit hash\": my_ut.get_git_head_short_hash(), \"graph_folder\": GRAPH_PATH, \"signal_seed\": SIGNAL_SEED, \"signal_gen_function\": my_sgn.draw_bkps_with_gap_constraint.__name__, \"signal_modif_func\": my_sgn.add_diagonal_white_noise.__name__, \"n_samples\": N_SAMPLES, \"diag_cov_max\": DIAG_COV_MAX, \"min_size_coef\": MIN_SEGMENT_LENGTH_COEF, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"SNR\": SNR } #, \"nb brakdown\": NB_BREAKDOWN, \"breakdown length\": BREAKDOWN_LENGTH}\n",
    "\n",
    "    # output formatting\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=False)\n",
    "    signal_metadata = my_ut.turn_all_list_of_dict_into_str(signal_metadata)\n",
    "    my_ut.create_parent_and_dump_json(data_dir, \"00_signal_metadata.json\", signal_metadata, indent=4)\n",
    "\n",
    "    # signal generation\n",
    "    for exp_id in range(len(os.listdir(GRAPH_PATH)) - 1):\n",
    "        adj_mat = np.load(f\"{GRAPH_PATH}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "        G = nx.from_numpy_array(adj_mat)\n",
    "        bkps, signal = signal_gen_func(G)\n",
    "        signal_modif = signal_modif_func(signal)\n",
    "        # signal_modif2, breakdowns = signal_modif_func2(signal_modif)\n",
    "        # breakdowns_dic[exp_id] = breakdowns\n",
    "        my_ut.save_signal_and_bkps(signal_modif, bkps, data_dir, str(exp_id))\n",
    "\n",
    "    # my_ut.create_parent_and_dump_json(data_dir, \"00_breakdowns_dict.json\", breakdowns_dic, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search algorithms: running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_numba_statio_normal_cost(G: nx.Graph, signal: np.ndarray, gt_bkps: List[int], statio_results: dict, exp_id: int):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    graph_lapl_mat = nx.laplacian_matrix(G).toarray().astype(np.float64)\n",
    "    ###############################################################\n",
    "    # graph_lapl_mat = np.eye(signal.shape[1])\n",
    "    ###############################################################\n",
    "    gft_square_cumsum = init_station_normal_cost(signal, graph_lapl_mat)\n",
    "    statio_bkps = numba_cpd_dynprog_statio_cost_2_optim(len(gt_bkps)-1, signal.shape[1], gft_square_cumsum)\n",
    "    statio_bkps = [int(bkp) for bkp in statio_bkps]\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "    statio_results[exp_id][\"gt\"] = gt_bkps\n",
    "    statio_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_numba_standard_mle_normal_cost(signal: np.ndarray, gt_bkps: List[int], normal_results: dict, exp_id:int):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    normal_bkps = numba_cpd_dynprog_mle_standard_cost_2_optim(len(gt_bkps) - 1, signal.shape[1], signal)\n",
    "    normal_bkps = [int(bkp) for bkp in normal_bkps]\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "    normal_results[exp_id][\"gt\"] = gt_bkps\n",
    "    normal_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statio_normal_cost(G: nx.Graph, signal: np.ndarray, gt_bkps: List[int], statio_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "    statio_results[exp_id][\"gt\"] = gt_bkps\n",
    "    statio_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_standard_mle_normal_cost(signal: np.ndarray, gt_bkps: List[int], normal_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "    normal_results[exp_id][\"gt\"] = gt_bkps\n",
    "    normal_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph_lasso_mle_cost(signal: np.ndarray, pen_mult_coef: float, gt_bkps: List[int], graph_lasso_results: dict, min_seg_length_coef: float):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        lasso_cost = CostGraphLasso(pen_mult_coef)\n",
    "        algo_lasso = rpt.Dynp(custom_cost=lasso_cost, jump=1, min_size=min_seg_length_coef*signal.shape[1]).fit(signal)\n",
    "        lasso_bkps = algo_lasso.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    graph_lasso_results[exp_id] = {}\n",
    "    graph_lasso_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    graph_lasso_results[exp_id][\"pred\"] = lasso_bkps\n",
    "    graph_lasso_results[exp_id][\"gt\"] = gt_bkps\n",
    "    graph_lasso_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME =  \"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "GRAPH_PATH = \"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "GRAPH_NAME =  NAME  #+ f\"_edge_prop_{EDGE_PROP}\" #\"exp_geo_20_nodes_av_deg_10\" #\"ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.05\" \n",
    "SIGNAL_PATH = \"data_1/signal/within_hyp/SNR_20_graph_lasso_calibration\" #\"data_1/signal/within_hyp/SNR_20_varying_segment_length\"\n",
    "MAX_ID_SUBSET = 20\n",
    "RESULT_DIR = \"results_1/synthetic/within_hypothesis/noisy_varying_segment_length/SNR_20_test_graph_lasso\"\n",
    "SNR = 20\n",
    "MIN_SEGMENT_LENGTH_COEF = 0.7\n",
    "MIN_SEGMENT_LENGTH_COEF_CPD_FROM_MIN = 1\n",
    "BKPS_GAP_CONSTRAINT = \"large\"\n",
    "PEN_COEF_VAL_LIST = [0.5, 0.7, 0.8, 1.0, 1.3, 2, 4, 8, 10]\n",
    "# ALPHA_MULT_COEF_LIST = [0.005, 0.01, 0.1, 1, 10, 100]\n",
    "# NB_BREAKDOWN = 6\n",
    "# BREAKDOWN_LENGTH_LIST = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "# SNR_LIST = [-5, 5, 15]\n",
    "# SEG_LENGTH_COEF_LIST = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "# EDGE_PROP_MODIF_LIST = [0.03, 0.05, 0.08, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "OTHER_GRAPH_SEED = 1\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# logging\n",
    "graph_path = os.path.join(GRAPH_PATH, GRAPH_NAME)\n",
    "graph_metadata = my_ut.open_json(f\"{graph_path}/00_graphs_metadata.json\")\n",
    "seg_length_hyp = \"minimal\"\n",
    "graph_rng = np.random.default_rng(OTHER_GRAPH_SEED)\n",
    "\n",
    "for PEN_COEF_MULT in PEN_COEF_VAL_LIST:\n",
    "    \n",
    "    SIGNAL_NAME = f\"{BKPS_GAP_CONSTRAINT}_x{MIN_SEGMENT_LENGTH_COEF}_SNR_{round(SNR, 4)}\" + \"_\" + GRAPH_NAME\n",
    "    signal_path = os.path.join(SIGNAL_PATH, SIGNAL_NAME)\n",
    "    signal_metadata = my_ut.open_json(f\"{signal_path}/00_signal_metadata.json\")\n",
    "    \n",
    "    RESULT_NAME = f'MIN_SEGMENT_LENGTH_COEF_CPD_FROM_MIN_{MIN_SEGMENT_LENGTH_COEF_CPD_FROM_MIN}_pen_coef_gridsearch_from_covcp_paper'\n",
    "    final_name = SIGNAL_NAME + \"_\" + RESULT_NAME\n",
    "    results_dir = os.path.join(RESULT_DIR, final_name)\n",
    "\n",
    "    exp_desc = \"Test of the graphical Lasso cost function: grid seach over the L1 penalty coefficient. The grid search is applied to the penalty coefficient, which is is computed as in the covcp paper but we also apply a multiplicative coefficient.\"\n",
    "    experiment_metadata = {\"datetime\": now, \"description\": exp_desc, \"commit hash\": my_ut.get_git_head_short_hash(), \"graph folder\": graph_path, \"graph metadata\": graph_metadata, \"signal folder\": SIGNAL_PATH + '/' + SIGNAL_NAME, \"signal metadata\": signal_metadata, \"min segment length hypothesis\": seg_length_hyp, \"max id experiment subset\": MAX_ID_SUBSET, \"lasso penalty coefficient\": \"from covcp\", \"multplicative coefficient of the segment length for the cp algo from the minimal size\": MIN_SEGMENT_LENGTH_COEF_CPD_FROM_MIN}\n",
    "\n",
    "    # output formatting\n",
    "    # statio_results = {}\n",
    "    # normal_results = {}\n",
    "    lasso_results = {}\n",
    "\n",
    "    # running CPD algorithms\n",
    "    for exp_id in tqdm(range(MAX_ID_SUBSET), desc='Running experiment...'):\n",
    "        exp_id = str(exp_id)\n",
    "        G, signal, gt_bkps, min_size = my_ut.load_data(graph_path, signal_path, exp_id, seg_length_hyp)\n",
    "        # run_numba_statio_normal_cost(G, signal, gt_bkps, statio_results, exp_id)\n",
    "        # run_numba_standard_mle_normal_cost(signal, gt_bkps, normal_results, exp_id)\n",
    "        run_graph_lasso_mle_cost(signal, PEN_COEF_MULT, gt_bkps, lasso_results, min_seg_length_coef=MIN_SEGMENT_LENGTH_COEF_CPD_FROM_MIN)\n",
    "\n",
    "    # my_ut.create_parent_and_dump_json(results_dir, \"statio_pred.json\", my_ut.turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "    # my_ut.create_parent_and_dump_json(results_dir, \"normal_pred.json\", my_ut.turn_all_list_of_dict_into_str(normal_results), indent=4)\n",
    "    my_ut.create_parent_and_dump_json(results_dir, f\"lasso_pred_pencoefmult_{PEN_COEF_MULT}.json\", my_ut.turn_all_list_of_dict_into_str(lasso_results), indent=4)\n",
    "\n",
    "my_ut.create_parent_and_dump_json(results_dir, \"experiment_metadata.json\", my_ut.turn_all_list_of_dict_into_str(experiment_metadata), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME =  \"ER_20_nodes_deg_10_bandwidth_0.4\" #ER_20_nodes_deg_10_bandwidth_0.4\n",
    "GRAPH_NAME =  NAME  #+ '_' + \"edge_prop_0.2\" #\"exp_geo_20_nodes_av_deg_10\" #\"ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.05\" \n",
    "GRAPH_PATH =   \"data_1/graphs/clean_ER_with_bandwidth\"  #\"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "SIGNAL_PATH = \"data_1/signal/within_hyp/varying_segment_length\" #\"data_1/signal/within_hyp/noisy_varying_segment_length\"\n",
    "SIGNAL_NAME =  \"large_x0.4_SNR_inf\" + '_' + NAME #\"large_x0.1_SNR_10\" + '_' + NAME\n",
    "MAX_ID_SUBSET = 1000\n",
    "RESULT_DIR =  \"results_1/synthetic/test/results_consistency_tests/addi_gaussian_noise_plus_connectivity\" #\"results_1/synthetic/ablation_study/large_x0.1\"\n",
    "RESULT_NAME = \"no_noise\"  #\"er_other_graph_same_folder_2\"\n",
    "LASSO_ALPHA = 0.1\n",
    "OTHER_GRAPH_SEED = 1\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "final_name = SIGNAL_NAME + \"_\" + RESULT_NAME\n",
    "results_dir = os.path.join(RESULT_DIR, final_name)\n",
    "\n",
    "# logging\n",
    "graph_path = os.path.join(GRAPH_PATH, GRAPH_NAME)\n",
    "signal_path = os.path.join(SIGNAL_PATH, SIGNAL_NAME)\n",
    "graph_metadata = my_ut.open_json(f\"{graph_path}/00_graphs_metadata.json\")\n",
    "signal_metadata = my_ut.open_json(f\"{signal_path}/00_signal_metadata.json\")\n",
    "seg_length_hyp = \"minimal\"\n",
    "graph_rng = np.random.default_rng(OTHER_GRAPH_SEED)\n",
    "\n",
    "\n",
    "exp_desc = \"Test to check consistency between results from A. results_1/synthetic/within_hypo_graph_connec_modif/SNR_10/large_x0.4/large_x0.4_SNR_10_ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.2, B. results_1/synthetic/within_hypo_diago_white_noise/large_x0.4_varying_SNR/large_x0.4_SNR_10_ER_20_nodes_deg_10_bandwidth_0.4_80_exp and C. results_1/synthetic/within_hypo_graph_connec_modif/SNR_inf/large_x0.4/large_x0.4_SNR_inf_ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.2\"  #\"Ablation study: another ER graph from the same folder is used to instantiate the cost function.\"\n",
    "experiment_metadata = {\"datetime\": now, \"description\": exp_desc, \"commit hash\": my_ut.get_git_head_short_hash(), \"graph folder\": graph_path, \"graph metadata\": graph_metadata, \"signal folder\": SIGNAL_PATH + '/' + SIGNAL_NAME, \"signal metadata\": signal_metadata, \"min segment length hypothesis\": seg_length_hyp, \"max id experiment subset\": MAX_ID_SUBSET}\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "# normal_results = {}\n",
    "# lasso_results = {}\n",
    "\n",
    "# running CPD algorithms\n",
    "for exp_id in tqdm(range(MAX_ID_SUBSET), desc='Running experiment...'):\n",
    "    exp_id = str(exp_id)\n",
    "    G, signal, gt_bkps, min_size = my_ut.load_data(graph_path, signal_path, exp_id, seg_length_hyp)\n",
    "    run_numba_statio_normal_cost(G, signal, gt_bkps, statio_results, exp_id)\n",
    "    # run_numba_standard_mle_normal_cost(signal, gt_bkps, normal_results, exp_id)\n",
    "    # run_graph_lasso_mle_cost(signal, gt_bkps, LASSO_ALPHA, lasso_results)\n",
    "\n",
    "my_ut.create_parent_and_dump_json(results_dir, \"experiment_metadata.json\", my_ut.turn_all_list_of_dict_into_str(experiment_metadata), indent=4)\n",
    "my_ut.create_parent_and_dump_json(results_dir, \"statio_pred.json\", my_ut.turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "# my_ut.create_parent_and_dump_json(results_dir, \"normal_pred.json\", my_ut.turn_all_list_of_dict_into_str(normal_results), indent=4)\n",
    "# create_parent_and_dump_json(results_dir, F\"lasso_pred_alpha_{LASSO_ALPHA}.json\", turn_all_list_of_dict_into_str(lasso_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real data experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading graph\n",
    "BCI2000_graph_path = \"data/real_datasets/eeg-motor-movementimagery-dataset-1.0.0/graphs/KNN_4_64_ch_graph_mat_adj_order_signal_header.npy\"\n",
    "BCI2000_64_ch_adj_mat = np.load(BCI2000_graph_path, allow_pickle=False)\n",
    "BCI2000_64_ch_G = nx.from_numpy_array(BCI2000_64_ch_adj_mat)\n",
    "\n",
    "data_folder = 'data/real_datasets/eeg-motor-movementimagery-dataset-1.0.0/processed_signals'\n",
    "folder_name = \"filtered_0.5-40_order_3_subsampled_8\"\n",
    "volunteer_id = 'S007'\n",
    "exp_ids = ['04', '05', '06']\n",
    "data_path = data_folder + '/' + folder_name\n",
    "\n",
    "# output formatting\n",
    "results_path = \"results_1/real_data/eeg-motor-movement\"\n",
    "exp_name= f'{volunteer_id}_exp_{\"-\".join(exp_ids)}'\n",
    "results_dir = results_path + '/' + folder_name + '/' + exp_name\n",
    "# statio_results = {}\n",
    "normal_results = {}\n",
    "\n",
    "# logging\n",
    "exp_metadata = {}\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "exp_metadata['date time'] = now\n",
    "exp_metadata['signal folder'] = data_path\n",
    "exp_metadata['volunteer id'] = volunteer_id\n",
    "exp_metadata['exp_ids'] = exp_ids\n",
    "exp_metadata['graph file'] = BCI2000_graph_path\n",
    "signal_metadata = my_ut.open_json(f\"{data_path}/signals_metadata.json\")\n",
    "pre_processing = {}\n",
    "pre_processing[\"filtering\"] = signal_metadata[\"filtering\"]\n",
    "pre_processing[\"subsampling frequency\"] = signal_metadata[\"subsampling frequency\"]\n",
    "exp_metadata['signal pre-processing'] = pre_processing\n",
    "\n",
    "for i, exp_id in enumerate(exp_ids):\n",
    "    \n",
    "    print(f\"Running exp {exp_id}\")\n",
    "\n",
    "    # loading signal and bkps\n",
    "    signal_path = f'{data_path}/volunteer{volunteer_id}_exp{exp_id}'\n",
    "    gt_bkps = my_ut.open_json(f\"{signal_path}_bkps.json\")\n",
    "    signal = np.load(f\"{signal_path}_signal.npy\", allow_pickle=False)\n",
    "    signal = signal.T\n",
    "\n",
    "    # running CPD algorithms\n",
    "    # run_numba_statio_normal_cost(BCI2000_64_ch_G, signal, gt_bkps, statio_results, exp_id)\n",
    "    run_numba_standard_mle_normal_cost(signal, gt_bkps, normal_results, exp_id)\n",
    "\n",
    "# my_ut.create_parent_and_dump_json(results_dir, \"experiment_metadata.json\", my_ut.turn_all_list_of_dict_into_str(exp_metadata), indent=4)\n",
    "# my_ut.create_parent_and_dump_json(results_dir, \"statio_pred.json\", my_ut.turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "my_ut.create_parent_and_dump_json(results_dir, \"normal_pred.json\", my_ut.turn_all_list_of_dict_into_str(normal_results), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REAL DATA ADAPTED\n",
    "#####################\n",
    "\n",
    "PRECI_RECALL_MARGIN_LIST = x_tick_labels =  list(range(5, 205, 5))\n",
    "res_folder_root = \"results_1/real_data/eeg-motor-movement/filtered_0.5-40_order_3_subsampled_4\"\n",
    "file_names = os.listdir(res_folder_root)\n",
    "PRED_FOLDER = [os.path.join(res_folder_root, file_name) for file_name in file_names]  #'.txt' not in file_name]\n",
    "# PRED_FOLDER = [res_folder_root]\n",
    "\n",
    "for pred_dir in PRED_FOLDER:\n",
    "\n",
    "    # fetching predictions\n",
    "    data_stats = my_ut.open_json(f\"{pred_dir}/experiment_metadata.json\")\n",
    "    data_stats = my_ut.open_json(f\"{pred_dir}/experiment_metadata.json\")\n",
    "    statio_pred_dic = my_ut.open_json(f\"{pred_dir}/statio_pred.json\")\n",
    "    # normal_pred_dic = my_ut.open_json(f\"{pred_dir}/normal_pred.json\")\n",
    "    # assert list(normal_pred_dic.keys()) == list(statio_pred_dic.keys())\n",
    "\n",
    "    # output formatting\n",
    "    metrics_dic = {}\n",
    "    metrics_dic[\"pred_path\"] = pred_dir\n",
    "    metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "\n",
    "    for PRECI_RECALL_MARGIN in PRECI_RECALL_MARGIN_LIST:\n",
    "        \n",
    "        metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "        statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "        normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "        for exp_id in statio_pred_dic.keys():\n",
    "            # compute metrics\n",
    "            statio_pred_bkps = my_ut.turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "            # normal_pred_bkps = my_ut.turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "            gt_bkps = my_ut.turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"gt\"])\n",
    "            my_res.compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "            # my_res.compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "            # add time values\n",
    "            statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "            # normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "        # results post-precessing and saving\n",
    "        full_results = {\"statio normal cost\": statio_results} #, \"normal cost\": normal_results}\n",
    "        full_results = my_res.compute_and_add_stat_on_metrics(full_results)\n",
    "        full_results[\"metadata\"] = metrics_dic\n",
    "        full_results = my_ut.turn_all_list_of_dict_into_str(full_results)\n",
    "        my_ut.create_parent_and_dump_json(pred_dir + '/metrics', f'metrics_{PRECI_RECALL_MARGIN}.json', full_results, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruptures.metrics import precision_recall\n",
    "\n",
    "gtttt = [10, 20, 30, 40, 50, 60, 100]\n",
    "preddd = [12, 22, 32, 42, 52, 62, 100]\n",
    "preci_margin = [5, 10, 20, 30, 40, 50, 60, 80]\n",
    "\n",
    "for margin in preci_margin:\n",
    "    preci, recall = precision_recall(gtttt, preddd, margin)\n",
    "    print(f'For margin={margin}:\\t preci={round(preci, 2)},  recall={round(recall, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SYNTHETIC DATA ADAPTED\n",
    "###########################\n",
    "\n",
    "PRECI_RECALL_MARGIN = 5\n",
    "res_folder_root = \"results_1/synthetic/within_hypo_censor_breakdown/SNR_20_large_x0.4/varying_breakdown_length\"\n",
    "file_names = os.listdir(res_folder_root)\n",
    "PRED_FOLDER = [os.path.join(res_folder_root, file_name) for file_name in file_names]  #'.txt' not in file_name]\n",
    "# PRED_FOLDER = [res_folder_root]\n",
    "\n",
    "for pred_dir in PRED_FOLDER:\n",
    "\n",
    "    # fetching predictions\n",
    "    data_stats = my_ut.open_json(f\"{pred_dir}/experiment_metadata.json\")\n",
    "    statio_pred_dic = my_ut.open_json(f\"{pred_dir}/statio_pred.json\")\n",
    "    normal_pred_dic = my_ut.open_json(f\"{pred_dir}/normal_pred.json\")\n",
    "    assert list(normal_pred_dic.keys()) == list(statio_pred_dic.keys())\n",
    "\n",
    "    # output formatting\n",
    "    metrics_dic = {}\n",
    "    metrics_dic[\"pred_path\"] = pred_dir\n",
    "    metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "    metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "    statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "    normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "    for exp_id in statio_pred_dic.keys():\n",
    "        # compute metrics\n",
    "        statio_pred_bkps = my_ut.turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "        normal_pred_bkps = my_ut.turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "        gt_bkps = my_ut.turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"gt\"])\n",
    "        my_res.compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "        my_res.compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "        # add time values\n",
    "        statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "        normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "    # results post-precessing and saving\n",
    "    full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "    full_results = my_res.compute_and_add_stat_on_metrics(full_results)\n",
    "    full_results[\"metadata\"] = metrics_dic\n",
    "    full_results = my_ut.turn_all_list_of_dict_into_str(full_results)\n",
    "    my_ut.create_parent_and_dump_json(pred_dir, f'metrics_{PRECI_RECALL_MARGIN}.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 10\n",
    "res_folder_root = \"results_1/synthetic/within_hypothesis_noisy/r_covcp_experiments/SNR_20\"\n",
    "file_names = os.listdir(res_folder_root)\n",
    "PRED_FOLDER = [os.path.join(res_folder_root, file_name) for file_name in file_names]  #'.txt' not in file_name]\n",
    "# PRED_FOLDER = [res_folder_root]\n",
    "\n",
    "for pred_dir in PRED_FOLDER:\n",
    "\n",
    "    # fetching predictions\n",
    "    data_stats = my_ut.open_json(f\"{pred_dir}/covcp_metadata.json\")\n",
    "    rcov_pred_dic = my_ut.open_json(f\"{pred_dir}/r_covcp_pred.json\")\n",
    "    # normal_pred_dic = my_ut.open_json(f\"{pred_dir}/normal_pred.json\")\n",
    "    # assert list(normal_pred_dic.keys()) == list(statio_pred_dic.keys())\n",
    "\n",
    "    # output formatting\n",
    "    metrics_dic = {}\n",
    "    metrics_dic[\"pred_path\"] = pred_dir\n",
    "    metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "    metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "    rcov_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "    # normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "    for exp_id in rcov_pred_dic.keys():\n",
    "        # compute metrics\n",
    "        rcov_pred_bkps = my_ut.turn_str_of_list_into_list_of_int(rcov_pred_dic[exp_id][\"pred\"])\n",
    "        if len(rcov_pred_bkps) == 1:\n",
    "            rcov_pred_bkps = [0] + rcov_pred_bkps \n",
    "        # normal_pred_bkps = my_ut.turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "        gt_bkps = my_ut.turn_str_of_list_into_list_of_int(rcov_pred_dic[exp_id][\"gt\"])\n",
    "        my_res.compute_and_update_metrics(gt_bkps, rcov_pred_bkps, rcov_results, PRECI_RECALL_MARGIN)\n",
    "        # my_res.compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "        # add time values\n",
    "        rcov_results[\"time\"][\"raw\"].append(rcov_pred_dic[exp_id][\"time\"])\n",
    "        # normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "    # results post-precessing and saving\n",
    "    full_results = {\"rcov normal cost\": rcov_results} #, \"normal cost\": normal_results}\n",
    "    full_results = my_res.compute_and_add_stat_on_metrics(full_results)\n",
    "    full_results[\"metadata\"] = data_stats\n",
    "    full_results = my_ut.turn_all_list_of_dict_into_str(full_results)\n",
    "    my_ut.create_parent_and_dump_json(pred_dir, f'metrics_{PRECI_RECALL_MARGIN}.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large_x0.4_SNR_20_ER_20_nodes_deg_10_bandwidth_0.4_pen_coef_gridsearch_from_covcp_paper']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.axis.XTick at 0x7f8ef6cb47d0>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6caa990>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6d573d0>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6cf1e10>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6afc150>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6afd850>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6affb50>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6b09d50>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6b0bfd0>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6b0e0d0>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6cd3e90>,\n",
       " <matplotlib.axis.XTick at 0x7f8ef6b10c50>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### INITIALIZATION: GLASSO CALIBRATION PLOTS\n",
    "##############################################\n",
    "\n",
    "preci_margin = 5\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "res_folder_root = \"results_1/synthetic/within_hypothesis_noisy/glasso_experiments/SNR_20_large_x0.4\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "res_folder_list = [os.path.join(res_folder_root, file_name) for file_name in res_file_name_list]\n",
    "print(res_file_name_list)\n",
    "\n",
    "# x_tick_labels =  [1, 2, 4]\n",
    "# x_tick_labels =  [0.6, 0.8, 1.0, 1.2, 1.4, 6, 8, 10, 12, 14, 20, 30]\n",
    "x_tick_labels =  [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 25]\n",
    "# x_tick_labels =  [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "abscissa_pos = list(range(len(x_tick_labels)))  #[\"x\" + str(i/10) for i in range(1, 11)]\n",
    "\n",
    "cost_func_keys =  [str(x_tick) for x_tick in x_tick_labels] # [\"statio normal cost\"] #, \"normal cost\"]\n",
    "metrics_keys =  [\"assignement_cost\"] #[\"f1_score\"] #, \"hausdorff\"]\n",
    "\n",
    "res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin)\n",
    "\n",
    "\n",
    "##### PLOTTING: W.R.T GLASSO CALIBRATION PLOTS\n",
    "##############################################\n",
    "\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(8*len(metrics_keys), 6))\n",
    "\n",
    "x_coords = abscissa_pos  #[float(x[1:]) for x in abscissa_pos]\n",
    "##### precision\n",
    "# plot_bar_and_scatter(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"f1_score\", axes[0], shift=0.3, to_label=True)\n",
    "y = []\n",
    "y_err = []\n",
    "for i, cost_func in enumerate(cost_func_keys):\n",
    "    y.append(res_dic[cost_func][\"assignement_cost\"][\"mean\"][0])\n",
    "    y_err.append(res_dic[cost_func][\"assignement_cost\"][\"std\"][0])\n",
    "axes.bar(x = x_coords, height=y, yerr=y_err, width=0.2, error_kw=error_kw)\n",
    "    # scatter plot based on the raw values\n",
    "    # add_raw_scatter_to_plot(abscissa_pos, res_dic, \"assignement_cost\", cost_func, axes)\n",
    "\n",
    "\n",
    "axes.set_title(f'Assignement score for {res_folder_root.split(\"/\")[-1]}')\n",
    "axes.set_ylim(bottom=0)\n",
    "axes.set_xlabel(\"Multiplicative coefficient\")\n",
    "axes.set_xticks(x_coords, x_tick_labels)\n",
    "\n",
    "\n",
    "##### CAPTIONNING AND METADATA\n",
    "##############################\n",
    "\n",
    "# fig.suptitle(\"ABLATION STUDY \\nER graphs; N_nodes=N=20; SNR=10; segment_length=0.1*N(N-1)/2; N_exp=1000\")\n",
    "# fig.subplots_adjust(bottom=0.17, top=0.85)\n",
    "# fig.legend(handles=legend_elements, loc='lower center', ncols=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION: W.R.T MARGIN PLOTS\n",
    "########################################\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "res_folder_root = \"results_1/real_data/eeg-motor-movement/filtered_0.5-40_order_3_subsampled_8\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "res_folder_list = [os.path.join(res_folder_root, file_name + '/metrics') for file_name in res_file_name_list]\n",
    "print(res_file_name_list)\n",
    "\n",
    "x_tick_labels =  list(range(5, 105, 5))\n",
    "abscissa_pos = list(range(len(x_tick_labels)))  #[\"x\" + str(i/10) for i in range(1, 11)]\n",
    "\n",
    "cost_func_keys = [\"statio normal cost\"] #, \"normal cost\"]\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\"} #, \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys =  [\"f1_score\"] #, \"hausdorff\"]\n",
    "\n",
    "per_margin_res_dic = {}\n",
    "\n",
    "for preci_margin_val in x_tick_labels:\n",
    "    per_margin_res_dic[preci_margin_val] = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin_val)\n",
    "\n",
    "\n",
    "##### PLOTTING: W.R.T MARGIN PLOTS\n",
    "##################################\n",
    "\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(8*len(metrics_keys), 6))\n",
    "\n",
    "x_coords = x_tick_labels # abscissa_pos  #[float(x[1:]) for x in abscissa_pos]\n",
    "\n",
    "##### precision\n",
    "# plot_bar_and_scatter(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"f1_score\", axes[0], shift=0.3, to_label=True)\n",
    "standard_plot_plus_fill_between_wrt_margin(cost_func_keys, per_margin_res_dic, x_coords, colors_per_cost_func, \"f1_score\", x_tick_labels, axes, to_label=True, alpha=0)\n",
    "\n",
    "\n",
    "axes.set_ylim(bottom=0, top=1.1)\n",
    "axes.set_ylabel(\"F1 score\")\n",
    "axes.set_xlabel(\"Margin (in number of samples)\")\n",
    "# axes.set_xticks(x_coords, x_tick_labels)\n",
    "\n",
    "\n",
    "##### CAPTIONNING AND METADATA\n",
    "##############################\n",
    "\n",
    "# fig.suptitle(\"ABLATION STUDY \\nER graphs; N_nodes=N=20; SNR=10; segment_length=0.1*N(N-1)/2; N_exp=1000\")\n",
    "# fig.subplots_adjust(bottom=0.17, top=0.85)\n",
    "# fig.legend(handles=legend_elements, loc='lower center', ncols=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION: W.R.T MARGIN PLOTS\n",
    "########################################\n",
    "\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(8*len(metrics_keys), 6))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "list_of_res_folder_root = [\n",
    "    \"results_1/real_data/eeg-motor-movement/filtered_0.5-40_order_3_subsampled_8\",\n",
    "    \"results_1/real_data/eeg-motor-movement/filtered_0.5-40_order_3_subsampled_4\",\n",
    "    \"results_1/real_data/eeg-motor-movement/filtered_0.5-40_order_3_subsampled_2\"\n",
    "]\n",
    "\n",
    "colors_per_folder_root = [\n",
    "    {\"normal cost\": \"dodgerblue\", \"statio normal cost\": \"darkorange\",},\n",
    "    {\"statio normal cost\": \"peru\"},\n",
    "    {\"statio normal cost\": \"teal\"}\n",
    "]\n",
    "#-----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "for i, res_folder_root in enumerate(list_of_res_folder_root) :\n",
    "\n",
    "    res_folder_list = [os.path.join(res_folder_root, file_name + '/metrics') for file_name in res_file_name_list]\n",
    "    print(res_file_name_list)\n",
    "\n",
    "    x_tick_labels =  list(range(5, 205, 5))\n",
    "    abscissa_pos = list(range(len(x_tick_labels)))  #[\"x\" + str(i/10) for i in range(1, 11)]\n",
    "\n",
    "    colors_per_cost_func = colors_per_folder_root[i]\n",
    "    cost_func_keys = list(colors_per_cost_func.keys())\n",
    "    metrics_keys = [\"f1_score\"] #, \"hausdorff\"]\n",
    "\n",
    "    per_margin_res_dic = {}\n",
    "\n",
    "    for preci_margin_val in x_tick_labels:\n",
    "        per_margin_res_dic[preci_margin_val] = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin_val)\n",
    "\n",
    "\n",
    "    ##### PLOTTING: W.R.T MARGIN PLOTS\n",
    "    ##################################\n",
    "\n",
    "    x_coords = x_tick_labels # abscissa_pos  #[float(x[1:]) for x in abscissa_pos]\n",
    "\n",
    "    ##### precision\n",
    "    # plot_bar_and_scatter(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"f1_score\", axes[0], shift=0.3, to_label=True)\n",
    "    standard_plot_plus_fill_between_wrt_margin(cost_func_keys, per_margin_res_dic, x_coords, colors_per_cost_func, \"f1_score\", x_tick_labels, axes, to_label=True, alpha=0)\n",
    "\n",
    "\n",
    "axes.set_ylim(bottom=0, top=1.1)\n",
    "axes.set_ylabel(\"F1 score\")\n",
    "axes.set_xlabel(\"Margin (in number of samples)\")\n",
    "# axes.set_xticks(x_coords, x_tick_labels)\n",
    "\n",
    "\n",
    "legend_elements = []\n",
    "for i, res_folder_root in enumerate(list_of_res_folder_root):\n",
    "    for cost_func in list(colors_per_folder_root[i].keys()):\n",
    "        color = colors_per_folder_root[i][cost_func]\n",
    "        legend_elements.append(Line2D([0], [0], color=color, lw=0.01, marker='x', label=f\"{res_folder_root[-12:]}_{COST_FUNC_NAME_TO_PLOT_LABEL[cost_func]}\"),)\n",
    "\n",
    "\n",
    "##### CAPTIONNING AND METADATA\n",
    "##############################\n",
    "\n",
    "axes.legend(handles=legend_elements, loc=\"lower right\", ncols=1)#len(legend_elements))\n",
    "\n",
    "# fig.suptitle(\"ABLATION STUDY \\nER graphs; N_nodes=N=20; SNR=10; segment_length=0.1*N(N-1)/2; N_exp=1000\")\n",
    "# fig.subplots_adjust(bottom=0.17, top=0.85)\n",
    "# fig.legend(handles=legend_elements, loc='lower center', ncols=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION 1 ABLATION STUDY\n",
    "########################################\n",
    "\n",
    "preci_margin = 2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "legend_elements = []\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "res_fold_root = \"results_1/synthetic/within_hypothesis_noisy/varying_segment_length/large_x0.1_SNR_10ER_20_nodes_deg_10_bandwidth_0.4_80exp\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\", \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "\n",
    "cost_func_keys = [\"normal cost\"]\n",
    "res_dic = get_res_per_metric_per_cost_func([res_fold_root], cost_func_keys, metrics_keys, preci_margin)\n",
    "x_coords = [0.1]\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "legend_elements.append(Line2D([0], [0], color=colors_per_cost_func[\"normal cost\"], lw=3, label=\"A. normal mle cost\"))\n",
    "\n",
    "cost_func_keys = [\"statio normal cost\"]\n",
    "res_dic = get_res_per_metric_per_cost_func([res_fold_root], cost_func_keys, metrics_keys, preci_margin)\n",
    "x_coords = [0.2]\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "legend_elements.append(Line2D([0], [0], color=colors_per_cost_func[\"statio normal cost\"], lw=3, label=\"B. statio cost\"))\n",
    "\n",
    "\n",
    "##### INITIALIZATION 2 ABLATION STUDY\n",
    "######################################\n",
    "\n",
    "res_folder_root = \"results_1/synthetic/ablation_study/large_x0.1\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "res_file_name_list.sort()\n",
    "res_folder_list = [os.path.join(res_folder_root, file_name) for file_name in res_file_name_list]\n",
    "cost_func_keys = [\"statio normal cost\"]\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "colors_per_file = [\"lightcoral\", \"darkorchid\", \"peru\", \"teal\"]\n",
    "x_coords_top = [0.3, 0.4, 0.5, 0.6]\n",
    "labels = [\"C. connectivity modified 0.05\", \"D. other ER graph\", \"E. exp sim geographical graphs\", \"F. diagonal in canonical basis\"]\n",
    "\n",
    "for i, file_name in enumerate(res_folder_list):\n",
    "\n",
    "    colors_per_cost_func = {\"statio normal cost\": colors_per_file[i]}\n",
    "    res_dic = get_res_per_metric_per_cost_func([file_name], cost_func_keys, metrics_keys, preci_margin)\n",
    "    x_coords = [x_coords_top[i]]\n",
    "    # precision\n",
    "    plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "    # haussdorff\n",
    "    plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "    legend_elements.append(Line2D([0], [0], color=colors_per_file[i], lw=3, label=labels[i]))\n",
    "\n",
    "\n",
    "axes[0].set_title(F'F1 Score (margin = {preci_margin})')\n",
    "axes[0].set_xticks([i/10 for i in range(1, 7)], [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n",
    "axes[0].set_xlabel(\"Different settings\")\n",
    "axes[1].set_title('Haussdorff')\n",
    "axes[1].set_xticks([i/10 for i in range(1, 7)], [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n",
    "axes[1].set_xlabel(\"Different settings\")\n",
    "axes[1].set_ylim(bottom=0, top=400)\n",
    "\n",
    "\n",
    "##### CAPTIONNING AND METADATA\n",
    "##############################\n",
    "\n",
    "fig.suptitle(\"ABLATION STUDY \\nER graphs; N_nodes=N=20; SNR=10; segment_length=0.1*N(N-1)/2; N_exp=1000\")\n",
    "fig.subplots_adjust(bottom=0.17, top=0.85)\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncols=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION WITH DIFFERENT GRAPH TYPES\n",
    "###############################################\n",
    "\n",
    "PRECI_RECALL_MARGIN = 5\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "pred_path = \"results_1/synthetic/within_hypothesis/varying_segment_length\"\n",
    "file_names = os.listdir(pred_path)\n",
    "PRED_FOLDER = [os.path.join(pred_path, file_name) for file_name in file_names if '.txt' not in file_name]\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "markers = ['o', \"s\"]\n",
    "linestyles = [\"dashed\", \"dotted\"]\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "abscissa_pos =  [\"x1\", \"x1.5\", \"x2\", \"x2.5\", \"x4\"]\n",
    "graph_types = [\"ER\"] # \"exp_geo\", \"KNN_geo\", \n",
    "graph_colors = ['darkorange'] #, 'dodgerblue', 'darkorchid']\n",
    "shift = 3\n",
    "\n",
    "\n",
    "##### PLOTTING\n",
    "##############\n",
    "\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(7*len(metrics_keys), 5)) #, layout='constrained')\n",
    "\n",
    "for i in range(len(graph_types)):\n",
    "\n",
    "    # re-arrange the folder name so they have the right position for meaningful plotting\n",
    "    graph_name = graph_types[i]\n",
    "    res_folder_list = [folder_name for folder_name in PRED_FOLDER if graph_name in folder_name]\n",
    "    print(res_folder_list)\n",
    "    res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, PRECI_RECALL_MARGIN)\n",
    "\n",
    "    x_coords = [float(x[1:]) + i*shift for x in abscissa_pos]\n",
    "    plot_scatter_errorbar(x_coords, cost_func_keys, graph_colors[i], markers, linestyles, \"f1_score\", res_dic, ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Number of nodes\")\n",
    "    axes[0].set_title(F'F1 SCORE (margin = {PRECI_RECALL_MARGIN})')\n",
    "    plot_scatter_errorbar(x_coords, cost_func_keys, graph_colors[i], markers, linestyles, \"hausdorff\", res_dic, ax=axes[1])\n",
    "    axes[1].set_xlabel(\"Number of nodes\")\n",
    "    axes[1].set_title('Haussdorff')\n",
    "    axes[1].set_ylim(bottom=0)\n",
    "\n",
    "x_tick_loc = [float(x[1:]) + 0* shift for x in abscissa_pos]\n",
    "axes[0].set_xticks(x_tick_loc, abscissa_pos)\n",
    "axes[1].set_xticks(x_tick_loc, abscissa_pos)\n",
    "\n",
    "\n",
    "##### CAPTIONNING AND METADATA\n",
    "##############################\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='k', lw=0.01, marker=markers[0], linestyle=linestyles[0], label=cost_func_keys[0]),\n",
    "    Line2D([0], [0], color='k', lw=0.01, marker=markers[1], linestyle=linestyles[1], label=cost_func_keys[1]),\n",
    "    Line2D([0], [0], color=graph_colors[0], lw=3, label=graph_types[0]),\n",
    "    # Line2D([0], [0], color=graph_colors[1], lw=3, label=graph_types[1]),\n",
    "    # Line2D([0], [0], color=graph_colors[2], lw=3, label=graph_types[2]),\n",
    "]               \n",
    "fig.suptitle(\"Comparison of cost functions with respect to the minimum segment length\")\n",
    "fig.subplots_adjust(bottom=0.17)\n",
    "fig.legend(handles=legend_elements, loc=\"lower center\", ncols=len(legend_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION RULE BASED FILE SELECTION\n",
    "################################################\n",
    "\n",
    "preci_margin = 2\n",
    "TO_SAVE = True\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "res_folder_root = \"results_1/synthetic/within_hypo_censor_breakdown/SNR_20_large_x0.4/varying_breakdown_length\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "res_file_name_list.sort()\n",
    "# res_file_name_list_left = res_file_name_list[5:]\n",
    "# res_file_name_list_right = res_file_name_list[:5]\n",
    "# res_file_name_list = res_file_name_list_left + res_file_name_list_right\n",
    "# res_file_name_list = [file_name for file_name in res_file_name_list if not '.txt' in file_name]\n",
    "# res_file_name_list = [file_name for file_name in res_file_name_list if len(file_name) < 66]\n",
    "# res_file_name_list = [file_name for file_name in res_file_name_list if '0_' in file_name[:20] or '5_' in file_name]\n",
    "# last_one = res_file_name_list.pop()\n",
    "# res_file_name_list.insert(0, last_one)\n",
    "res_file_name_list = ['large_x0.4_SNR_20_NBbd_6_bklength_50_ER_20_nodes_deg_10_bandwidth_0.4_80_exp', 'large_x0.4_SNR_20_NBbd_6_bklength_100_ER_20_nodes_deg_10_bandwidth_0.4_80_exp', 'large_x0.4_SNR_20_NBbd_6_bklength_150_ER_20_nodes_deg_10_bandwidth_0.4_80_exp', 'large_x0.4_SNR_20_NBbd_6_bklength_200_ER_20_nodes_deg_10_bandwidth_0.4_80_exp', 'large_x0.4_SNR_20_NBbd_6_bklength_250_ER_20_nodes_deg_10_bandwidth_0.4_80_exp', 'large_x0.4_SNR_20_NBbd_6_bklength_300_ER_20_nodes_deg_10_bandwidth_0.4_80_exp', 'large_x0.4_SNR_20_NBbd_6_bklength_350_ER_20_nodes_deg_10_bandwidth_0.4_80_exp', 'large_x0.4_SNR_20_NBbd_6_bklength_400_ER_20_nodes_deg_10_bandwidth_0.4_80_exp', 'large_x0.4_SNR_20_NBbd_6_bklength_450_ER_20_nodes_deg_10_bandwidth_0.4_80_exp', 'large_x0.4_SNR_20_NBbd_6_bklength_500_ER_20_nodes_deg_10_bandwidth_0.4_80_exp']\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "res_folder_list = [os.path.join(res_folder_root, file_name) for file_name in res_file_name_list]\n",
    "print(res_file_name_list)\n",
    "\n",
    "abscissa_pos = range(len(res_folder_list))  #[\"x\" + str(i/10) for i in range(1, 11)]\n",
    "x_tick_labels =  [x for x in range(50, 550, 50)] #[-5, 0, 5, 10, 15, 20]   #[file_name[7:10] + '%' for file_name in res_file_name_list] \n",
    "\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\", \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"f1_score\"] #, \"hausdorff\"]\n",
    "\n",
    "res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin)\n",
    "\n",
    "\n",
    "##### PLOTTING\n",
    "##############\n",
    "\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(8*len(metrics_keys), 6))\n",
    "\n",
    "x_coords = abscissa_pos  #[float(x[1:]) for x in abscissa_pos]\n",
    "##### precision\n",
    "# plot_bar_and_scatter(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"f1_score\", axes[0], shift=0.3, to_label=True)\n",
    "standard_plot_plus_fill_between(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"f1_score\", axes, to_label=True)\n",
    "axes.set_title(F'F1 SCORE (margin = {preci_margin})')\n",
    "axes.set_ylim(bottom=0)\n",
    "axes.set_xlabel(\"Breakdown length (in number of samples)\")\n",
    "axes.set_xticks(x_coords, x_tick_labels)\n",
    "##### haussdorff\n",
    "# plot_bar_and_scatter(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"hausdorff\", axes[1], shift=0.3)\n",
    "# # standard_plot_plus_fill_between(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"hausdorff\", axes[1])\n",
    "# axes[1].set_title('Haussdorff')\n",
    "# axes[1].set_xlabel(\"Breakdown length (in number of samples)\")\n",
    "# axes[1].set_ylim(bottom=0)\n",
    "# axes[1].set_xticks(x_coords, x_tick_labels)\n",
    "\n",
    "#### addditional content\n",
    "# additional_result_file = 'results_1/synthetic/within_hypothesis/varying_segment_length/large_x0.4_SNR_inf_ER_20_nodes_deg_10_bandwidth_0.4_1000_exp'\n",
    "# additional_res_dic = get_res_per_metric_per_cost_func([additional_result_file], [\"normal cost\"], ['f1_score'], preci_margin)\n",
    "# mean_normal_val = additional_res_dic['normal cost']['f1_score']['mean']\n",
    "# axes.axhline(y=mean_normal_val, xmin=0, xmax=1, color='dodgerblue', label='mean mle normal cost')\n",
    "\n",
    "\n",
    "# axes.legend(loc='center right')\n",
    "axes.legend(loc=(0.7, 0.35))\n",
    "\n",
    "\n",
    "##### CAPTIONNING AND METADATA\n",
    "##############################\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=list(colors_per_cost_func.values())[0], lw=3, label=list(colors_per_cost_func.keys())[0]),\n",
    "    # Line2D([0], [0], color=list(colors_per_cost_func.values())[1], lw=3, label=list(colors_per_cost_func.keys())[1]),\n",
    "    # Line2D([0], [0], color=list(colors_per_cost_func.values())[1], lw=3, label=list(colors_per_cost_func.keys())[1]),\n",
    "    # Line2D([0], [0], color=graph_colors[1], lw=3, label=graph_types[1]),\n",
    "    # Line2D([0], [0], color=graph_colors[2], lw=3, label=graph_types[2]),\n",
    "] \n",
    "\n",
    "metadata = get_metadata_for_plot(res_folder_list)\n",
    "subtitle = define_subtitle_based_on_exp_metadata(metadata)\n",
    "suptitle = \"Evaluation of the robustness with respect to censor breakdown.\"\n",
    "# suptitle = \"Evaluation of the robustness with respect to additive gaussian white noise.\"\n",
    "# suptitle = \"Evaluation of the robustness with respect to the quality of the observed connectivity of the graph\"\n",
    "# suptitle = \"Evaluation of the influence of the stationarity segment length.\"\n",
    "title = suptitle + '\\n' + subtitle + ' ~ Nb breakdowns=' + str(6)\n",
    "\n",
    "fig.suptitle(title)\n",
    "fig.subplots_adjust(bottom=0.17, top=0.85)\n",
    "# fig.legend(handles=legend_elements, loc=\"lower center\", ncols=len(legend_elements))\n",
    "# fig.legend(loc=\"lower center\", ncols=4)\n",
    "\n",
    "if TO_SAVE:\n",
    "    plot_path = \"images/results_1/robustness_with_respect_to_censor_breakdown/varying_breakdown_length/SNR_20_large_x0.4_NBbk_6\"\n",
    "    plot_name =   f'only_f1_mean_std_standard_plot_margin_{preci_margin}'  #     f'bar_plot_margin_{preci_margin}' #    \n",
    "    plt.savefig(f\"{plot_path}/{plot_name}\")\n",
    "    my_ut.create_parent_and_dump_json(plot_path, f'{plot_name}.json', metadata, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"Izenman2008\">[Izenman2008]</a>\n",
    "Izenman Alan J. (2008). Introduction to Random-Matrix Theory [asc.ohio-state.edu]\n",
    "\n",
    "<a id=\"Ryan2023\">[Ryan2023]</a>\n",
    "Sean Ryan and Rebecca Killick. Detecting Changes in Covariance via Random Matrix Theory. Technometrics, 65(4):480–491, October 2023. Publisher: Taylor & Francis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
