{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance matrix change-point detection under graph stationarity assumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "\n",
    "Let $y = (y_1, \\ldots, y_t, \\ldots, y_T), y_t \\in \\mathbb{R}^{N}$ a graph signal lying on the nodes of the graph $G = (V, E, W)$, with $N =|V|$.\n",
    "\n",
    "We aim at detecting changes of the (spatial) covariance matrix $\\Sigma_t$ of the graph signals $y_t$. We assume that there exits an unknown set of change-points $\\Tau = (t_1, \\ldots, t_K) \\subset [1, T]$ with unknown cardinality such that the covariance matrix of the graph signals is constant over any segment $[t_k, t_{k+1}]$. We do the following hypothesis:\n",
    "\n",
    "1. the signals $y_t$ follow a multivariate Gaussian distribution with fixed covariance matrix over each segment and known mean $\\mu$, i.e:\n",
    "$$\\forall k \\in [1, K] ~ \\forall t \\in [t_k, t_{k+1}] \\quad y_t \\sim \\mathcal{N}(\\mu, \\Sigma_k)$$\n",
    "\n",
    "2. over each segment, the signals $y_t$ verify the second order wide-sense graph stationarity:\n",
    "$$\\forall k \\in [1, K] \\quad \\Sigma_k = U \\text{diag}(\\gamma_k)U^T $$\n",
    "\n",
    "where the matrix $U$ contains the eigenvectors of the graph combinatorial Laplacian matrix $L = D - W$ in its columns. \n",
    "\n",
    "The Graph Fourier Transform $\\tilde{y}$ of a signal $y$ is defined by $\\tilde{y} = U^T y $.\n",
    "\n",
    "\n",
    "Based on the above assumptions, the cost derived from the maximum log-likelihood over a segment $[a, b-1]$ writes:\n",
    "\n",
    "\\begin{align*}\n",
    "    c_s(y_{a}, \\ldots, y_{b-1}) = ~ & (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + ~ \\sum_{t=a}^{b-1} \\sum_{n=1}^N \\frac{\\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2}{\\hat{\\gamma}_{a.b}^{(n)}} = ~ (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + N(b-a)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\hat{\\mu}_{T}$ is the empirical mean of the process over $[0, T]$\n",
    "- $\\hat{\\gamma}_{a..b}$ is the (empirical) biased correlogram/periodogram of the process over $[a, b-1]$: $\\hat{\\gamma}_{a..b} = \\frac{1}{(b-a)} \\sum_{t=a}^{b-1} \\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.linalg import eigh\n",
    "from ruptures.base import BaseCost\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from numba import njit\n",
    "from typing import List, Callable, Literal, Dict\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from matplotlib.lines import Line2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostGraphStatioNormal(BaseCost):\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"graph_sationary_normal_cost\"\n",
    "\n",
    "    def __init__(self, laplacian_mat) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            laplacian_mat (array): the discrete Laplacian matrix of the graph: D - W\n",
    "            where D is the diagonal matrix diag(d_i) of the node degrees and W the adjacency matrix\n",
    "        \"\"\"\n",
    "        self.graph_laplacian_mat = laplacian_mat\n",
    "        self.signal = None\n",
    "        self.gft_square_cumsum = None\n",
    "        self.gft_mean = None\n",
    "        self.min_size = laplacian_mat.shape[0]\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, signal):\n",
    "        \"\"\"Performs pre-computations for per-segment approximation cost.\n",
    "\n",
    "        NOTE: the number of dimensions of the signal and their ordering\n",
    "        must match those of the nodes of the graph.\n",
    "        The function eigh used below returns the eigenvector corresponding to \n",
    "        the ith eigenvalue in the ith column eigvect[:, i]\n",
    "\n",
    "        Args:\n",
    "            signal (array): of shape [n_samples, n_dim].\n",
    "        \"\"\"\n",
    "        self.signal = signal\n",
    "        # computation of the GFSS\n",
    "        _, eigvects = eigh(self.graph_laplacian_mat)\n",
    "        gft =  signal @ eigvects # equals signal.dot(eigvects) = eigvects.T.dot(signal.T).T\n",
    "        self.gft_mean = np.mean(gft, axis=0)\n",
    "        # computation of the per-segment cost utils\n",
    "        self.gft_square_cumsum = np.concatenate([np.zeros((1, signal.shape[1])), np.cumsum((gft - self.gft_mean[None, :])**2, axis=0)], axis=0)\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "\n",
    "        Returns:\n",
    "            float: segment cost\n",
    "        \"\"\"\n",
    "        if end - start < self.min_size:\n",
    "            raise ValueError(f'end - start shoud be higher than {self.min_size}')\n",
    "        sub_square_sum = self.gft_square_cumsum[end] - self.gft_square_cumsum[start]\n",
    "        return (end  - start) * np.sum(np.log(sub_square_sum / (end - start)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setting v.0: synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on the minimum distance between consecutive change points (minimum segment length)\n",
    "\n",
    "We require that the different change points $(t_1, \\ldots, t_K)$ verify:\n",
    "\n",
    "$$|t_{k+1} - t_k| >= l ~ \\forall k \\in [1, K-1] $$\n",
    "\n",
    "where $l$ can be seen as the minimum admissible segment length. In this paragraph we give a meaningful lower bound of this parameter. Such lower bound is related to the computation of the cost functions over the segments $[a, b] \\subset [0, T]$, namely the graph stationary normal cost function $c_s$ described above and the standard normal cost function $c_n$:\n",
    "\n",
    "- $ c_n(y_{a}, \\ldots, y_{b-1}) = (b - a) \\log  \\left[ \\det \\left( \\hat \\Sigma_{a..b} \\right) \\right]$\n",
    "- $ c_s(y_{a}, \\ldots, y_{b-1}) = (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} $\n",
    "\n",
    "Based on the formula of the spectrogram $\\hat{\\gamma}_{a.b}$ given in the introduction, there is no numerical constraints for the feasibility of the computation. However, the $\\log [ \\det ( \\cdot ) ]$ function used in the formula for $c_n$ should be applied to invertible matrices $\\Sigma_{a..b}$ only. Therefore, we should focus on the conditions under which the matrix:\n",
    "\n",
    "$$ \\hat \\Sigma_{a..b} = \\frac{1}{b-a} \\sum_{t=a}^{b-1} (y_t - \\mu_T) (y_t - \\mu_T)^T \\quad \\text{ with } y_t \\sim \\mathcal{N}(\\mu, \\Sigma)  $$\n",
    "\n",
    "is invertible. Actually, such conditions have already been clearly stated in different works from Random Matrix Theory (RMT). For instance, it is shown in [[Izenman2008](#Izenman2008)] that $n \\hat \\Sigma_{a..b} \\sim \\mathcal{W}(b-a, \\Sigma)$ follows the Wishart distribution. In this framework, it is possible to study the distribution of the eigenvalues of $\\hat \\Sigma_{a..b}$ and to deduce that: \n",
    "\n",
    "$$ \\text{If } b-a > N \\text{ with } N  \\text{ the dimension of } y_t, \\text{ then } \\hat \\Sigma_{a..b} \\text{ is almost surely invertible }   $$\n",
    "\n",
    "Conversely, it is possible to show that if $ b-a < N $ (the number of observations is lower then the number of variables), the matrix $\\hat \\Sigma_{a..b}$ is nalmost surely not invertible. This can be done by considering the family the first $(N+1)$ columns of $\\hat \\Sigma_{a..b}$.\n",
    "\n",
    "Thus, the right lower-bound $l$ should be $\\boxed{l = N}$, which is consistent with statement from [[Ryan2023](#Ryan2023)].\n",
    "\n",
    "Note: with segments of length $l$, one is not guaranteed to compute good estimates of the covariance matrix, but at least such computations is almost surely admissible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data generation\n",
    "\n",
    "We design two graph generation scenarios.\n",
    "\n",
    "1. Erdős–Rényi (ER) graphs with random parameters\n",
    "\n",
    "In this scenario, we pick random number of nodes $N$ uniformly drawn in $[N_{\\min}, N_{\\max}]$, that may vary according to the experiment, and a random edge probability $p$ uniformly drawn in $[0.15, 0.5]$. The latter was chosen so that the generated graphs empirically have a realistic connectivity.\n",
    "\n",
    "2. Geographic-like graphs \n",
    "\n",
    "We pick a random number $N$ of nodes uniformly drawn in $[N_{\\min}, N_{\\max}]$. The nodes are randomly located in $[0, 1]^2$ using the uniform law $\\mathcal{U}([0, 1]^2)$. Eventually, we build the adjacency matrix of the graph by applying a threshold $\\rho$ to the distance separating the nodes. More formally, if we denote $(W_{ij})_{1 \\leq i, j \\leq N}$ the coefficients of the adjacency matrix, we explore the following two formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "    W_{ij} = \\mathbb{I} \\left( \\|p_i - p_j \\|_2 \\leq \\rho \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    W_{ij} = \\frac{\\rho}{\\|p_i - p_j \\|_2}  \\mathbb{I} \\left( \\|p_i - p_j \\|_2 \\leq \\rho \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $p_i$ denotes the 2D coordinates of node $i$. In the above formulas, the value of $\\rho$ is chosen empirically so that the resulting graphs visually exhibit connectivity patterns that are consistent to what one may expect when using a graph structure for signal analysis. More precisely, we use the following values:\n",
    "\n",
    "- for $N$ drawn in $[10, 50]$: $~$ $\\rho = 0.3$\n",
    "- for $N$ drawn in $[80, 110]$: $\\rho = 0.2$\n",
    "\n",
    "This scenario simulates 2D geographic graphs and is more likely to match realistic use-cases.\n",
    "\n",
    "\n",
    "### Signal generation\n",
    "\n",
    "Let $G$ be a graph randomly generated using one of the above scenarios. We recall that the laplacian matrix $L$ of the graph verifies $L = U \\Lambda U^T$, where the columns of $U$ are the eigenvectors of $L$. We now describe how to generate a signal $(y_t)_{1 \\leq t \\leq T}$ that verifies the hypothesis presented in the [problem formulation](#problem-formulation).\n",
    "\n",
    "We first pick an admissible number of change points $K$ depending on the signal length $T=1000$ and the minimum segment length $l = N$ by sampling $~  \\mathcal{U}([1, \\min(\\frac{T}{10}, \\frac{T}{l})])$. The change points $(t_k)_{1 \\leq k \\leq K}$ are uniformly drawn in $[l, T-l]$, by checking that a newly selected change point does not break the minimum segment length criteria. Therefore, it may happen that after a limit number of iterations, not $K$ change points were drawn, but we still use the same notations (SHOULD BE CHANGED).\n",
    "\n",
    "Finally, we apply the following formula to generate the signal $(y_t)_{1 \\leq t \\leq T}$:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\forall ~ k \\in [1, K-1] ~ \\forall  t \\in [t_k, t_{k+1}] \\quad  y_t \\sim \\mathcal{N}_N(0, \\Sigma_k) \\quad \\text{ with } \\quad \\Sigma_k = U \\text{diag}(\\gamma_k) U^T ~  \\text{ and } ~ \\gamma_k \\sim \\mathcal{U}niform_N([0, 1]) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_head_short_hash() -> str:\n",
    "    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).decode('ascii').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_all_list_of_dict_into_str(data:dict):\n",
    "    new_dict = {}\n",
    "    for key, val in data.items():\n",
    "        if isinstance(val, list):\n",
    "            new_dict[key] = str(val)\n",
    "        elif isinstance(val, dict):\n",
    "            new_dict[key] = turn_all_list_of_dict_into_str(val)\n",
    "        else:\n",
    "            new_dict[key] = val\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_str_of_list_into_list_of_int(list_str):\n",
    "    assert list_str[0] == '[' and list_str[-1] == ']'\n",
    "    list_of_str = list_str[1:-1].split(',')\n",
    "    return [int(val_str) for val_str in list_of_str]\n",
    "\n",
    "def turn_str_of_list_into_list_of_float(list_str):\n",
    "    assert list_str[0] == '[' and list_str[-1] == ']'\n",
    "    list_of_str = list_str[1:-1].split(',')\n",
    "    return [float(val_str) for val_str in list_of_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_and_dump_json(parent_dir, name, data, indent=None):\n",
    "    if os.path.exists(os.path.join(parent_dir, name)):\n",
    "        raise FileExistsError\n",
    "    if not os.path.exists(parent_dir):\n",
    "        Path(parent_dir).mkdir(parents=True, exist_ok=False)\n",
    "    with open(os.path.join(parent_dir, name), 'w+') as f:\n",
    "        json.dump(data, f, indent=indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(path):\n",
    "    with open(path, 'r+') as f:\n",
    "        content = json.load(f)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_er_graphs(params_rng, nx_graph_seed, max_n_nodes, min_n_nodes=10, min_edge_p=0.15, max_edge_p=0.5):\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    edge_p = min_edge_p + (max_edge_p-min_edge_p) * params_rng.random()\n",
    "    G = nx.erdos_renyi_graph(n=n_nodes, p=edge_p, seed=nx_graph_seed)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "for _ in range(5):\n",
    "    G = generate_random_er_graphs(params_rng, graph_seed, max_n_nodes=30)\n",
    "    coord = nx.spring_layout(G, seed=0)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_ba_graphs(params_rng, nx_graph_seed, min_n_nodes, max_n_nodes):\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    m = int(params_rng.normal(loc=n_nodes//10, scale=1))\n",
    "    G = nx.barabasi_albert_graph(n=n_nodes, m=m, seed=nx_graph_seed)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "for _ in range(5):\n",
    "    G = generate_random_ba_graphs(params_rng, graph_seed, min_n_nodes=10, max_n_nodes=30)\n",
    "    coord = nx.spring_layout(G, seed=0)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_weighted_geographic_graph(params_rng, min_n_nodes, max_n_nodes, dist_threshold):\n",
    "    # generation of the nodes coordinates\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "    # computation of the adjacency matrix based on distance threshold \n",
    "    dist_mat_condensed = pdist(nodes_coord, metric='euclidean')\n",
    "    adj_mat = squareform((dist_threshold/dist_mat_condensed)*(dist_mat_condensed < dist_threshold).astype(int))\n",
    "    G = nx.Graph(adj_mat)\n",
    "    return G, nodes_coord\n",
    "\n",
    "def generate_random_geographic_graph(params_rng, min_n_nodes, max_n_nodes, dist_threshold):\n",
    "    # generation of the nodes coordinates\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "    # computation of the adjacency matrix based on distance threshold \n",
    "    dist_mat_condensed = pdist(nodes_coord, metric='euclidean')\n",
    "    adj_mat = squareform((dist_mat_condensed < dist_threshold).astype(int))\n",
    "    G = nx.Graph(adj_mat)\n",
    "    return G, nodes_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(5*4, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "for _ in range(5):\n",
    "    G, coord = generate_random_geographic_graph(params_rng, min_n_nodes=80, max_n_nodes=110, dist_threshold=0.20)\n",
    "    coord_dic = {i: coord[i, :] for i in range(coord.shape[0])}\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord_dic, node_size=50, ax=axes[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "\n",
    "n_nodes = params_rng.integers(low=80, high=110+1)\n",
    "nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "# computation of the adjacency matrix based on distance threshold \n",
    "dist_mat_condensed = pdist(nodes_coord, metric='euclidean')\n",
    "# --> to check that there is no 0 value in the condensed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaus_signal_with_cov_diag_in_basis(n_dims, n_samples, basis, signal_rng, diag_cov_max=1):\n",
    "    # randomly draw diagonal coef (in the fourier space)\n",
    "    diag_coefs = diag_cov_max * signal_rng.random(n_dims)\n",
    "    diag_mat = np.diag(diag_coefs)\n",
    "    # compute the corresponding covariance matrix and signal \n",
    "    cov_mat = basis @ diag_mat @ basis.T\n",
    "    signal = signal_rng.multivariate_normal(np.zeros(n_dims), cov_mat, size=n_samples)\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_length = Literal[\"large\", \"minimal\"]\n",
    "\n",
    "def get_min_size_for_hyp(n_dims, hyp:seg_length):\n",
    "    # the minimal segment length for admissible computations\n",
    "    min_size = n_dims\n",
    "    if hyp == \"large\":\n",
    "        #for segment long enough for good estimates\n",
    "        min_size = n_dims * (n_dims-1) / 2\n",
    "    return min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bkps_with_gap_constraint(n_samples, bkps_gap, bkps_rng, n_bkps_max, max_tries=10000):\n",
    "    # randomly pick an admissible number of bkps\n",
    "    n_bkps = bkps_rng.integers(low=1, high=min(n_bkps_max, n_samples // bkps_gap))\n",
    "    bkps = []\n",
    "    n_tries = 0\n",
    "    # select admissible randomly drawn bkps\n",
    "    while n_tries < max_tries and len(bkps) < n_bkps:\n",
    "        new_bkp = bkps_rng.integers(low=bkps_gap, high=n_samples-bkps_gap)\n",
    "        to_keep = True\n",
    "        for bkp in bkps:\n",
    "            if abs(new_bkp - bkp) < bkps_gap:\n",
    "                to_keep = False\n",
    "                break\n",
    "        if to_keep:\n",
    "            bkps.append(new_bkp)\n",
    "        n_tries+=1\n",
    "    bkps.sort()\n",
    "    return bkps + [n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_fixed_nb_bkps_with_gap_constraint(n_samples, bkps_gap, bkps_rng, max_tries=10000):\n",
    "    # randomly pick an admissible number of bkps\n",
    "    n_bkps = bkps_rng.integers(low=n_samples // bkps_gap - 1, high= n_samples // bkps_gap)\n",
    "    bkps = []\n",
    "    n_tries = 0\n",
    "    # select admissible randomly drawn bkps\n",
    "    while n_tries < max_tries and len(bkps) < n_bkps:\n",
    "        new_bkp = bkps_rng.integers(low=bkps_gap, high=n_samples-bkps_gap)\n",
    "        to_keep = True\n",
    "        for bkp in bkps:\n",
    "            if abs(new_bkp - bkp) < bkps_gap:\n",
    "                to_keep = False\n",
    "                break\n",
    "        if to_keep:\n",
    "            bkps.append(new_bkp)\n",
    "        n_tries+=1\n",
    "    bkps.sort()\n",
    "    return bkps + [n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data writting and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(G:nx.Graph, signal:np.ndarray, bkps:List[int], dir:str):\n",
    "    # create subfolder\n",
    "    Path(dir).mkdir(parents=True, exist_ok=False)\n",
    "    # save graph\n",
    "    adj_mat = nx.to_numpy_array(G)\n",
    "    with open(f'{dir}/graph_adj_mat.npy', 'wb+') as nx_f:\n",
    "        np.save(nx_f, adj_mat, allow_pickle=False)\n",
    "    # save signal\n",
    "    with open(f'{dir}/signal.npy', 'wb+') as np_f:\n",
    "        np.save(np_f, signal, allow_pickle=False)\n",
    "    # save bkps\n",
    "    bkps_str = [int(bkp) for bkp in bkps]\n",
    "    create_parent_and_dump_json(dir, \"bkps.json\", bkps_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str):\n",
    "    adj_mat = np.load(f\"{path}/graph_adj_mat.npy\", allow_pickle=False)\n",
    "    G = nx.from_numpy_array(adj_mat)\n",
    "    signal = np.load(f\"{path}/signal.npy\", allow_pickle=False)\n",
    "    bkps = open_json(f\"{path}/bkps.json\")\n",
    "    return G, signal, bkps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results computation and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruptures.metrics import precision_recall\n",
    "from ruptures.metrics import hausdorff\n",
    "\n",
    "def compute_f1_score(preci, recall):\n",
    "    if preci + recall > 0:\n",
    "        return 2 * (preci * recall) / (preci + recall)\n",
    "    return 0\n",
    "\n",
    "def compute_and_update_metrics(true_bkps, pred_bkps, metrics_dic, prec_rec_margin):\n",
    "    preci, recall = precision_recall(true_bkps, pred_bkps, prec_rec_margin)\n",
    "    f1_score = compute_f1_score(preci, recall)\n",
    "    hsdrf = hausdorff(true_bkps, pred_bkps)\n",
    "    metrics_dic[\"precision\"]['raw'].append(round(preci, 4))\n",
    "    metrics_dic[\"recall\"]['raw'].append(round(recall, 4))\n",
    "    metrics_dic[\"f1_score\"]['raw'].append(round(f1_score, 4))\n",
    "    metrics_dic[\"hausdorff\"]['raw'].append(int(hsdrf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_add_stat_on_metrics(model_metrics:dict):\n",
    "    for model_res in model_metrics.values():\n",
    "        for metric_name, res in model_res.items():\n",
    "            model_res[metric_name]['mean'] = round(np.mean(res['raw']), ndigits=4)\n",
    "            model_res[metric_name]['std'] = round(np.std(res['raw']), ndigits=4)\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(metrics_per_models, stats, dir, comment=''):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    to_save = {\"date_time\": now, 'comment': comment}\n",
    "    to_save[\"hyper-parameters\"] = stats\n",
    "    to_save[\"results\"] = metrics_per_models\n",
    "    to_save = turn_all_list_of_dict_into_str(to_save)\n",
    "    create_parent_and_dump_json(dir, now + '.json', to_save, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin):\n",
    "    # initialization\n",
    "    stat_per_metric_per_cost_func = {}\n",
    "    for cost_func in cost_func_keys:\n",
    "        stat_per_metric_per_cost_func[cost_func] = {}\n",
    "        for metric in metrics_keys:\n",
    "            stat_per_metric_per_cost_func[cost_func][metric] = {\"mean\": [], \"std\": [], \"raw\": []}\n",
    "    # parsing metrics file to store results adequately\n",
    "    for folder_name in res_folder_list:\n",
    "        file_metric = open_json(os.path.join(folder_name, f\"metrics_{preci_margin}.json\"))\n",
    "        for cost_func in cost_func_keys:\n",
    "            for metric in metrics_keys:\n",
    "                mean = file_metric[cost_func][metric][\"mean\"]\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"mean\"].append(mean)\n",
    "                std = file_metric[cost_func][metric][\"std\"]\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"std\"].append(std)\n",
    "                raw_str = file_metric[cost_func][metric][\"raw\"]\n",
    "                raw = turn_str_of_list_into_list_of_float(raw_str)\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"raw\"].append(raw)\n",
    "    return stat_per_metric_per_cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.08\n",
    "shift = 0.1\n",
    "error_kw = {\"capsize\": 5, \"elinewidth\": 0.8, \"capthick\": 2}\n",
    "\n",
    "def plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, met_name, ax, to_label):\n",
    "    plot_x_coord = np.linspace(0, 1, num=(len(abscissa)+1))[:-1]\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        label = None\n",
    "        if to_label:\n",
    "            label = cost_func\n",
    "        color = colors_per_cost_func[cost_func]\n",
    "        # bar plot based on the statistics\n",
    "        x = plot_x_coord + shift*i\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.bar(x = x, height=y, yerr=y_err, label=label, width=bar_width, error_kw=error_kw, color=color)\n",
    "        # scatter plot based on the raw values\n",
    "        raw = np.array(res_dic[cost_func][met_name][\"raw\"])\n",
    "        raw.flatten()\n",
    "        x_scat_val = []\n",
    "        for x_id in range(len(x)):\n",
    "            x_scat_val = x_scat_val + [x[x_id]] * len(raw[x_id])\n",
    "        ax.scatter(x=x_scat_val, y=raw, alpha=0.3, c='k', s=10)\n",
    "    ax.set_xticks(plot_x_coord + shift*len(cost_func_keys)/2 - shift/2, abscissa)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_errorbar_manually(abscissa, y, yerr, marker, linestyle, linewidth, markersize, color, ax):\n",
    "    # create top and bottom lim\n",
    "    y_err_top = [y_pos + yerr_val for y_pos, yerr_val in zip(y, yerr)]\n",
    "    y_err_bottom = [y_pos - yerr_val for y_pos, yerr_val in zip(y, yerr)]\n",
    "    # add the top and bottom markers\n",
    "    ax.scatter(abscissa, y_err_top, s=markersize, c=color, marker=marker)\n",
    "    ax.scatter(abscissa, y_err_bottom, s=markersize, c=color, marker=marker)\n",
    "    # add the vertical lines\n",
    "    ax.vlines(x=abscissa, ymin=y_err_bottom, ymax=y_err_top, color=color, linewidth=linewidth, linestyle=linestyle)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_errorbar(abscissa_pos, cost_func_keys, color, markers, linestyles, met_name, res_dic, ax):\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.plot(abscissa_pos, y, c=color, marker=markers[i], markersize=7, linewidth=2, linestyle=linestyles[i])\n",
    "        add_errorbar_manually(abscissa_pos, y, y_err, marker=markers[i], linestyle=linestyles[i], linewidth=1, markersize=20, color=color, ax=ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "preci_margin = 5\n",
    "res_folder_list = [\n",
    "    \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop002\",\n",
    "    \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop005\",\n",
    "    \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop01\",\n",
    "    \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop02\",\n",
    "]\n",
    "\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "markers = ['o', \"s\"]\n",
    "linestyles = [\"dashed\", \"dotted\"]\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "abscissa_pos =  [2, 4, 6, 8]\n",
    "graph_types = [\"exp geo\", \"KNN geo\"]\n",
    "graph_colors = ['darkorange', 'dodgerblue']\n",
    "shift = 0.2\n",
    "\n",
    "res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin)\n",
    "\n",
    "##### PLOTTING\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(7*len(metrics_keys), 5)) #, layout='constrained')\n",
    "\n",
    "for i in range(len(graph_types)):\n",
    "    x_coords = [x + i*shift for x in abscissa_pos]\n",
    "    draw_ticks = i==0\n",
    "    plot_scatter_errorbar(x_coords, cost_func_keys, graph_colors[i], markers, linestyles, \"f1_score\", res_dic, ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Number of nodes\")\n",
    "    axes[0].set_title('F1 SCORE per cost function')\n",
    "    plot_scatter_errorbar(x_coords, cost_func_keys, graph_colors[i], markers, linestyles, \"hausdorff\", res_dic, ax=axes[1])\n",
    "    axes[1].set_xlabel(\"Number of nodes\")\n",
    "    axes[1].set_title('Haussdorff per cost function')\n",
    "    axes[1].set_ylim(bottom=0)\n",
    "\n",
    "axes[0].set_xticks(abscissa_pos, abscissa_pos)\n",
    "axes[1].set_xticks(abscissa_pos, abscissa_pos)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='k', lw=0.01, marker=markers[0], linestyle=linestyles[0], label=cost_func_keys[0]),\n",
    "    Line2D([0], [0], color='k', lw=0.01, marker=markers[1], linestyle=linestyles[1], label=cost_func_keys[1]),\n",
    "    Line2D([0], [0], color='darkorange', lw=3, label=\"Exp sim geo graphs\"),\n",
    "]               \n",
    "fig.suptitle(\"Comparison\")\n",
    "fig.subplots_adjust(bottom=0.17)\n",
    "fig.legend(handles=legend_elements, loc=\"lower center\", ncols=len(legend_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "res_folder_list = [\n",
    "    \"results/synthetic/A_within_hypothesis/A_5/geo_min_gap_weighted\",\n",
    "    \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop01\",\n",
    "    \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop02\"\n",
    "]\n",
    "abscissa =  [0.05, 0.1, 0.2]\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\", \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"precision\", \"hausdorff\"]\n",
    "res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin=5)\n",
    "\n",
    "##### PLOTTING\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(5*len(metrics_keys), 6)) #, layout='constrained')\n",
    "# precision\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, \"precision\", axes[0], to_label=True)\n",
    "axes[0].set_xlabel(\"proportion\")\n",
    "axes[0].set_title('Precision per cost function')\n",
    "# haussdorff\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, \"hausdorff\", axes[1],  to_label=False)\n",
    "axes[1].set_xlabel(\"proportion\")\n",
    "axes[1].set_title('Haussdorff per cost function')\n",
    "axes[1].set_ylim(bottom=0)\n",
    "\n",
    "fig.suptitle(\"Comparison\")\n",
    "fig.legend(loc=(0.3, 0), ncols=len(cost_func_keys))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data verifying the hypothesis of the model\n",
    "\n",
    "In this experiment, we generate data according to the hypothesis presented in the [problem formulation](#problem-formulation) and we compare our method to the cost function for standard covariance change detection in Gaussian models (that is supposed to cover our hypothesis). More precisely, we randomly generate a graph and a corresponding multivariate Gaussian signal, undergoing a (known) random number of covariance change points. The comparison between the two methods relies on the precision / recall and Hausdorff metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_in_hyp_with_max_tries(G:nx.Graph, signal_rng:np.random.Generator, n_bkps_max, hyp:seg_length, n_samples:int, diag_cov_max=1):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = get_min_size_for_hyp(n_dims=n_dims, hyp=hyp)\n",
    "    bkps = draw_bkps_with_gap_constraint(n_samples, min_size, signal_rng, n_bkps_max)\n",
    "    # generate the signal\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 3\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "# G = generate_random_er_graphs(graph_rng, nx_graph_seed, max_n_nodes=30)\n",
    "G, _ = generate_random_weighted_geographic_graph(graph_rng, min_n_nodes=40, max_n_nodes=50, dist_threshold=0.30)\n",
    "bkps, s = generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_samples=1000, diag_cov_max=10, hyp='min', n_bkps_max=10)\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(mosaic='ABB', figsize=(14, 3))\n",
    "nx.draw_networkx(G, ax=axes['A'])\n",
    "for i in range(6):\n",
    "    axes['B'].plot(10*i+s[:, i])\n",
    "axes['B'].set_yticks([])\n",
    "for bkp in bkps[:-1]:\n",
    "    axes['B'].axvline(x=bkp, c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data not verifying the hypothesis of the model at all\n",
    "\n",
    "In what follows, we work with signals verifying hypothesis 1 from the [the problem formulation](#problem-formulation), but not respecting the second hypothesis. More precisely, we will generate covariance matrices that are diagonalizable in a basis different from the Fourier basis $U$. To do so, we generate a graph $G'$  different from the graph $G$ used to compute the cost function. Then, we apply the same process and formula as previously described to generate the signal but using the eigenvectors $U'$ of the laplacian matrix $L'$ of $G'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_from_other_basis(G:nx.Graph, signal_rng:np.random.Generator, hyp:seg_length, n_bkps_max, n_samples, diag_cov_max=1):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = get_min_size_for_hyp(n_dims=n_dims, hyp=hyp)\n",
    "    bkps = draw_bkps_with_gap_constraint(n_samples, min_size, signal_rng, n_bkps_max)\n",
    "    # generate another graph to compute the signal covariance matrices\n",
    "    G_for_cov = generate_random_er_graphs(signal_rng, NX_GRAPH_SEED, max_n_nodes=n_dims, min_n_nodes=n_dims, min_edge_p=0.01, max_edge_p=1)\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G_for_cov).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 100\n",
    "NX_GRAPH_SEED = 1\n",
    "GRAPH_SEED = 2\n",
    "SIGNAL_SEED = 3\n",
    "MIN_N_NODES = 80\n",
    "MAX_N_NODES = 110\n",
    "DIST_THRESHOLD = 0.2\n",
    "N_SAMPLES = 1000\n",
    "DIAG_COV_MAX = 1\n",
    "MAX_N_BKPS = 10\n",
    "BKPS_GAP_CONSTRAINT: seg_length = \"minimal\" \n",
    "\n",
    "###################################################################\n",
    "NAME = \"weighted_geo_min_gap_cons\"\n",
    "data_dir = \"data/synthetic_data/exp_B_totally_out_of_hypothesis/B_3/\" + NAME\n",
    "desc = ['Minimal gap constraint, using weighted geographical graphs', 'The covariance matrix of the signal is diagonal in the Fourier basis of a random Erdos-Renyi graphs', 'Purpose: check performance when totally out of the model.']\n",
    "## FUNCTIONS USED FOR GRAPH GENERATION\n",
    "###################################################################\n",
    "\n",
    "# initialization\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "data_metadata = {\"description\": desc, \"commit hash\":get_git_head_short_hash(), \"graph_func\": generate_random_weighted_geographic_graph.__name__, \"signal_func\": generate_rd_signal_from_other_basis.__name__, \"n_iter\": N_EXP, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"n_samples\": N_SAMPLES, \"min_n_nodes\": MIN_N_NODES, \"max_n_nodes\": MAX_N_NODES, \"dist_threshold\": DIST_THRESHOLD, \"max_n_bkps\": MAX_N_BKPS, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"diag_cov_max\": DIAG_COV_MAX, \"n_nodes\": [], 'n_bkps': []}\n",
    "\n",
    "# data generation and saving\n",
    "for exp_id in range(N_EXP):\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, _ = generate_random_weighted_geographic_graph(graph_rng, min_n_nodes=MIN_N_NODES, max_n_nodes=MAX_N_NODES, dist_threshold=DIST_THRESHOLD)\n",
    "    data_metadata[\"n_nodes\"].append(int(G.number_of_nodes()))\n",
    "    gt_bkps, signal = generate_rd_signal_from_other_basis(G, signal_rng, hyp=BKPS_GAP_CONSTRAINT, n_bkps_max=MAX_N_BKPS, n_samples=N_SAMPLES, diag_cov_max=DIAG_COV_MAX)\n",
    "    data_metadata[\"n_bkps\"].append(len(gt_bkps)-1)\n",
    "    save_data(G, signal, gt_bkps, subdir)\n",
    "\n",
    "# storing metadata\n",
    "data_metadata = turn_all_list_of_dict_into_str(data_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"metadata.json\", data_metadata, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "DATE = None\n",
    "if DATE :\n",
    "    date = \"2024-06-18_17-22-19\"\n",
    "    data_dir = \"data/synthetic_data/exp_B_totally_out_of_hypothesis/B_1/\" + date\n",
    "\n",
    "################################################################\n",
    "results_dir = \"results/synthetic/B_totally_out_of_hypothesis/B_3/\" + NAME\n",
    "data_dir = data_dir\n",
    "################################################################\n",
    "\n",
    "data_stats = open_json(f\"{data_dir}/metadata.json\")\n",
    "data_stats[\"bkps\"] = {}\n",
    "data_stats[\"data_path\"] = data_dir\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "normal_results = {}\n",
    "\n",
    "subfold_list = [int(subdir) for subdir in os.listdir(data_dir) if '.' not in subdir]\n",
    "subfold_list.sort()\n",
    "for exp_id in tqdm(subfold_list, desc='Running experiment...'):\n",
    "    \n",
    "    exp_id = str(exp_id) # just for compatibility\n",
    "\n",
    "    # loading data\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, signal, gt_bkps = read_data(subdir)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes(), hyp=data_stats[\"bkps_gap_hyp\"])\n",
    "    data_stats[\"bkps\"][exp_id] = gt_bkps\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "\n",
    "# storing\n",
    "create_parent_and_dump_json(results_dir, \"data_stats.json\", turn_all_list_of_dict_into_str(data_stats), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 2\n",
    "\n",
    "#############################################################\n",
    "pred_dir = results_dir\n",
    "#############################################################\n",
    "\n",
    "# initialization\n",
    "data_stats = open_json(f\"{results_dir}/data_stats.json\")\n",
    "statio_pred_dic = open_json(f\"{results_dir}/statio_pred.json\")\n",
    "normal_pred_dic = open_json(f\"{results_dir}/normal_pred.json\")\n",
    "gt_bkps_dic = data_stats.pop('bkps')\n",
    "\n",
    "# output formatting\n",
    "metrics_dic = {}\n",
    "metrics_dic[\"pred_path\"] = results_dir\n",
    "metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in gt_bkps_dic.keys():\n",
    "    # compute metrics\n",
    "    normal_pred_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "    statio_pred_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "    gt_bkps = turn_str_of_list_into_list_of_int(gt_bkps_dic[exp_id])\n",
    "    compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "    compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "    # add time values\n",
    "    normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "    statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "full_results = compute_and_add_stat_on_metrics(full_results)\n",
    "full_results = turn_all_list_of_dict_into_str(full_results)\n",
    "create_parent_and_dump_json(pred_dir, f'metrics_{PRECI_RECALL_MARGIN}.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "PRECI_RECALL_MARGIN = 2\n",
    "res_folder_list = [\n",
    "    \"results/synthetic/B_totally_out_of_hypothesis/B_4/non_weighted_geo_min_gap_cons\",\n",
    "    \"results/synthetic/B_totally_out_of_hypothesis/B_3/geo_min_gap_cons_weighted\",\n",
    "    \"results/synthetic/B_totally_out_of_hypothesis/B_5/weighted_geo_min_gap_cons\",\n",
    "]\n",
    "abscissa =  [\"big non weighted\", \"big weighted\", \"medium weighted\"]\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\", \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"precision\", \"hausdorff\"]\n",
    "res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin=PRECI_RECALL_MARGIN)\n",
    "\n",
    "##### PLOTTING\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(6*len(metrics_keys), 5)) #layout='constrained')\n",
    "# precision\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, \"precision\", axes[0], to_label=True)\n",
    "axes[0].set_title('Precision per cost function')\n",
    "axes[0].set_ylim(bottom=0)\n",
    "# haussdorff\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, \"hausdorff\", axes[1],  to_label=False)\n",
    "axes[1].set_title('Haussdorff per cost function')\n",
    "axes[1].set_ylim(bottom=0)\n",
    "axes[1].set_ylim(top=300)\n",
    "\n",
    "N_min = 80\n",
    "N_max = 110\n",
    "graph_type = \"(non-) / weighted geographic\"\n",
    "gap_hypothesis = \"minimal\"\n",
    "N_iter = 100\n",
    "T = 1000\n",
    "\n",
    "# fig.suptitle(f\"T={T}, N_min={N_min}, N_max={N_max}, {graph_type} graphs, {N_iter} iterations, margin={PRECI_RECALL_MARGIN}\")\n",
    "fig.suptitle(f\"T={T}, [N_min, N_max] varies, {graph_type} graphs, {N_iter} iterations, margin={PRECI_RECALL_MARGIN}\")\n",
    "fig.legend(loc=(0.3, 0), ncols=len(cost_func_keys))\n",
    "\n",
    "save_path = \"images/results/synthetic/B_out_of_hyp/graph_type\"\n",
    "if not os.path.exists(save_path):\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=False)\n",
    "name = 'big_medium_geo.png'\n",
    "fig.savefig(os.path.join(save_path, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data verifying the hypothesis of the models, with node dropping to simulate breakdowns\n",
    "\n",
    "In what follows, we work with signals verifying the two hypothesis from the [the problem formulation](#problem-formulation). Additionally, we will randomly select a (very) small number of nodes and simulate the breakdown of the corresponding sensors by setting the value of the signal lying on this node to 0 for a random time length. \n",
    "\n",
    "More formally, let denote $\\eta_{max}$ the hyper-parameter corresponding to the maximal proportion of nodes that undergo a breakdown. We denote $N^{max}_{broken} = \\lfloor \\eta_{max} * N \\rfloor$. For each signal $(y_t)_{1 \\leq t \\leq T} \\in \\mathbb{R}^{T \\times N}$ , we draw $N_{broken}$ number of nodes undergoing a breakdown in $ ~  \\mathcal{U}niform([0, N^{max}_{broken}])$. Then, for a node $i$ undergoing a breakdown we apply:\n",
    "\n",
    "\\begin{equation}\n",
    "    t_{start} ~ \\sim ~ \\mathcal{U}niform([0, T-1]), ~ t_{end} ~ \\sim ~ \\mathcal{U}niform([t_{start}, T]) \\qquad \\forall ~  t \\in [t_{start}, t_{end}] ~ y_t^i = 0\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, by increasing the value of $\\eta_{max}$ we evaluate the robustness of our cost function with respect to brutal, isolated and uncorrelated mean changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_signal_to_simulate_breakdown(signal, signal_rng, n_breakdown_max):\n",
    "    # initialization\n",
    "    n_samples = signal.shape[0]\n",
    "    n_breakdown = signal_rng.integers(1, n_breakdown_max+1)\n",
    "    # randomly pick the location and time length of the breakdowns\n",
    "    breakdowns = {}\n",
    "    broken_node_ids = signal_rng.integers(0, signal.shape[1], size=(n_breakdown))\n",
    "    for node_id in broken_node_ids:\n",
    "        start = int(signal_rng.integers(0, n_samples-1))\n",
    "        end = int(signal_rng.integers(start, n_samples))\n",
    "        signal[start:end, node_id] = 0\n",
    "        breakdowns[int(node_id)] = (start, end)\n",
    "    return signal, breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G = generate_random_er_graphs(graph_rng, nx_graph_seed, max_n_nodes=30)\n",
    "bkps, s = generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "s, breakdowns = modify_signal_to_simulate_breakdown(s, signal_rng, n_breakdown_max=G.number_of_nodes()//10)\n",
    "\n",
    "print(\"The generated breakdowns are:\", breakdowns)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,3))\n",
    "for i in range(5):\n",
    "    ax.plot(10*i+s[:, i])\n",
    "for i, node_id in enumerate(breakdowns.keys()):\n",
    "    ax.plot(10*(i+5)+s[:, node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 100\n",
    "NX_GRAPH_SEED = 1\n",
    "GRAPH_SEED = 2\n",
    "SIGNAL_SEED = 3\n",
    "MIN_N_NODES = 40\n",
    "MAX_N_NODES = 50\n",
    "DIST_THRESHOLD = 0.3\n",
    "N_SAMPLES = 1000\n",
    "PROP_N_NODES_BREAKDOWN = 0.4\n",
    "DIAG_COV_MAX = 1\n",
    "MAX_N_BKPS = 10\n",
    "BKPS_GAP_CONSTRAINT: seg_length = \"minimal\" \n",
    "\n",
    "###################################################################\n",
    "NAME = \"min_gap_cons_non_weighted_geo_prop04\"\n",
    "data_dir = \"data/synthetic_data/exp_C_censor_breakdown_in_hypothesis/C_5/\" + NAME\n",
    "desc = ['Minimal gap constraint, using geographical non-weighted graphs, with high number of nodes', 'The signal is generated withing the model hypothesis but we simulate random rare censor breakdowns', 'Purpose: check if the cost functions are robust w.r.t isolated and rare censor breakdowns.']\n",
    "## FUNCTIONS USED FOR GRAPH GENERATION\n",
    "###################################################################\n",
    "\n",
    "# initialization\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "data_metadata = {\"description\": desc, \"commit hash\":get_git_head_short_hash(), \"graph_func\": generate_random_geographic_graph.__name__, \"signal_func\": generate_rd_signal_in_hyp_with_max_tries.__name__ + \" + \" + modify_signal_to_simulate_breakdown.__name__, \"n_iter\": N_EXP, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"n_samples\": N_SAMPLES, \"min_n_nodes\": MIN_N_NODES, \"max_n_nodes\": MAX_N_NODES, \"dist_threshold\": DIST_THRESHOLD, \"max_n_bkps\": MAX_N_BKPS, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"diag_cov_max\": DIAG_COV_MAX, \"prop_n_nodes_breakdown\": PROP_N_NODES_BREAKDOWN, \"n_nodes\": [], 'n_bkps': [], \"breakdowns\": []}\n",
    "\n",
    "# data generation and saving\n",
    "for exp_id in range(N_EXP):\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, _ = generate_random_geographic_graph(graph_rng, min_n_nodes=MIN_N_NODES, max_n_nodes=MAX_N_NODES, dist_threshold=DIST_THRESHOLD)\n",
    "    n_nodes = int(G.number_of_nodes())\n",
    "    data_metadata[\"n_nodes\"].append(n_nodes)\n",
    "    gt_bkps, signal = generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_bkps_max=MAX_N_BKPS, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, diag_cov_max=DIAG_COV_MAX)\n",
    "    signal, breakdowns = modify_signal_to_simulate_breakdown(signal, signal_rng, int(n_nodes*PROP_N_NODES_BREAKDOWN))\n",
    "    data_metadata[\"n_bkps\"].append(len(gt_bkps)-1)\n",
    "    data_metadata[\"breakdowns\"].append(breakdowns)\n",
    "    save_data(G, signal, gt_bkps, subdir)\n",
    "\n",
    "# storing metadata\n",
    "data_metadata = turn_all_list_of_dict_into_str(data_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"metadata.json\", data_metadata, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "DATE = None\n",
    "if DATE :\n",
    "    date = \"2024-06-18_17-22-19\"\n",
    "    data_dir = \"data/synthetic_data/exp_B_totally_out_of_hypothesis/B_1/\" + date\n",
    "\n",
    "################################################################\n",
    "results_dir = \"results/synthetic/C_censor_breakdown_in_hypothesis/C_5/\" + NAME\n",
    "data_dir = data_dir\n",
    "################################################################\n",
    "\n",
    "data_stats = open_json(f\"{data_dir}/metadata.json\")\n",
    "data_stats[\"bkps\"] = {}\n",
    "data_stats[\"data_path\"] = data_dir\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "normal_results = {}\n",
    "\n",
    "subfold_list = [int(subdir) for subdir in os.listdir(data_dir) if '.' not in subdir]\n",
    "subfold_list.sort()\n",
    "for exp_id in tqdm(subfold_list, desc='Running experiment...'):\n",
    "\n",
    "    exp_id = str(exp_id) # just for compatibility\n",
    "\n",
    "    # loading data\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, signal, gt_bkps = read_data(subdir)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes(), hyp=data_stats[\"bkps_gap_hyp\"])\n",
    "    data_stats[\"bkps\"][exp_id] = gt_bkps\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "\n",
    "# storing\n",
    "create_parent_and_dump_json(results_dir, \"data_stats.json\", turn_all_list_of_dict_into_str(data_stats), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 2\n",
    "\n",
    "#############################################################\n",
    "pred_dir = results_dir\n",
    "#############################################################\n",
    "\n",
    "# initialization\n",
    "data_stats = open_json(f\"{results_dir}/data_stats.json\")\n",
    "statio_pred_dic = open_json(f\"{results_dir}/statio_pred.json\")\n",
    "normal_pred_dic = open_json(f\"{results_dir}/normal_pred.json\")\n",
    "gt_bkps_dic = data_stats.pop('bkps')\n",
    "\n",
    "# output formatting\n",
    "metrics_dic = {}\n",
    "metrics_dic[\"pred_path\"] = results_dir\n",
    "metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in gt_bkps_dic.keys():\n",
    "    # compute metrics\n",
    "    normal_pred_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "    statio_pred_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "    gt_bkps = turn_str_of_list_into_list_of_int(gt_bkps_dic[exp_id])\n",
    "    compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "    compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "    # add time values\n",
    "    normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "    statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "full_results = compute_and_add_stat_on_metrics(full_results)\n",
    "full_results = turn_all_list_of_dict_into_str(full_results)\n",
    "create_parent_and_dump_json(pred_dir, f'metrics_{PRECI_RECALL_MARGIN}.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "PRECI_RECALL_MARGIN = 2\n",
    "res_folder_list = [\n",
    "    \"results/synthetic/C_censor_breakdown_in_hypothesis/C_3/min_gap_cons_weighted_geo_prop002\",\n",
    "    \"results/synthetic/C_censor_breakdown_in_hypothesis/C_3/min_gap_cons_weighted_geo_prop01\",\n",
    "    \"results/synthetic/C_censor_breakdown_in_hypothesis/C_3/min_gap_cons_weighted_geo_prop02\",\n",
    "    \"results/synthetic/C_censor_breakdown_in_hypothesis/C_3/min_gap_cons_weighted_geo_prop05\",\n",
    "]\n",
    "abscissa =  [\"eta = 0.02\", \"eta = 0.1\", \"eta = 0.2\", \"eta = 0.5\"]\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\", \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"precision\", \"hausdorff\"]\n",
    "res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin=PRECI_RECALL_MARGIN)\n",
    "\n",
    "##### PLOTTING\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(6*len(metrics_keys), 5)) #layout='constrained')\n",
    "# precision\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, \"precision\", axes[0], to_label=True)\n",
    "axes[0].set_title('Precision per cost function')\n",
    "axes[0].set_ylim(bottom=0)\n",
    "# haussdorff\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, \"hausdorff\", axes[1],  to_label=False)\n",
    "axes[1].set_title('Haussdorff per cost function')\n",
    "axes[1].set_ylim(bottom=0)\n",
    "axes[1].set_ylim(top=300)\n",
    "\n",
    "N_min = 80\n",
    "N_max = 110\n",
    "graph_type = \"weighted geographic\"\n",
    "gap_hypothesis = \"minimal\"\n",
    "N_iter = 100\n",
    "T = 1000\n",
    "\n",
    "fig.suptitle(f\"T={T}, N_min={N_min}, N_max={N_max}, {graph_type} graphs, {N_iter} iterations, margin={PRECI_RECALL_MARGIN}\")\n",
    "fig.legend(loc=(0.3, 0), ncols=len(cost_func_keys))\n",
    "\n",
    "save_path = \"images/results/synthetic/C_censor_breakdown/nodes_prop\"\n",
    "if not os.path.exists(save_path):\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=False)\n",
    "name = 'geo_weighted.png'\n",
    "fig.savefig(os.path.join(save_path, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Robustness with respect to (spatially and temporaly) independent additive white noise\n",
    "\n",
    "In this experiment, we keep using signals that verify our two hypothesis. Though we add a temporally independent white noise with scalar covariance matrix to such signal. More formally, we apply the change point detection algorithm to the signal $(y'_t)_{1 \\leq t \\leq T}$ defined by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\forall ~  t \\in [0, T] \\quad y'_t = y_t + e_t \\quad \\text{ with } \\quad e_t \\sim \\mathcal{N}_N(0, \\sigma)\n",
    "\\end{equation}\n",
    "\n",
    "We evaluate the performance of our cost function against increasing value of $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_diagonal_white_noise(signal_rng:np.random.Generator, signal, sigma):\n",
    "    n_samples , n_dims = signal.shape\n",
    "    cov_mat = sigma * np.eye(n_dims)\n",
    "    white_noise = signal_rng.multivariate_normal(np.zeros(n_dims), cov_mat, size=n_samples)\n",
    "    return signal + white_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G, _ = generate_random_weighted_geographic_graph(graph_rng, min_n_nodes=10, max_n_nodes=30, dist_threshold=0.3)\n",
    "bkps, s = generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "s_noise = add_diagonal_white_noise(signal_rng, s, sigma=3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,2))\n",
    "for i in range(5):\n",
    "    axes[0].plot(10*i+s[:, i])\n",
    "for i in range(5):\n",
    "    axes[1].plot(10*i+s_noise[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 100\n",
    "NX_GRAPH_SEED = 1\n",
    "GRAPH_SEED = 2\n",
    "SIGNAL_SEED = 3\n",
    "MIN_N_NODES = 40\n",
    "MAX_N_NODES = 50\n",
    "DIST_THRESHOLD = 0.3\n",
    "DIAG_COV_MAX = 1\n",
    "N_SAMPLES = 1000\n",
    "MAX_N_BKPS = 10\n",
    "SIGNAL_TO_NOISE_RATIO = 4\n",
    "BKPS_GAP_CONSTRAINT: seg_length = \"minimal\" \n",
    "\n",
    "###################################################################\n",
    "NAME = \"min_gap_cons_non_weighted_geo_snr4\"\n",
    "data_dir = \"data/synthetic_data/exp_D_in_hypo_diag_white_noise/D_5/\" + NAME\n",
    "desc = ['Minimal gap constraint, using non-weighted geographical graphs.', 'The signal is generated withing the model hypothesis but we add temporally and spatially independent white noise, meaning that the white noise follows multivariate gaussian distribution with scalar covariance matrix', 'Purpose: check if the cost functions are sensitive to additive white noise with scalar matrix covariance (none of the nodes are linked).']\n",
    "## FUNCTIONS USED FOR GRAPH GENERATION\n",
    "###################################################################\n",
    "\n",
    "# initialization\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "data_metadata = {\"description\": desc, \"commit hash\":get_git_head_short_hash(), \"graph_func\": generate_random_geographic_graph.__name__, \"signal_func\": generate_rd_signal_in_hyp_with_max_tries.__name__ + \" + \" + add_diagonal_white_noise.__name__, \"n_iter\": N_EXP, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"n_samples\": N_SAMPLES, \"min_n_nodes\": MIN_N_NODES, \"dist_threshold\": DIST_THRESHOLD, \"max_n_nodes\": MAX_N_NODES, \"max_n_bkps\": MAX_N_BKPS, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"diag_cov_max\": DIAG_COV_MAX, \"signal_to_noise_ratio\": SIGNAL_TO_NOISE_RATIO, \"n_nodes\": [], 'n_bkps': []}\n",
    "\n",
    "# data generation and saving\n",
    "for exp_id in range(N_EXP):\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, _ = generate_random_geographic_graph(graph_rng, min_n_nodes=MIN_N_NODES, max_n_nodes=MAX_N_NODES, dist_threshold=DIST_THRESHOLD)\n",
    "    n_nodes = int(G.number_of_nodes())\n",
    "    data_metadata[\"n_nodes\"].append(n_nodes)\n",
    "    gt_bkps, signal = generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_bkps_max=MAX_N_BKPS, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, diag_cov_max=DIAG_COV_MAX)\n",
    "    signal = add_diagonal_white_noise(signal_rng, signal, sigma=DIAG_COV_MAX/SIGNAL_TO_NOISE_RATIO)\n",
    "    data_metadata[\"n_bkps\"].append(len(gt_bkps)-1)\n",
    "    save_data(G, signal, gt_bkps, subdir)\n",
    "\n",
    "# storing metadata\n",
    "data_metadata = turn_all_list_of_dict_into_str(data_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"metadata.json\", data_metadata, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "DATE = None\n",
    "if DATE :\n",
    "    date = \"2024-06-18_17-22-19\"\n",
    "    data_dir = \"data/synthetic_data/exp_B_totally_out_of_hypothesis/B_1/\" + date\n",
    "\n",
    "################################################################\n",
    "results_dir = \"results/synthetic/D_in_hypo_diag_white_noise/D_5/\" + NAME\n",
    "data_dir = data_dir\n",
    "################################################################\n",
    "\n",
    "data_stats = open_json(f\"{data_dir}/metadata.json\")\n",
    "data_stats[\"bkps\"] = {}\n",
    "data_stats[\"data_path\"] = data_dir\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "normal_results = {}\n",
    "\n",
    "subfold_list = [int(subdir) for subdir in os.listdir(data_dir) if '.' not in subdir]\n",
    "subfold_list.sort()\n",
    "for exp_id in tqdm(subfold_list, desc='Running experiment...'):\n",
    "\n",
    "    exp_id = str(exp_id) # just for compatibility\n",
    "\n",
    "    # loading data\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, signal, gt_bkps = read_data(subdir)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes(), hyp=data_stats[\"bkps_gap_hyp\"])\n",
    "    data_stats[\"bkps\"][exp_id] = gt_bkps\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "\n",
    "# storing\n",
    "create_parent_and_dump_json(results_dir, \"data_stats.json\", turn_all_list_of_dict_into_str(data_stats), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 2\n",
    "\n",
    "#############################################################\n",
    "pred_dir = results_dir\n",
    "#############################################################\n",
    "\n",
    "# initialization\n",
    "data_stats = open_json(f\"{results_dir}/data_stats.json\")\n",
    "statio_pred_dic = open_json(f\"{results_dir}/statio_pred.json\")\n",
    "normal_pred_dic = open_json(f\"{results_dir}/normal_pred.json\")\n",
    "gt_bkps_dic = data_stats.pop('bkps')\n",
    "\n",
    "# output formatting\n",
    "metrics_dic = {}\n",
    "metrics_dic[\"pred_path\"] = results_dir\n",
    "metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in gt_bkps_dic.keys():\n",
    "    # compute metrics\n",
    "    normal_pred_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "    statio_pred_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "    gt_bkps = turn_str_of_list_into_list_of_int(gt_bkps_dic[exp_id])\n",
    "    compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "    compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "    # add time values\n",
    "    normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "    statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "full_results = compute_and_add_stat_on_metrics(full_results)\n",
    "full_results = turn_all_list_of_dict_into_str(full_results)\n",
    "create_parent_and_dump_json(pred_dir, f'metrics_{PRECI_RECALL_MARGIN}.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "PRECI_RECALL_MARGIN = 2\n",
    "res_folder_list = [\n",
    "    \"results/synthetic/D_in_hypo_diag_white_noise/D_5/min_gap_cons_non_weighted_geo_snr1\",\n",
    "    \"results/synthetic/D_in_hypo_diag_white_noise/D_5/min_gap_cons_non_weighted_geo_snr2\",\n",
    "    \"results/synthetic/D_in_hypo_diag_white_noise/D_5/min_gap_cons_non_weighted_geo_snr4\",\n",
    "]\n",
    "abscissa =  [\"SNR = 1\", \"SNR = 2\", \"SNR = 4\"]\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\", \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"precision\", \"hausdorff\"]\n",
    "res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin=PRECI_RECALL_MARGIN)\n",
    "\n",
    "##### PLOTTING\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(6*len(metrics_keys), 5)) #layout='constrained')\n",
    "# precision\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, \"precision\", axes[0], to_label=True)\n",
    "axes[0].set_title('Precision per cost function')\n",
    "axes[0].set_ylim(bottom=0)\n",
    "# haussdorff\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, \"hausdorff\", axes[1],  to_label=False)\n",
    "axes[1].set_title('Haussdorff per cost function')\n",
    "axes[1].set_ylim(bottom=0)\n",
    "axes[1].set_ylim(top=400)\n",
    "\n",
    "N_min = 40\n",
    "N_max = 50\n",
    "graph_type = \"non-weighted geographic\"\n",
    "gap_hypothesis = \"minimal\"\n",
    "N_iter = 100\n",
    "T = 1000\n",
    "\n",
    "fig.suptitle(f\"T={T}, N_min={N_min}, N_max={N_max}, {graph_type} graphs, {N_iter} iterations, margin={PRECI_RECALL_MARGIN}\")\n",
    "fig.legend(loc=(0.3, 0), ncols=len(cost_func_keys))\n",
    "\n",
    "save_path = \"images/results/synthetic/D_diag_white_noise/SNR\"\n",
    "if not os.path.exists(save_path):\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=False)\n",
    "name = 'geo_non_weighted.png'\n",
    "fig.savefig(os.path.join(save_path, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Robustness with respect to (temporaly) independent additive plain white noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Robustness with respect to the graph structure: modification of the connectivity\n",
    "\n",
    "In this experiment, we do not generate the signal $(y_t)_{1 \\leq t \\leq T}$ with the graph $G$ that is used to compute the cost function. Instead, we rather utilize the laplacian matrix $L_{noisy}$ of a noisy version $G_{noisy}$ of the original graph $G$.\n",
    "\n",
    "Let denote $M = |E|$ the number of edges in $G$ and $\\eta_{edge}$ the proportion of edges that we modify. We randomly remove $M_{remove} = \\lfloor M * \\eta_{edge} / 2 \\rfloor$ from the set E and we randomly add $M_{add} = \\lfloor M * \\eta_{edge} / 2 \\rfloor$ to $E$. In both cases, we select the edges randomly, but we always check that the resulting noisy graph $G_{noisy}$ still has the same number of nodes as $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_graph_connectivity(G:nx.Graph, edge_prop, graph_rng:np.random.Generator):\n",
    "    # initialization\n",
    "    edges = [e for e in G.edges()]\n",
    "    n_edges = len(edges)\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    n_modif_edges = int(n_edges * edge_prop)\n",
    "    # removing some random edges\n",
    "    edge_ids_to_remove = graph_rng.integers(low=0, high=n_edges, size=n_modif_edges//2)\n",
    "    edges_to_keep = [e for i, e in enumerate(edges) if i not in edge_ids_to_remove]\n",
    "    # adding some edges\n",
    "    new_edges_to_add = []\n",
    "    while len(new_edges_to_add) < n_modif_edges//2 :\n",
    "        new_edge = tuple(graph_rng.integers(low=0, high=n_nodes, size=2))\n",
    "        if new_edge[0] != new_edge[1]:\n",
    "            if new_edge not in new_edges_to_add:\n",
    "                new_edges_to_add.append(new_edge)\n",
    "    edges_new_graph = edges_to_keep + new_edges_to_add\n",
    "    new_G = nx.from_edgelist(edges_new_graph)\n",
    "    if new_G.number_of_nodes() == n_nodes:\n",
    "        return nx.from_edgelist(edges_new_graph)\n",
    "    else:\n",
    "        return modify_graph_connectivity(G, edge_prop, graph_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G, coord = generate_random_weighted_geographic_graph(graph_rng, min_n_nodes=80, max_n_nodes=110, dist_threshold=0.2)\n",
    "G_modif = modify_graph_connectivity(G, 0.1, graph_rng)\n",
    "bkps, s = generate_rd_signal_in_hyp_with_max_tries(G_modif, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "\n",
    "original_edges = set([(min(e), max(e)) for e in G.edges()])\n",
    "new_edges = set([(min(e), max(e)) for e in G_modif.edges()])\n",
    "\n",
    "added = new_edges.difference(original_edges)\n",
    "withdrawn  = original_edges.difference(new_edges)\n",
    "same = new_edges.intersection(original_edges)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# original graph\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=same, edge_color='k', ax=axes[0])\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=withdrawn, edge_color='r', width=3, ax=axes[0])\n",
    "nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[0], edgelist = [])\n",
    "# modified graph\n",
    "nx.draw_networkx_edges(G_modif, pos=coord, edgelist=same, edge_color='k')\n",
    "nx.draw_networkx_edges(G_modif, pos=coord, edgelist=added, width=3, edge_color='g')\n",
    "nx.draw_networkx(G_modif, with_labels=False, pos=coord, node_size=50, ax=axes[1], edgelist = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 100\n",
    "NX_GRAPH_SEED = 1\n",
    "GRAPH_SEED = 2\n",
    "SIGNAL_SEED = 3\n",
    "MIN_N_NODES = 40\n",
    "MAX_N_NODES = 50\n",
    "DIST_THRESHOLD = 0.3\n",
    "DIAG_COV_MAX = 1\n",
    "N_SAMPLES = 1000\n",
    "MAX_N_BKPS = 10\n",
    "EDGE_PROP_MODIF = 0.05\n",
    "BKPS_GAP_CONSTRAINT: seg_length = \"minimal\" \n",
    "\n",
    "###################################################################\n",
    "NAME = \"min_gap_cons_non_weighted_geo_edgeprop005\"\n",
    "data_dir = \"data/synthetic_data/exp_F_in_hypo_graph_connec_modif/F_5/\" + NAME\n",
    "desc = ['Minimal gap constraint, using non-weighted geographical graphs, for testing with a proportion of edges being modified.', 'The signal is generated within the model hypothesis but based on the Laplacian of a modified graph and we use the orginal graph to compute the cost function (that is, we use a noisy version of the graph to compute the cost function).', 'Purpose: check if the cost functions are sensitive to a noisy graph observation']\n",
    "## FUNCTIONS USED FOR GRAPH GENERATION\n",
    "###################################################################\n",
    "\n",
    "# initialization\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "data_metadata = {\"description\": desc, \"commit hash\":get_git_head_short_hash(), \"graph_func\": generate_random_geographic_graph.__name__, \"signal_func\": generate_rd_signal_in_hyp_with_max_tries.__name__ + \" + \" + modify_graph_connectivity.__name__ + \": the signal is generated with cov mat coming from the modified graph, and the cost function will be instantiated with the original graph\", \"n_iter\": N_EXP, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"n_samples\": N_SAMPLES, \"min_n_nodes\": MIN_N_NODES, \"dist_threshold\": DIST_THRESHOLD, \"max_n_nodes\": MAX_N_NODES, \"max_n_bkps\": MAX_N_BKPS, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"diag_cov_max\": DIAG_COV_MAX, \"modif_edges_prop\": EDGE_PROP_MODIF, \"n_nodes\": [], 'n_bkps': []}\n",
    "\n",
    "# data generation and saving\n",
    "for exp_id in range(N_EXP):\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, _ = generate_random_geographic_graph(graph_rng, min_n_nodes=MIN_N_NODES, max_n_nodes=MAX_N_NODES, dist_threshold=DIST_THRESHOLD)\n",
    "    G_modif = modify_graph_connectivity(G, EDGE_PROP_MODIF, graph_rng)\n",
    "    n_nodes = int(G.number_of_nodes())\n",
    "    data_metadata[\"n_nodes\"].append(n_nodes)\n",
    "    gt_bkps, signal = generate_rd_signal_in_hyp_with_max_tries(G_modif, signal_rng, n_bkps_max=MAX_N_BKPS, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, diag_cov_max=DIAG_COV_MAX)\n",
    "    data_metadata[\"n_bkps\"].append(len(gt_bkps)-1)\n",
    "    save_data(G, signal, gt_bkps, subdir)\n",
    "    adj_mat_modif = nx.to_numpy_array(G_modif)\n",
    "    with open(f'{subdir}/modif_graph_adj_mat.npy', 'wb+') as nx_f:\n",
    "        np.save(nx_f, adj_mat_modif, allow_pickle=False)\n",
    "\n",
    "# storing metadata\n",
    "data_metadata = turn_all_list_of_dict_into_str(data_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"metadata.json\", data_metadata, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "DATE = None\n",
    "if DATE :\n",
    "    date = \"2024-06-18_17-22-19\"\n",
    "    data_dir = \"data/synthetic_data/exp_B_totally_out_of_hypothesis/B_1/\" + date\n",
    "\n",
    "################################################################\n",
    "results_dir = \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/\" + NAME\n",
    "data_dir = data_dir\n",
    "################################################################\n",
    "\n",
    "data_stats = open_json(f\"{data_dir}/metadata.json\")\n",
    "data_stats[\"bkps\"] = {}\n",
    "data_stats[\"data_path\"] = data_dir\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "normal_results = {}\n",
    "\n",
    "subfold_list = [int(subdir) for subdir in os.listdir(data_dir) if '.' not in subdir]\n",
    "subfold_list.sort()\n",
    "for exp_id in tqdm(subfold_list, desc='Running experiment...'):\n",
    "\n",
    "    exp_id = str(exp_id) # just for compatibility\n",
    "\n",
    "    # loading data\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, signal, gt_bkps = read_data(subdir)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes(), hyp=data_stats[\"bkps_gap_hyp\"])\n",
    "    data_stats[\"bkps\"][exp_id] = gt_bkps\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "\n",
    "# storing\n",
    "create_parent_and_dump_json(results_dir, \"data_stats.json\", turn_all_list_of_dict_into_str(data_stats), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 5\n",
    "\n",
    "#############################################################\n",
    "# pred_dir = results_dir\n",
    "pred_dir = \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop002\"\n",
    "results_dir = pred_dir\n",
    "#############################################################\n",
    "\n",
    "# initialization\n",
    "data_stats = open_json(f\"{results_dir}/data_stats.json\")\n",
    "statio_pred_dic = open_json(f\"{results_dir}/statio_pred.json\")\n",
    "normal_pred_dic = open_json(f\"{results_dir}/normal_pred.json\")\n",
    "gt_bkps_dic = data_stats.pop('bkps')\n",
    "\n",
    "# output formatting\n",
    "metrics_dic = {}\n",
    "metrics_dic[\"pred_path\"] = results_dir\n",
    "metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []},  \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in gt_bkps_dic.keys():\n",
    "    # compute metrics\n",
    "    normal_pred_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "    statio_pred_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "    gt_bkps = turn_str_of_list_into_list_of_int(gt_bkps_dic[exp_id])\n",
    "    compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "    compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "    # add time values\n",
    "    normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "    statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "full_results = compute_and_add_stat_on_metrics(full_results)\n",
    "full_results = turn_all_list_of_dict_into_str(full_results)\n",
    "create_parent_and_dump_json(pred_dir, f'metrics_{PRECI_RECALL_MARGIN}.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "PRECI_RECALL_MARGIN = 5\n",
    "res_folder_list = [\n",
    "    \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop002\",\n",
    "    \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop005\",\n",
    "    \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop01\",\n",
    "    \"results/synthetic/F_in_hypo_graph_connec_modif/F_5/min_gap_cons_non_weighted_geo_edgeprop02\",\n",
    "]\n",
    "abscissa =  [\"eta = 0.02\", \"eta = 0.05\", \"eta = 0.1\", \"eta = 0.2\"]\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\", \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"precision\", \"hausdorff\"]\n",
    "res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin=PRECI_RECALL_MARGIN)\n",
    "\n",
    "##### PLOTTING\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(6*len(metrics_keys), 5)) #layout='constrained')\n",
    "# precision\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, \"precision\", axes[0], to_label=True)\n",
    "axes[0].set_title('Precision per cost function')\n",
    "axes[0].set_ylim(bottom=0)\n",
    "# haussdorff\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, \"hausdorff\", axes[1],  to_label=False)\n",
    "axes[1].set_title('Haussdorff per cost function')\n",
    "axes[1].set_ylim(bottom=0)\n",
    "axes[1].set_ylim(top=300)\n",
    "\n",
    "N_min = 40\n",
    "N_max = 50\n",
    "graph_type = \"non-weighted geographic\"\n",
    "gap_hypothesis = \"minimal\"\n",
    "N_iter = 100\n",
    "T = 1000\n",
    "\n",
    "fig.suptitle(f\"T={T}, N_min={N_min}, N_max={N_max}, {graph_type} graphs, {N_iter} iterations, margin={PRECI_RECALL_MARGIN}\")\n",
    "fig.legend(loc=(0.3, 0), ncols=len(cost_func_keys))\n",
    "\n",
    "save_path = \"images/results/synthetic/F_noisy_grap/edge_proportion\"\n",
    "if not os.path.exists(save_path):\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=False)\n",
    "name = 'geo_non_weighted.png'\n",
    "fig.savefig(os.path.join(save_path, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setting v.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data writing and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph(G:nx.Graph, path):\n",
    "    if os.path.exists(path):\n",
    "        raise FileExistsError\n",
    "    # save graph\n",
    "    adj_mat = nx.to_numpy_array(G)\n",
    "    with open(path, 'wb+') as nx_f:\n",
    "        np.save(nx_f, adj_mat, allow_pickle=False)\n",
    "\n",
    "def save_signal_and_bkps(signal:np.ndarray, bkps:List[int], dir, name):\n",
    "    path = os.path.join(dir, name)\n",
    "    if os.path.exists(f\"{path}_signal.npy\"):\n",
    "        raise FileExistsError\n",
    "    # save signal\n",
    "    with open(f\"{path}_signal.npy\", 'wb+') as np_f:\n",
    "        np.save(np_f, signal, allow_pickle=False)\n",
    "    # save bkps\n",
    "    bkps_str = [int(bkp) for bkp in bkps]\n",
    "    with open(f\"{path}_bkps.json\", 'w+') as f:\n",
    "        json.dump(bkps_str, f, indent=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_1(graph_path: str, signal_path: str, exp_id: int):\n",
    "    adj_mat = np.load(f\"{graph_path}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "    G = nx.from_numpy_array(adj_mat)\n",
    "    signal = np.load(f\"{signal_path}/{exp_id}_signal.npy\", allow_pickle=False)\n",
    "    bkps = open_json(f\"{signal_path}/{exp_id}_bkps.json\")\n",
    "    return G, signal, bkps\n",
    "\n",
    "def load_data(graph_path: str, signal_path: str, exp_id: int, hyp:seg_length):\n",
    "    G, signal, bkps = read_data_1(graph_path, signal_path, exp_id)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes(), hyp=hyp)\n",
    "    return G, signal, bkps, min_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation and vizualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_er_graphs_fixed_nodes_nb(params_rng, nx_graph_seed, n_nodes, target_deg, bandwidth_coef):\n",
    "    min_edge_p = (1- bandwidth_coef) * target_deg / (n_nodes -1)\n",
    "    max_edge_p = (1+ bandwidth_coef) * target_deg / (n_nodes -1)\n",
    "    edge_p = min_edge_p + (max_edge_p-min_edge_p) * params_rng.random()\n",
    "    G = nx.erdos_renyi_graph(n=n_nodes, p=edge_p, seed=nx_graph_seed)\n",
    "    params = {\"edge_prob\": edge_p}\n",
    "    return G, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "n_nodes = 40\n",
    "target_deg = 10\n",
    "bandwidth = 0.4\n",
    "\n",
    "av_deg = []\n",
    "for _ in range(5):\n",
    "    G, a = generate_random_er_graphs_fixed_nodes_nb(params_rng, _, n_nodes, target_deg, bandwidth)\n",
    "    coord = nx.spring_layout(G, seed=0)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])\n",
    "    degrees = [val for (node, val) in G.degree()]\n",
    "    av_deg.append(sum(degrees)/n_nodes)\n",
    "print(av_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_geographic_graph_with_gauss_kernel(params_rng, n_nodes, target_degree):\n",
    "    # generation of the nodes coordinates\n",
    "    nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "    # computation of the threshold for the exponential adjacency matrix\n",
    "    dist_mat_condensed = pdist(nodes_coord, metric='euclidean')\n",
    "    sigma = np.median(dist_mat_condensed)  # median heuristic\n",
    "    expsim_condensed = np.exp(-(dist_mat_condensed**2) / (sigma**2))\n",
    "    # ordering the values to find the right threshold\n",
    "    ordered_exp_sim = np.sort(expsim_condensed)\n",
    "    n_edge_for_target_deg = target_degree*n_nodes//2\n",
    "    threshold = ordered_exp_sim[-n_edge_for_target_deg+1]\n",
    "    # creating the corresponding adjacency matrix and graph\n",
    "    adj_mat_condensed = np.where(expsim_condensed > threshold, expsim_condensed, 0.0)\n",
    "    adj_mat = squareform(adj_mat_condensed)\n",
    "    G = nx.Graph(adj_mat)\n",
    "    return G, nodes_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "n_nodes = 80\n",
    "target_deg = 10\n",
    "av_deg = []\n",
    "to_plot_coords = []\n",
    "for _ in range(5):\n",
    "    G, coord = generate_random_geographic_graph_with_gauss_kernel(params_rng,  n_nodes=n_nodes, target_degree=target_deg)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])\n",
    "    degrees = [val for (node, val) in G.degree()]\n",
    "    av_deg.append(sum(degrees)/n_nodes)\n",
    "    to_plot_coords.append(coord)\n",
    "print(av_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libpysal.weights import KNN\n",
    "\n",
    "def generate_KNN_random_geographic_graph(params_rng, n_nodes, K):\n",
    "    # generation of the nodes coordinates \n",
    "    nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "    # computation of the KNN adjacency matrix\n",
    "    knn_weights = KNN.from_array(nodes_coord, K)\n",
    "    # build the graph\n",
    "    G_directed = knn_weights.to_networkx()\n",
    "    G = G_directed.to_undirected()\n",
    "    return G, nodes_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "n_nodes = 80\n",
    "K = 8\n",
    "av_deg = []\n",
    "for _ in range(5):\n",
    "    G, coord = generate_KNN_random_geographic_graph(params_rng,  n_nodes=n_nodes, K=K)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])\n",
    "    degrees = [val for (node, val) in G.degree()]\n",
    "    av_deg.append(sum(degrees)/n_nodes)\n",
    "print(av_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 1000\n",
    "GRAPH_SEED = 1\n",
    "N_NODES = 100\n",
    "TARGET_DEGREE = 10\n",
    "K_NEIGHBOUR = 8\n",
    "ER_BANDWIDTH = 0.4\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "\n",
    "# logging\n",
    "NAME = f\"ER_{N_NODES}_nodes_deg_{TARGET_DEGREE}_bandwidth_{ER_BANDWIDTH}\"\n",
    "data_dir = \"data_1/graphs/clean_ER_with_bandwidth/\" + NAME\n",
    "graphs_desc = \"ER graphs with fixed number of nodes. The edge probability is randomly drawn based on the target degree but also using a bandwidth parameter to allow for more diversity. Otherwise, for a given number of nodes and edge probability, the generated graphs would always be the same based on the networkx implementation.\"\n",
    "graph_gen_func = lambda rng, graph_seed : generate_random_er_graphs_fixed_nodes_nb(rng, graph_seed, n_nodes=N_NODES, target_deg=TARGET_DEGREE, bandwidth_coef=ER_BANDWIDTH)\n",
    "graphs_metadata = {\"datetime\": now, \"description\": graphs_desc, \"commit hash\": get_git_head_short_hash(), \"graph_func\": generate_random_er_graphs_fixed_nodes_nb.__name__, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": \"index\", \"n_nodes\": N_NODES, \"target_degree\": TARGET_DEGREE, \"bandwidth coefficient\": ER_BANDWIDTH}\n",
    "\n",
    "# output formatting\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=False)\n",
    "graphs_metadata = turn_all_list_of_dict_into_str(graphs_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"00_graphs_metadata.json\", graphs_metadata, indent=4)\n",
    "\n",
    "# graph generation\n",
    "for exp_id in range(N_EXP):\n",
    "    G, coords = graph_gen_func(graph_rng, exp_id)\n",
    "    save_graph(G, f\"{data_dir}/{exp_id}_mat_adj.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal and bkps generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_in_hyp(G:nx.Graph, signal_rng:np.random.Generator, hyp:seg_length, n_samples:int, diag_cov_max):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = get_min_size_for_hyp(n_dims=n_dims, hyp=hyp)\n",
    "    bkps = draw_bkps_with_gap_constraint(n_samples=n_samples, bkps_gap=min_size, bkps_rng=signal_rng, n_bkps_max=n_samples)\n",
    "    # generate the signal\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_in_hyp_with_fixed_min_size(G:nx.Graph, signal_rng:np.random.Generator, hyp:seg_length, n_samples:int, min_size_coef:int, diag_cov_max):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = int(min_size_coef * get_min_size_for_hyp(n_dims=n_dims, hyp=hyp))\n",
    "    bkps = draw_fixed_nb_bkps_with_gap_constraint(n_samples=n_samples, bkps_gap=min_size, bkps_rng=signal_rng)\n",
    "    # generate the signal\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_FOLDER = \"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "GRAPH_FOLDER_NAME = \"ER_40_nodes_deg_10_bandwidth_0.4\"\n",
    "GRAPH_PATH = os.path.join(GRAPH_FOLDER, GRAPH_FOLDER_NAME)\n",
    "SIGNAL_SEED = 3\n",
    "DIAG_COV_MAX = 1\n",
    "MIN_SEGMENT_LENGTH_COEF = 1.2\n",
    "SNR = 2\n",
    "N_SAMPLES = 1000\n",
    "BKPS_GAP_CONSTRAINT: seg_length = \"minimal\" \n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "sigma_noise = DIAG_COV_MAX / ( 10**(SNR / 10) )\n",
    "\n",
    "# logging\n",
    "NAME = GRAPH_FOLDER_NAME\n",
    "data_dir = f\"data_1/signal/within_hyp/varying_segment_length/minimal_x{MIN_SEGMENT_LENGTH_COEF}_\" + NAME\n",
    "signal_desc = \"'Data verifying the two hypothesis, with a given number of bkps fixed by a coefficient applied to the minimal segment length.\"\n",
    "signal_gen_func = lambda G : generate_rd_signal_in_hyp_with_fixed_min_size(G, signal_rng, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, min_size_coef=MIN_SEGMENT_LENGTH_COEF, diag_cov_max=DIAG_COV_MAX)\n",
    "signal_metadata = {\"datetime\": now, \"description\": signal_desc, \"commit hash\": get_git_head_short_hash(), \"graph_folder\": GRAPH_PATH, \"signal_seed\": SIGNAL_SEED, \"signal_gen_function\": generate_rd_signal_in_hyp.__name__, \"n_samples\": N_SAMPLES, \"diag_cov_max\": DIAG_COV_MAX, \"min_size_coef\": MIN_SEGMENT_LENGTH_COEF, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT}\n",
    "\n",
    "# output formatting\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=False)\n",
    "signal_metadata = turn_all_list_of_dict_into_str(signal_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"00_signal_metadata.json\", signal_metadata, indent=4)\n",
    "\n",
    "# signal generation\n",
    "for exp_id in range(len(os.listdir(GRAPH_PATH)) - 1):\n",
    "    adj_mat = np.load(f\"{GRAPH_PATH}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "    G = nx.from_numpy_array(adj_mat)\n",
    "    bkps, signal = signal_gen_func(G)\n",
    "    save_signal_and_bkps(signal, bkps, data_dir, str(exp_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statio_normal_cost(G: nx.Graph, signal: np.ndarray, gt_bkps: List[int], statio_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "    statio_results[exp_id][\"gt\"] = gt_bkps\n",
    "    statio_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_standard_mle_normal_cost(signal: np.ndarray, gt_bkps: List[int], normal_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "    normal_results[exp_id][\"gt\"] = gt_bkps\n",
    "    normal_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba version\n",
    "\n",
    "- do not use lists, fill-in pre-allocated np.ndarray\n",
    "- use functions rather than classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_station_normal_cost(signal, graph_laplacian_mat):\n",
    "    # computation of the graph fourier transform\n",
    "    _, eigvects = eigh(graph_laplacian_mat)\n",
    "    gft =  signal @ eigvects # equals signal.dot(eigvects) = eigvects.T.dot(signal.T).T\n",
    "    gft_mean = np.mean(gft, axis=0)\n",
    "    # computation of the per-segment cost utils\n",
    "    gft_square_cumsum = np.concatenate([np.zeros((1, signal.shape[1])), np.cumsum((gft - gft_mean[None, :])**2, axis=0)], axis=0)\n",
    "    return gft_square_cumsum.astype(np.float64)\n",
    "\n",
    "@njit\n",
    "def numba_statio_cost_func(start, end, gft_square_cumsum):\n",
    "    sub_square_sum = gft_square_cumsum[end] - gft_square_cumsum[start]\n",
    "    return np.float64(end  - start) * np.sum(np.log(sub_square_sum / (end - start)), dtype=np.float64)\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_statio_cost(n_bkps:int, min_size:int, data: np.ndarray) -> List[np.int64]:\n",
    "    n_samples = data.shape[0]\n",
    "    # if no bkp to find\n",
    "    if n_bkps == 0:\n",
    "        return np.array([1000], dtype=np.int64)\n",
    "    # full partitions costs\n",
    "    full_part_cost = np.inf * np.ones((n_bkps, n_samples, n_samples), dtype=np.float64)\n",
    "    # compute the segment cost with no bpk, for admissible segment only\n",
    "    for start in range(0, n_samples-min_size):\n",
    "        # until n_samples + 1 because the call to cost_function(start, end, data) computes over [start, end-1]\n",
    "        for end in range(start+min_size, n_samples):  \n",
    "            full_part_cost[0, start, end] = numba_statio_cost_func(start, end, data)\n",
    "    # compute the cost of the possible higher order partitions \n",
    "    for bkp_order in range(1, n_bkps):\n",
    "        min_multi_seg_length = (bkp_order + 1) * min_size\n",
    "        for start in range(0, n_samples-min_multi_seg_length):\n",
    "            for end in range(start + min_multi_seg_length, n_samples):\n",
    "                min_size_left_seg = min_multi_seg_length - min_size\n",
    "                full_part_cost[bkp_order, start, end] = np.min(full_part_cost[bkp_order-1, start, start+min_size_left_seg:end-min_size+1] + full_part_cost[0, start+min_size_left_seg:end-min_size+1, end])\n",
    "    # successively pick the bkps from the right-end of the whole signal\n",
    "    bkps = np.int64(n_samples-1) * np.ones(n_bkps+1, dtype=np.int64)\n",
    "    for bkp_id in range(n_bkps, 0, -1):\n",
    "        min_multi_seg_length = np.int64(bkp_id * min_size) \n",
    "        bkp_right = bkps[bkp_id]\n",
    "        bkp_left = min_multi_seg_length + np.argmin(full_part_cost[bkp_id-1, 0, min_multi_seg_length:bkp_right-min_size+1] + full_part_cost[0, min_multi_seg_length:bkp_right-min_size+1, bkp_right])\n",
    "        bkps[bkp_id-1] = bkp_left\n",
    "    return bkps\n",
    "\n",
    "def run_numba_statio_normal_cost(G: nx.Graph, signal: np.ndarray, gt_bkps: List[int], statio_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    graph_lapl_mat = nx.laplacian_matrix(G).toarray().astype(np.float64)\n",
    "    gft_square_cumsum = init_station_normal_cost(signal, graph_lapl_mat)\n",
    "    statio_bkps = numba_cpd_dynprog_statio_cost(len(gt_bkps)-1, signal.shape[1], gft_square_cumsum)\n",
    "    statio_bkps = [int(bkp) for bkp in statio_bkps]\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "    statio_results[exp_id][\"gt\"] = gt_bkps\n",
    "    statio_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import slogdet\n",
    "\n",
    "@njit\n",
    "def standard_normal_cost_func(start, end, signal):\n",
    "    sub = signal[start:end]\n",
    "    cov = np.cov(sub.T)\n",
    "    cov += 1e-6 * np.eye(signal.shape[1])\n",
    "    _, val = slogdet(cov)\n",
    "    return np.float64(val * (end - start))\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_mle_standard_cost(n_bkps:int, min_size:int, signal: np.ndarray) -> List[np.int64]:\n",
    "    n_samples = signal.shape[0]\n",
    "    # if no bkp to find\n",
    "    if n_bkps == 0:\n",
    "        return np.array([1000], dtype=np.int64)\n",
    "    # full partitions costs\n",
    "    full_part_cost = np.inf * np.ones((n_bkps, n_samples, n_samples), dtype=np.float64)\n",
    "    # compute the segment cost with no bpk, for admissible segment only\n",
    "    for start in range(0, n_samples-min_size+1):\n",
    "        # until n_samples + 1 because the call to cost_function(start, end, data) computes over [start, end-1]\n",
    "        for end in range(start+min_size, n_samples+1):  \n",
    "            full_part_cost[0, start, end] = standard_normal_cost_func(start, end, signal)\n",
    "    # compute the cost of the possible higher order partitions \n",
    "    for bkp_order in range(1, n_bkps):\n",
    "        min_multi_seg_length = (bkp_order + 1) * min_size\n",
    "        for start in range(0, n_samples-min_multi_seg_length):\n",
    "            for end in range(start + min_multi_seg_length, n_samples):\n",
    "                min_size_left_seg = min_multi_seg_length - min_size\n",
    "                full_part_cost[bkp_order, start, end] = np.min(full_part_cost[bkp_order-1, start, start+min_size_left_seg:end-min_size+1] + full_part_cost[0, start+min_size_left_seg:end-min_size+1, end])\n",
    "    # successively pick the bkps from the right-end of the whole signal\n",
    "    bkps = np.int64(n_samples) * np.ones(n_bkps+1, dtype=np.int64)\n",
    "    for bkp_id in range(n_bkps, 0, -1):\n",
    "        min_multi_seg_length = np.int64(bkp_id * min_size) \n",
    "        bkp_right = bkps[bkp_id]\n",
    "        bkp_left = min_multi_seg_length + np.argmin(full_part_cost[bkp_id-1, 0, min_multi_seg_length:bkp_right-min_size+1] + full_part_cost[0, min_multi_seg_length:bkp_right-min_size+1, bkp_right])\n",
    "        bkps[bkp_id-1] = bkp_left\n",
    "    return bkps\n",
    "\n",
    "def run_numba_standard_mle_normal_cost(signal: np.ndarray, gt_bkps: List[int], normal_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    normal_bkps = numba_cpd_dynprog_mle_standard_cost(len(gt_bkps) - 1, signal.shape[1], signal)\n",
    "    normal_bkps = [int(bkp) for bkp in normal_bkps]\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "    normal_results[exp_id][\"gt\"] = gt_bkps\n",
    "    normal_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"exp_geo_100_nodes_av_deg_10\"\n",
    "GRAPH_PATH = \"data_1/graphs/clean_exp_geo\"\n",
    "SIGNAL_PATH = \"data_1/signal/within_hyp/segment_length_minimal\"\n",
    "MAX_ID_SUBSET = 80\n",
    "RESULT_DIR = \"results_1/synthetic/within_hypothesis\"\n",
    "RESULT_NAME = \"80_exp\"\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "final_name = NAME + \"_\" + RESULT_NAME\n",
    "results_dir = os.path.join(RESULT_DIR, final_name)\n",
    "\n",
    "# logging\n",
    "graph_path = os.path.join(GRAPH_PATH, NAME)\n",
    "signal_path = os.path.join(SIGNAL_PATH, NAME)\n",
    "graph_metadata = open_json(f\"{graph_path}/00_graphs_metadata.json\")\n",
    "signal_metadata = open_json(f\"{signal_path}/00_signal_metadata.json\")\n",
    "seg_length_hyp = signal_metadata['bkps_gap_hyp']\n",
    "\n",
    "exp_desc = \"Within hypothesis, with no limit on the number of bkps. One of the goals is to evaluate the influence of the number of nodes.\"\n",
    "experiment_metadata = {\"datetime\": now, \"description\": exp_desc, \"commit hash\": get_git_head_short_hash(), \"graph folder\": GRAPH_PATH, \"graph metadata\": graph_metadata, \"signal folder\": SIGNAL_PATH, \"signal metadata\": signal_metadata, \"min segment length hypothesis\": seg_length_hyp, \"max id experiment subset\": MAX_ID_SUBSET}\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "normal_results = {}\n",
    "\n",
    "# running CPD algorithms\n",
    "for exp_id in tqdm(range(MAX_ID_SUBSET), desc='Running experiment...'):\n",
    "    exp_id = str(exp_id)\n",
    "    G, signal, gt_bkps, min_size = load_data(graph_path, signal_path, exp_id, seg_length_hyp)\n",
    "    run_numba_statio_normal_cost(G, signal, gt_bkps, statio_results)\n",
    "    run_numba_standard_mle_normal_cost(signal, gt_bkps, normal_results)\n",
    "\n",
    "create_parent_and_dump_json(results_dir, \"experiment_metadata.json\", turn_all_list_of_dict_into_str(experiment_metadata), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 2\n",
    "pred_path = \"results_1/synthetic/within_hypothesis/graph_types_and_number_of_nodes\"\n",
    "file_names = os.listdir(pred_path)\n",
    "PRED_FOLDER = [os.path.join(pred_path, file_name) for file_name in file_names]\n",
    "\n",
    "for pred_dir in PRED_FOLDER:\n",
    "\n",
    "    # fetching predictions\n",
    "    data_stats = open_json(f\"{pred_dir}/experiment_metadata.json\")\n",
    "    statio_pred_dic = open_json(f\"{pred_dir}/statio_pred.json\")\n",
    "    normal_pred_dic = open_json(f\"{pred_dir}/normal_pred.json\")\n",
    "    assert list(normal_pred_dic.keys()) == list(statio_pred_dic.keys())\n",
    "\n",
    "    # output formatting\n",
    "    metrics_dic = {}\n",
    "    metrics_dic[\"pred_path\"] = pred_dir\n",
    "    metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "    metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "    statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "    normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "    for exp_id in normal_pred_dic.keys():\n",
    "        # compute metrics\n",
    "        normal_pred_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "        statio_pred_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "        gt_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"gt\"])\n",
    "        compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "        compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "        # add time values\n",
    "        normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "        statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "    # results post-precessing and saving\n",
    "    full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "    full_results = compute_and_add_stat_on_metrics(full_results)\n",
    "    full_results[\"metadata\"] = metrics_dic\n",
    "    full_results = turn_all_list_of_dict_into_str(full_results)\n",
    "    create_parent_and_dump_json(pred_dir, f'metrics_{PRECI_RECALL_MARGIN}.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "PRECI_RECALL_MARGIN = 2\n",
    "pred_path = \"results_1/synthetic/within_hypothesis/graph_types_and_number_of_nodes\"\n",
    "file_names = os.listdir(pred_path)\n",
    "PRED_FOLDER = [os.path.join(pred_path, file_name) for file_name in file_names]\n",
    "\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "markers = ['o', \"s\"]\n",
    "linestyles = [\"dashed\", \"dotted\"]\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "abscissa_pos =  [20, 40, 60, 80, 100]\n",
    "graph_types = [\"exp_geo\", \"KNN_geo\", \"ER\"]\n",
    "graph_colors = ['darkorange', 'dodgerblue', 'darkorchid']\n",
    "shift = 3\n",
    "\n",
    "\n",
    "##### PLOTTING\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(7*len(metrics_keys), 5)) #, layout='constrained')\n",
    "\n",
    "for i in range(len(graph_types)):\n",
    "\n",
    "    # re-arrange the folder name so they have the right position for meaningful plotting\n",
    "    graph_name = graph_types[i]\n",
    "    res_folder_list = [folder_name for folder_name in PRED_FOLDER if graph_name in folder_name]\n",
    "    res_folder_list.sort()\n",
    "    res_folder_list.append(res_folder_list[0])\n",
    "    res_folder_list.pop(0)\n",
    "    res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, PRECI_RECALL_MARGIN)\n",
    "\n",
    "    x_coords = [x + i*shift for x in abscissa_pos]\n",
    "    plot_scatter_errorbar(x_coords, cost_func_keys, graph_colors[i], markers, linestyles, \"f1_score\", res_dic, ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Number of nodes\")\n",
    "    axes[0].set_title('F1 SCORE per cost function')\n",
    "    plot_scatter_errorbar(x_coords, cost_func_keys, graph_colors[i], markers, linestyles, \"hausdorff\", res_dic, ax=axes[1])\n",
    "    axes[1].set_xlabel(\"Number of nodes\")\n",
    "    axes[1].set_title('Haussdorff per cost function')\n",
    "    axes[1].set_ylim(bottom=0)\n",
    "\n",
    "x_tick_loc = [x + 1*shift for x in abscissa_pos]\n",
    "axes[0].set_xticks(x_tick_loc, abscissa_pos)\n",
    "axes[1].set_xticks(x_tick_loc, abscissa_pos)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='k', lw=0.01, marker=markers[0], linestyle=linestyles[0], label=cost_func_keys[0]),\n",
    "    Line2D([0], [0], color='k', lw=0.01, marker=markers[1], linestyle=linestyles[1], label=cost_func_keys[1]),\n",
    "    Line2D([0], [0], color='darkorange', lw=3, label=\"Exp sim geo graphs\"),\n",
    "]               \n",
    "fig.suptitle(\"Comparison\")\n",
    "fig.subplots_adjust(bottom=0.17)\n",
    "fig.legend(handles=legend_elements, loc=\"lower center\", ncols=len(legend_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"Izenman2008\">[Izenman2008]</a>\n",
    "Izenman Alan J. (2008). Introduction to Random-Matrix Theory [asc.ohio-state.edu]\n",
    "\n",
    "<a id=\"Ryan2023\">[Ryan2023]</a>\n",
    "Sean Ryan and Rebecca Killick. Detecting Changes in Covariance via Random Matrix Theory. Technometrics, 65(4):480–491, October 2023. Publisher: Taylor & Francis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
