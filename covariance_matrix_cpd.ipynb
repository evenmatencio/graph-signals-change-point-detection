{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance matrix change-point detection under graph stationarity assumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "\n",
    "Let $y = (y_1, \\ldots, y_t, \\ldots, y_T), y_t \\in \\mathbb{R}^{N}$ a graph signal lying on the nodes of the graph $G = (V, E, W)$, with $N =|V|$.\n",
    "\n",
    "We aim at detecting changes of the (spatial) covariance matrix $\\Sigma_t$ of the graph signals $y_t$. We assume that there exits an unknown set of change-points $\\Tau = (t_1, \\ldots, t_K) \\subset [1, T]$ with unknown cardinality such that the covariance matrix of the graph signals is constant over any segment $[t_k, t_{k+1}]$. We do the following hypothesis:\n",
    "\n",
    "1. the signals $y_t$ follow a multivariate Gaussian distribution with fixed covariance matrix over each segment and known mean $\\mu$, i.e:\n",
    "$$\\forall k \\in [1, K] ~ \\forall t \\in [t_k, t_{k+1}] \\quad y_t \\sim \\mathcal{N}(\\mu, \\Sigma_k)$$\n",
    "\n",
    "2. over each segment, the signals $y_t$ verify the second order wide-sense graph stationarity:\n",
    "$$\\forall k \\in [1, K] \\quad \\Sigma_k = U \\text{diag}(\\gamma_k)U^T $$\n",
    "\n",
    "where the matrix $U$ contains the eigenvectors of the graph combinatorial Laplacian matrix $L = D - W$ in its columns. \n",
    "\n",
    "The Graph Fourier Transform $\\tilde{y}$ of a signal $y$ is defined by $\\tilde{y} = U^T y $.\n",
    "\n",
    "\n",
    "Based on the above assumptions, the cost derived from the maximum log-likelihood over a segment $[a, b-1]$ writes:\n",
    "\n",
    "\\begin{align*}\n",
    "    c_s(y_{a}, \\ldots, y_{b-1}) = ~ & (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + ~ \\sum_{t=a}^{b-1} \\sum_{n=1}^N \\frac{\\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2}{\\hat{\\gamma}_{a.b}^{(n)}} = ~ (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + N(b-a)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\hat{\\mu}_{T}$ is the empirical mean of the process over $[0, T]$\n",
    "- $\\hat{\\gamma}_{a..b}$ is the (empirical) biased correlogram/periodogram of the process over $[a, b-1]$: $\\hat{\\gamma}_{a..b} = \\frac{1}{(b-a)} \\sum_{t=a}^{b-1} \\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.linalg import eigh\n",
    "from ruptures.base import BaseCost\n",
    "\n",
    "class CostGraphStatioNormal(BaseCost):\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"graph_sationary_normal_cost\"\n",
    "\n",
    "    def __init__(self, laplacian_mat) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            laplacian_mat (array): the discrete Laplacian matrix of the graph: D - W\n",
    "            where D is the diagonal matrix diag(d_i) of the node degrees and W the adjacency matrix\n",
    "        \"\"\"\n",
    "        self.graph_laplacian_mat = laplacian_mat\n",
    "        self.signal = None\n",
    "        self.gft_square_cumsum = None\n",
    "        self.gft_mean = None\n",
    "        self.min_size = laplacian_mat.shape[0]\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, signal):\n",
    "        \"\"\"Performs pre-computations for per-segment approximation cost.\n",
    "\n",
    "        NOTE: the number of dimensions of the signal and their ordering\n",
    "        must match those of the nodes of the graph.\n",
    "        The function eigh used below returns the eigenvector corresponding to \n",
    "        the ith eigenvalue in the ith column eigvect[:, i]\n",
    "\n",
    "        Args:\n",
    "            signal (array): of shape [n_samples, n_dim].\n",
    "        \"\"\"\n",
    "        self.signal = signal\n",
    "        # Computation of the GFSS\n",
    "        _, eigvects = eigh(self.graph_laplacian_mat)\n",
    "        gft =  signal @ eigvects # equals signal.dot(eigvects) = eigvects.T.dot(signal.T).T\n",
    "        self.gft_mean = np.mean(gft, axis=0)\n",
    "        # Computation of the per-segment cost utils\n",
    "        self.gft_square_cumsum = np.concatenate([np.zeros((1, signal.shape[1])), np.cumsum((gft - self.gft_mean[None, :])**2, axis=0)], axis=0)\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "\n",
    "        Returns:\n",
    "            float: segment cost\n",
    "        \"\"\"\n",
    "        if end - start < self.min_size:\n",
    "            raise ValueError(f'end - start shoud be higher than {self.min_size}')\n",
    "        sub_square_sum = self.gft_square_cumsum[end] - self.gft_square_cumsum[start]\n",
    "        return (end  - start) * np.sum(np.log(sub_square_sum / (end - start)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments: synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on the minimum distance between consecutive change points (minimum segment length)\n",
    "\n",
    "We require that the different change points $(t_1, \\ldots, t_K)$ verify:\n",
    "\n",
    "$$|t_{k+1} - t_k| >= l ~ \\forall k \\in [1, K-1] $$\n",
    "\n",
    "where $l$ can be seen as the minimum admissible segment length. In this paragraph we give a meaningful lower bound of this parameter. Such lower bound is related to the computation of the cost functions over the segments $[a, b] \\subset [0, T]$, namely the graph stationary normal cost function $c_s$ described above and the standard normal cost function $c_n$:\n",
    "\n",
    "- $ c_n(y_{a}, \\ldots, y_{b-1}) = (b - a) \\log  \\left[ \\det \\left( \\hat \\Sigma_{a..b} \\right) \\right]$\n",
    "- $ c_s(y_{a}, \\ldots, y_{b-1}) = (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} $\n",
    "\n",
    "Based on the formula of the spectrogram $\\hat{\\gamma}_{a.b}$ given in the introduction, there is no numerical constraints for the feasibility of the computation. However, the $\\log [ \\det ( \\cdot ) ]$ function used in the formula for $c_n$ should be applied to invertible matrices $\\Sigma_{a..b}$ only. Therefore, we should focus on the conditions under which the matrix:\n",
    "\n",
    "$$ \\hat \\Sigma_{a..b} = \\frac{1}{b-a} \\sum_{t=a}^{b-1} (y_t - \\mu_T) (y_t - \\mu_T)^T \\quad \\text{ with } y_t \\sim \\mathcal{N}(\\mu, \\Sigma)  $$\n",
    "\n",
    "is invertible. Actually, such conditions have already been clearly stated in different works from Random Matrix Theory (RMT). For instance, it is shown in [[Izenman2008](#Izenman2008)] that $n \\hat \\Sigma_{a..b} \\sim \\mathcal{W}(b-a, \\Sigma)$ follows the Wishart distribution. In this framework, it is possible to study the distribution of the eigenvalues of $\\hat \\Sigma_{a..b}$ and to deduce that: \n",
    "\n",
    "$$ \\text{If } b-a > N \\text{ with } N  \\text{ the dimension of } y_t, \\text{ then } \\hat \\Sigma_{a..b} \\text{ is almost surely invertible }   $$\n",
    "\n",
    "Conversely, it is possible to show that if $ b-a < N $ (the number of observations is lower then the number of variables), the matrix $\\hat \\Sigma_{a..b}$ is nalmost surely not invertible. This can be done by considering the family the first $(N+1)$ columns of $\\hat \\Sigma_{a..b}$.\n",
    "\n",
    "Thus, the right lower-bound $l$ should be $\\boxed{l = N}$, which is consistent with statement from [[Ryan2023](#Ryan2023)].\n",
    "\n",
    "Note: with segments of length $l$, one is not guaranteed to compute good estimates of the covariance matrix, but at least such computations is almost surely admissible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import networkx as nx\n",
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_head_short_hash() -> str:\n",
    "    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).decode('ascii').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_all_list_of_dict_into_str(data:dict):\n",
    "    new_dict = {}\n",
    "    for key, val in data.items():\n",
    "        if isinstance(val, list):\n",
    "            new_dict[key] = str(val)\n",
    "        elif isinstance(val, dict):\n",
    "            new_dict[key] = turn_all_list_of_dict_into_str(val)\n",
    "        else:\n",
    "            new_dict[key] = val\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_str_of_list_into_list_of_int(list_str):\n",
    "    assert list_str[0] == '[' and list_str[-1] == ']'\n",
    "    list_of_str = list_str[1:-1].split(',')\n",
    "    return [int(val_str) for val_str in list_of_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_and_dump_json(parent_dir, name, data, indent=None):\n",
    "    if os.path.exists(os.path.join(parent_dir, name)):\n",
    "        raise FileExistsError\n",
    "    if not os.path.exists(parent_dir):\n",
    "        Path(parent_dir).mkdir(parents=True, exist_ok=False)\n",
    "    with open(os.path.join(parent_dir, name), 'w+') as f:\n",
    "        json.dump(data, f, indent=indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(path):\n",
    "    with open(path, 'r+') as f:\n",
    "        content = json.load(f)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_er_graphs(params_rng, nx_graph_seed, max_n_nodes, min_n_nodes=10, min_edge_p=0.15, max_edge_p=0.5):\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    edge_p = min_edge_p + (max_edge_p-min_edge_p) * params_rng.random()\n",
    "    G = nx.erdos_renyi_graph(n=n_nodes, p=edge_p, seed=nx_graph_seed)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "for _ in range(5):\n",
    "    G = generate_random_er_graphs(params_rng, graph_seed, max_n_nodes=30)\n",
    "    coord = nx.spring_layout(G, seed=0)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_ba_graphs(params_rng, nx_graph_seed, min_n_nodes, max_n_nodes):\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    m = int(params_rng.normal(loc=n_nodes//10, scale=1))\n",
    "    G = nx.barabasi_albert_graph(n=n_nodes, m=m, seed=nx_graph_seed)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "for _ in range(5):\n",
    "    G = generate_random_ba_graphs(params_rng, graph_seed, min_n_nodes=10, max_n_nodes=30)\n",
    "    coord = nx.spring_layout(G, seed=0)\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def generate_random_geographic_graph(params_rng, min_n_nodes, max_n_nodes):\n",
    "    # generation of the nodes coordinates\n",
    "    n_nodes = params_rng.integers(low=min_n_nodes, high=max_n_nodes+1)\n",
    "    nodes_coord = params_rng.random(size=(n_nodes, 2))\n",
    "    # computation of the adjacency matrix based on distance threshold \n",
    "    dist_threshold = 0.35\n",
    "    dist_mat_condensed = pdist(nodes_coord, metric='euclidean')\n",
    "    adj_mat = squareform(dist_mat_condensed*(dist_mat_condensed < dist_threshold).astype(int))\n",
    "    G = nx.Graph(adj_mat)\n",
    "    return G, nodes_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seed = 0\n",
    "params_seed = 1\n",
    "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
    "params_rng = np.random.default_rng(seed=params_seed)\n",
    "for _ in range(5):\n",
    "    G, coord = generate_random_geographic_graph(params_rng, min_n_nodes=10, max_n_nodes=30)\n",
    "    coord_dic = {i: coord[i, :] for i in range(coord.shape[0])}\n",
    "    nx.draw_networkx(G, with_labels=False, pos=coord_dic, node_size=50, ax=axes[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaus_signal_with_cov_diag_in_basis(n_dims, n_samples, basis, signal_rng, diag_cov_max=1):\n",
    "    # randomly draw diagonal coef (in the fourier space)\n",
    "    diag_coefs = diag_cov_max * signal_rng.random(n_dims)\n",
    "    diag_mat = np.diag(diag_coefs)\n",
    "    # compute the corresponding covariance matrix and signal \n",
    "    cov_mat = basis @ diag_mat @ basis.T\n",
    "    signal = signal_rng.multivariate_normal(np.zeros(n_dims), cov_mat, size=n_samples)\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_length = Literal[\"large\", \"minimal\"]\n",
    "\n",
    "def get_min_size_for_hyp(n_dims, hyp:seg_length):\n",
    "    # the minimal segment length for admissible computations\n",
    "    min_size = n_dims\n",
    "    if hyp == \"large\":\n",
    "        #for segment long enough for good estimates\n",
    "        min_size = n_dims * (n_dims-1) / 2\n",
    "    return min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bkps_with_gap_constraint(n_samples, bkps_gap, bkps_rng, n_bkps_max, max_tries=10000):\n",
    "    # randomly pick an admissible number of bkps\n",
    "    n_bkps = bkps_rng.integers(low=1, high=min(n_bkps_max, n_samples // bkps_gap))\n",
    "    bkps = []\n",
    "    n_tries = 0\n",
    "    # select admissible randomly drawn bkps\n",
    "    while n_tries < max_tries and len(bkps) < n_bkps:\n",
    "        new_bkp = bkps_rng.integers(low=bkps_gap, high=n_samples-bkps_gap)\n",
    "        to_keep = True\n",
    "        for bkp in bkps:\n",
    "            if abs(new_bkp - bkp) < bkps_gap:\n",
    "                to_keep = False\n",
    "                break\n",
    "        if to_keep:\n",
    "            bkps.append(new_bkp)\n",
    "        n_tries+=1\n",
    "    bkps.sort()\n",
    "    return bkps + [n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data writting and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(G:nx.Graph, signal:np.ndarray, bkps:List[int], dir:str):\n",
    "    # create subfolder\n",
    "    Path(dir).mkdir(parents=True, exist_ok=False)\n",
    "    # save graph\n",
    "    adj_mat = nx.to_numpy_array(G)\n",
    "    with open(f'{dir}/graph_adj_mat.npy', 'wb+') as nx_f:\n",
    "        np.save(nx_f, adj_mat, allow_pickle=False)\n",
    "    # save signal\n",
    "    with open(f'{dir}/signal.npy', 'wb+') as np_f:\n",
    "        np.save(np_f, signal, allow_pickle=False)\n",
    "    # save bkps\n",
    "    bkps_str = [int(bkp) for bkp in bkps]\n",
    "    create_parent_and_dump_json(dir, \"bkps.json\", bkps_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str):\n",
    "    adj_mat = np.load(f\"{path}/graph_adj_mat.npy\", allow_pickle=False)\n",
    "    G = nx.from_numpy_array(adj_mat)\n",
    "    signal = np.load(f\"{path}/signal.npy\", allow_pickle=False)\n",
    "    bkps = open_json(f\"{path}/bkps.json\")\n",
    "    return G, signal, bkps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results computation and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruptures.metrics import precision_recall\n",
    "from ruptures.metrics import hausdorff\n",
    "\n",
    "def compute_and_update_metrics(true_bkps, pred_bkps, metrics_dic, prec_rec_margin):\n",
    "    preci, recall = precision_recall(true_bkps, pred_bkps, prec_rec_margin)\n",
    "    hsdrf = hausdorff(true_bkps, pred_bkps)\n",
    "    metrics_dic[\"precision\"]['raw'].append(round(preci, 4))\n",
    "    metrics_dic[\"recall\"]['raw'].append(round(recall, 4))\n",
    "    metrics_dic[\"hausdorff\"]['raw'].append(hsdrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_add_stat_on_metrics(model_metrics:dict):\n",
    "    for model_res in model_metrics.values():\n",
    "        for metric_name, res in model_res.items():\n",
    "            model_res[metric_name]['mean'] = round(np.mean(res['raw']), ndigits=4)\n",
    "            model_res[metric_name]['std'] = round(np.std(res['raw']), ndigits=4)\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(metrics_per_models, stats, dir, comment=''):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    to_save = {\"date_time\": now, 'comment': comment}\n",
    "    to_save[\"hyper-parameters\"] = stats\n",
    "    to_save[\"results\"] = metrics_per_models\n",
    "    to_save = turn_all_list_of_dict_into_str(to_save)\n",
    "    create_parent_and_dump_json(dir, now + '.json', to_save, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data verifying the hypothesis of the model\n",
    "\n",
    "In this experiment, we generate data according to the hypothesis presented in the [problem formulation](#problem-formulation) and we compare our method to the cost function for standard covariance change detection in Gaussian models (that is supposed to cover our hypothesis). More precisely, we randomly generate a graph and a corresponding multivariate Gaussian signal, undergoing a (known) random number of covariance change points. The comparison between the two methods relies on the precision / recall and Hausdorff metrics.\n",
    "\n",
    "More formally we generate Erdős–Rényi (ER) graphs with a random number of nodes $N$ uniformly drawn in $[N_{\\min}, N_{\\max}]$, that may vary according to the experiment, and a random edge probability $p$ uniformly drawn in $[0.15, 0.5]$. The latter was chosen so that the generated graphs empirically have a connectivity similar to real life graphs. Then, we pick an admissible number of change points $K$ depending on the signal length $T=1000$ and the minimum segment length $l = N$ by sampling $~  \\mathcal{U}([1, \\min(\\frac{T}{10}, \\frac{T}{l})])$. Finally, if $G$ denotes the running graph and $L = U \\Lambda U^T$ is the eigendecomposition of its laplacian matrix, the signal is generated as follows: \n",
    "\n",
    "$$ \\forall ~ k \\in [1, K-1] ~ \\forall  t \\in [t_k, t_{k+1}] \\quad  y_t \\sim \\mathcal{N}_N(0, \\Sigma_k) \\quad \\text{ with } \\quad \\Sigma_k = U \\text{diag}(\\gamma_k) U^T ~  \\text{ and } ~ \\gamma_k \\sim \\mathcal{U}niform_N([0, 1]) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_in_hyp(G:nx.Graph, signal_rng:np.random.Generator, n_bkps_max, hyp:seg_length, n_samples:int, diag_cov_max=1):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = get_min_size_for_hyp(n_dims=n_dims, hyp=hyp)\n",
    "    bkps = draw_bkps_with_gap_constraint(n_samples, min_size, signal_rng, n_bkps_max)\n",
    "    # generate the signal\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 3\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "# G = generate_random_er_graphs(graph_rng, nx_graph_seed, max_n_nodes=30)\n",
    "G, _ = generate_random_geographic_graph(graph_rng, min_n_nodes=10, max_n_nodes=30)\n",
    "bkps, s = generate_rd_signal_in_hyp(G, signal_rng, n_samples=1000, diag_cov_max=10, hyp='large', n_bkps_max=10)\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(mosaic='ABB', figsize=(16, 3))\n",
    "nx.draw_networkx(G, ax=axes['A'])\n",
    "for i in range(6):\n",
    "    axes['B'].plot(10*i+s[:, i])\n",
    "axes['B'].set_yticks([])\n",
    "for bkp in bkps[:-1]:\n",
    "    axes['B'].axvline(x=bkp, c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 50\n",
    "NX_GRAPH_SEED = 1\n",
    "GRAPH_SEED = 2\n",
    "SIGNAL_SEED = 3\n",
    "MAX_N_NODES = 25\n",
    "MIN_N_NODES = 10\n",
    "DIAG_COV_MAX = 1\n",
    "N_SAMPLES = 1000\n",
    "MAX_N_BKPS = 10\n",
    "BKPS_GAP_CONSTRAINT: seg_length = \"minimal\" \n",
    "\n",
    "###################################################################\n",
    "NAME = \"geo_min_gap_cons\"\n",
    "data_dir = \"data/synthetic_data/exp_A_within_hypothesis/A_2/\" + NAME\n",
    "desc = ['Minimal gap constraint, using geographical graphs', 'Data verifying the two hypothesis', 'Purpose: sanity-check when data fits into the model.']\n",
    "## FUNCTIONS USED FOR GRAPH GENERATION\n",
    "###################################################################\n",
    "\n",
    "# initialization\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "data_metadata = {\"description\": desc, \"commit hash\":get_git_head_short_hash(), \"graph_func\": generate_random_geographic_graph.__name__, \"signal_func\": generate_rd_signal_in_hyp.__name__, \"n_iter\": N_EXP, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"n_samples\": N_SAMPLES, \"min_n_nodes\": MIN_N_NODES, \"max_n_nodes\": MAX_N_NODES, \"max_n_bkps\": MAX_N_BKPS, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"diag_cov_max\": DIAG_COV_MAX, \"n_nodes\": [], 'n_bkps': []}\n",
    "\n",
    "# data generation and saving\n",
    "for exp_id in range(N_EXP):\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    # G = generate_random_er_graphs(graph_rng, NX_GRAPH_SEED, max_n_nodes=MAX_N_NODES)\n",
    "    G, _ = generate_random_geographic_graph(graph_rng, min_n_nodes=MIN_N_NODES, max_n_nodes=MAX_N_NODES)\n",
    "    data_metadata[\"n_nodes\"].append(int(G.number_of_nodes()))\n",
    "    gt_bkps, signal = generate_rd_signal_in_hyp(G, signal_rng, n_bkps_max=MAX_N_BKPS, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, diag_cov_max=DIAG_COV_MAX)\n",
    "    data_metadata[\"n_bkps\"].append(len(gt_bkps)-1)\n",
    "    save_data(G, signal, gt_bkps, subdir)\n",
    "\n",
    "# storing metadata\n",
    "data_metadata = turn_all_list_of_dict_into_str(data_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"metadata.json\", data_metadata, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "DATE = None\n",
    "if DATE :\n",
    "    date = \"2024-06-18_17-22-19\"\n",
    "    data_dir = \"data/synthetic_data/exp_A_within_hypothesis/A_1/\" + date\n",
    "\n",
    "################################################################\n",
    "results_dir = \"results/synthetic/A_within_hypothesis/A_2/\" + NAME\n",
    "data_dir = data_dir\n",
    "################################################################\n",
    "\n",
    "data_stats = open_json(f\"{data_dir}/metadata.json\")\n",
    "data_stats[\"bkps\"] = {}\n",
    "data_stats[\"data_path\"] = data_dir\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "normal_results = {}\n",
    "\n",
    "subfold_list = [int(subdir) for subdir in os.listdir(data_dir) if '.' not in subdir]\n",
    "subfold_list.sort()\n",
    "for exp_id in tqdm(subfold_list, desc='Running experiment...'):\n",
    "\n",
    "    exp_id = str(exp_id) # just for compatibility with dict formatting\n",
    "    \n",
    "    # loading data\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, signal, gt_bkps = read_data(subdir)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes(), hyp=data_stats[\"bkps_gap_hyp\"])\n",
    "    data_stats[\"bkps\"][exp_id] = gt_bkps\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "\n",
    "# storing\n",
    "create_parent_and_dump_json(results_dir, \"data_stats.json\", turn_all_list_of_dict_into_str(data_stats), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 2\n",
    "\n",
    "#############################################################\n",
    "pred_dir = results_dir\n",
    "#############################################################\n",
    "\n",
    "# initialization\n",
    "data_stats = open_json(f\"{results_dir}/data_stats.json\")\n",
    "statio_pred_dic = open_json(f\"{results_dir}/statio_pred.json\")\n",
    "normal_pred_dic = open_json(f\"{results_dir}/normal_pred.json\")\n",
    "gt_bkps_dic = data_stats.pop('bkps')\n",
    "\n",
    "# output formatting\n",
    "metrics_dic = {}\n",
    "metrics_dic[\"pred_path\"] = results_dir\n",
    "metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in gt_bkps_dic.keys():\n",
    "    # compute metrics\n",
    "    normal_pred_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "    statio_pred_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "    gt_bkps = turn_str_of_list_into_list_of_int(gt_bkps_dic[exp_id])\n",
    "    compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "    compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "    # add time values\n",
    "    normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "    statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "full_results = compute_and_add_stat_on_metrics(full_results)\n",
    "full_results = turn_all_list_of_dict_into_str(full_results)\n",
    "create_parent_and_dump_json(pred_dir, 'metrics.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data not verifying the hypothesis of the model at all\n",
    "\n",
    "In what follows, we work with signals verifying hypothesis 1 from the [the problem formulation](#problem-formulation), but not respecting the second hypothesis. More precisely, we will generate covariance matrices that are diagonalizable in a basis different from the Fourier one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rd_signal_from_other_basis(G:nx.Graph, signal_rng:np.random.Generator, hyp:seg_length, n_bkps_max, n_samples, diag_cov_max=1):\n",
    "    # randomly draw a set of admissible change points\n",
    "    n_dims = G.number_of_nodes()\n",
    "    min_size = get_min_size_for_hyp(n_dims=n_dims, hyp=hyp)\n",
    "    bkps = draw_bkps_with_gap_constraint(n_samples, min_size, signal_rng, n_bkps_max)\n",
    "    # generate another graph to compute the signal covariance matrices\n",
    "    G_for_cov = generate_random_er_graphs(signal_rng, NX_GRAPH_SEED, max_n_nodes=n_dims, min_n_nodes=n_dims, min_edge_p=0.01, max_edge_p=1)\n",
    "    _, eigvects = eigh(nx.laplacian_matrix(G_for_cov).toarray())\n",
    "    signal_gen_func = lambda size: generate_gaus_signal_with_cov_diag_in_basis(n_dims, size, eigvects, signal_rng, diag_cov_max)\n",
    "    signal = signal_gen_func(bkps[0])\n",
    "    # add each sub-segment\n",
    "    for i in range(1, len(bkps)):\n",
    "        sub_signal = signal_gen_func(bkps[i] - bkps[i-1])\n",
    "        signal = np.concatenate([signal, sub_signal], axis=0)\n",
    "    return bkps, signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 50\n",
    "NX_GRAPH_SEED = 1\n",
    "GRAPH_SEED = 2\n",
    "SIGNAL_SEED = 3\n",
    "MIN_N_NODES = 10\n",
    "MAX_N_NODES = 25\n",
    "N_SAMPLES = 1000\n",
    "DIAG_COV_MAX = 1\n",
    "MAX_N_BKPS = 10\n",
    "BKPS_GAP_CONSTRAINT: seg_length = \"minimal\" \n",
    "\n",
    "###################################################################\n",
    "NAME = \"geo_min_gap_cons\"\n",
    "data_dir = \"data/synthetic_data/exp_B_totally_out_of_hypothesis/B_2/\" + NAME\n",
    "desc = ['Minimal gap constraint, using geographical graphs', 'The covariance matrix of the signal is diagonal in the Fourier basis of a random Erdos-Renyi graphs', 'Purpose: check performance when totally out of the model.']\n",
    "## FUNCTIONS USED FOR GRAPH GENERATION\n",
    "###################################################################\n",
    "\n",
    "# initialization\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "data_metadata = {\"description\": desc, \"commit hash\":get_git_head_short_hash(), \"graph_func\": generate_random_geographic_graph.__name__, \"signal_func\": generate_rd_signal_from_other_basis.__name__, \"n_iter\": N_EXP, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"n_samples\": N_SAMPLES, \"min_n_nodes\": MIN_N_NODES, \"max_n_nodes\": MAX_N_NODES, \"max_n_bkps\": MAX_N_BKPS, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"diag_cov_max\": DIAG_COV_MAX, \"n_nodes\": [], 'n_bkps': []}\n",
    "\n",
    "# data generation and saving\n",
    "for exp_id in range(N_EXP):\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, _ = generate_random_geographic_graph(graph_rng, min_n_nodes=MIN_N_NODES, max_n_nodes=MAX_N_NODES)\n",
    "    data_metadata[\"n_nodes\"].append(int(G.number_of_nodes()))\n",
    "    gt_bkps, signal = generate_rd_signal_from_other_basis(G, signal_rng, hyp=BKPS_GAP_CONSTRAINT, n_bkps_max=MAX_N_BKPS, n_samples=N_SAMPLES, diag_cov_max=DIAG_COV_MAX)\n",
    "    data_metadata[\"n_bkps\"].append(len(gt_bkps)-1)\n",
    "    save_data(G, signal, gt_bkps, subdir)\n",
    "\n",
    "# storing metadata\n",
    "data_metadata = turn_all_list_of_dict_into_str(data_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"metadata.json\", data_metadata, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "DATE = None\n",
    "if DATE :\n",
    "    date = \"2024-06-18_17-22-19\"\n",
    "    data_dir = \"data/synthetic_data/exp_B_totally_out_of_hypothesis/B_1/\" + date\n",
    "\n",
    "################################################################\n",
    "results_dir = \"results/synthetic/B_totally_out_of_hypothesis/B_2/\" + NAME\n",
    "data_dir = data_dir\n",
    "################################################################\n",
    "\n",
    "data_stats = open_json(f\"{data_dir}/metadata.json\")\n",
    "data_stats[\"bkps\"] = {}\n",
    "data_stats[\"data_path\"] = data_dir\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "normal_results = {}\n",
    "\n",
    "subfold_list = [int(subdir) for subdir in os.listdir(data_dir) if '.' not in subdir]\n",
    "subfold_list.sort()\n",
    "for exp_id in tqdm(subfold_list, desc='Running experiment...'):\n",
    "    \n",
    "    exp_id = str(exp_id) # just for compatibility\n",
    "\n",
    "    # loading data\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, signal, gt_bkps = read_data(subdir)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes(), hyp=data_stats[\"bkps_gap_hyp\"])\n",
    "    data_stats[\"bkps\"][exp_id] = gt_bkps\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "\n",
    "# storing\n",
    "create_parent_and_dump_json(results_dir, \"data_stats.json\", turn_all_list_of_dict_into_str(data_stats), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 2\n",
    "\n",
    "#############################################################\n",
    "pred_dir = results_dir\n",
    "#############################################################\n",
    "\n",
    "# initialization\n",
    "data_stats = open_json(f\"{results_dir}/data_stats.json\")\n",
    "statio_pred_dic = open_json(f\"{results_dir}/statio_pred.json\")\n",
    "normal_pred_dic = open_json(f\"{results_dir}/normal_pred.json\")\n",
    "gt_bkps_dic = data_stats.pop('bkps')\n",
    "\n",
    "# output formatting\n",
    "metrics_dic = {}\n",
    "metrics_dic[\"pred_path\"] = results_dir\n",
    "metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in gt_bkps_dic.keys():\n",
    "    # compute metrics\n",
    "    normal_pred_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "    statio_pred_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "    gt_bkps = turn_str_of_list_into_list_of_int(gt_bkps_dic[exp_id])\n",
    "    compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "    compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "    # add time values\n",
    "    normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "    statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "full_results = compute_and_add_stat_on_metrics(full_results)\n",
    "full_results = turn_all_list_of_dict_into_str(full_results)\n",
    "create_parent_and_dump_json(pred_dir, 'metrics.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data verifying the hypothesis of the models, with node dropping to simulate breakdowns\n",
    "\n",
    "In what follows, we work with signals verifying the two hypothesis from the [the problem formulation](#problem-formulation). Additionally, we will randomly select a (very) small number of nodes and simulate the breakdown of the corresponding sensors by setting the value of signal lying on this node to 0 for a random time length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_signal_to_simulate_breakdown(signal, signal_rng, n_breakdown_max):\n",
    "    # initialization\n",
    "    n_samples = signal.shape[0]\n",
    "    n_breakdown = signal_rng.integers(1, n_breakdown_max+1)\n",
    "    # randomly pick the location and time length of the breakdowns\n",
    "    breakdowns = {}\n",
    "    broken_node_ids = signal_rng.integers(0, signal.shape[1], size=(n_breakdown))\n",
    "    for node_id in broken_node_ids:\n",
    "        start = signal_rng.integers(0, n_samples-1)\n",
    "        end = signal_rng.integers(start, n_samples)\n",
    "        signal[start:end, node_id] = 0\n",
    "        breakdowns[node_id] = (start, end)\n",
    "    return signal, breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G = generate_random_er_graphs(graph_rng, nx_graph_seed, max_n_nodes=30)\n",
    "bkps, s = generate_rd_signal_in_hyp(G, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "s, breakdowns = modify_signal_to_simulate_breakdown(s, signal_rng, n_breakdown_max=G.number_of_nodes()//10)\n",
    "\n",
    "print(\"The generated breakdowns are:\", breakdowns)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,3))\n",
    "for i in range(5):\n",
    "    ax.plot(10*i+s[:, i])\n",
    "for i, node_id in enumerate(breakdowns.keys()):\n",
    "    ax.plot(10*(i+5)+s[:, node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 50\n",
    "NX_GRAPH_SEED = 1\n",
    "GRAPH_SEED = 2\n",
    "SIGNAL_SEED = 3\n",
    "MIN_N_NODES = 10\n",
    "MAX_N_NODES = 25\n",
    "N_SAMPLES = 1000\n",
    "DIAG_COV_MAX = 1\n",
    "MAX_N_BKPS = 10\n",
    "BKPS_GAP_CONSTRAINT: seg_length = \"minimal\" \n",
    "\n",
    "###################################################################\n",
    "NAME = \"min_gap_cons\"\n",
    "data_dir = \"data/synthetic_data/exp_C_censor_breakdown_in_hypothesis/C_2/\" + NAME\n",
    "desc = ['Minimal gap constraint, using geographical graphs', 'The signal is generated withing the model hypothesis but we simulate random rare censor breakdowns', 'Purpose: check if the cost functions are robust w.r.t isolated and rare censor breakdowns.']\n",
    "## FUNCTIONS USED FOR GRAPH GENERATION\n",
    "###################################################################\n",
    "\n",
    "# initialization\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "data_metadata = {\"description\": desc, \"commit hash\":get_git_head_short_hash(), \"graph_func\": generate_random_geographic_graph.__name__, \"signal_func\": generate_rd_signal_in_hyp.__name__ + \" + \" + modify_signal_to_simulate_breakdown.__name__, \"n_iter\": N_EXP, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"n_samples\": N_SAMPLES, \"min_n_nodes\": MIN_N_NODES, \"max_n_nodes\": MAX_N_NODES, \"max_n_bkps\": MAX_N_BKPS, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"diag_cov_max\": DIAG_COV_MAX, \"n_nodes\": [], 'n_bkps': [], \"breakdowns\": []}\n",
    "\n",
    "# data generation and saving\n",
    "for exp_id in range(N_EXP):\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, _ = generate_random_geographic_graph(graph_rng, min_n_nodes=MIN_N_NODES, max_n_nodes=MAX_N_NODES)\n",
    "    n_nodes = int(G.number_of_nodes())\n",
    "    data_metadata[\"n_nodes\"].append(n_nodes)\n",
    "    gt_bkps, signal = generate_rd_signal_in_hyp(G, signal_rng, n_bkps_max=MAX_N_BKPS, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, diag_cov_max=DIAG_COV_MAX)\n",
    "    signal, breakdowns = modify_signal_to_simulate_breakdown(signal, signal_rng, n_nodes//10)\n",
    "    data_metadata[\"n_bkps\"].append(len(gt_bkps)-1)\n",
    "    data_metadata[\"breakdowns\"].append(breakdowns)\n",
    "    save_data(G, signal, gt_bkps, subdir)\n",
    "\n",
    "# storing metadata\n",
    "data_metadata = turn_all_list_of_dict_into_str(data_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"metadata.json\", data_metadata, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "DATE = None\n",
    "if DATE :\n",
    "    date = \"2024-06-18_17-22-19\"\n",
    "    data_dir = \"data/synthetic_data/exp_B_totally_out_of_hypothesis/B_1/\" + date\n",
    "\n",
    "################################################################\n",
    "results_dir = \"results/synthetic/C_censor_breakdown_in_hypothesis/C_2/\" + NAME\n",
    "data_dir = data_dir\n",
    "################################################################\n",
    "\n",
    "data_stats = open_json(f\"{data_dir}/metadata.json\")\n",
    "data_stats[\"bkps\"] = {}\n",
    "data_stats[\"data_path\"] = data_dir\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "normal_results = {}\n",
    "\n",
    "subfold_list = [int(subdir) for subdir in os.listdir(data_dir) if '.' not in subdir]\n",
    "subfold_list.sort()\n",
    "for exp_id in tqdm(subfold_list, desc='Running experiment...'):\n",
    "\n",
    "    exp_id = str(exp_id) # just for compatibility\n",
    "\n",
    "    # loading data\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, signal, gt_bkps = read_data(subdir)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes(), hyp=data_stats[\"bkps_gap_hyp\"])\n",
    "    data_stats[\"bkps\"][exp_id] = gt_bkps\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "\n",
    "# storing\n",
    "create_parent_and_dump_json(results_dir, \"data_stats.json\", turn_all_list_of_dict_into_str(data_stats), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 2\n",
    "\n",
    "#############################################################\n",
    "pred_dir = results_dir\n",
    "#############################################################\n",
    "\n",
    "# initialization\n",
    "data_stats = open_json(f\"{results_dir}/data_stats.json\")\n",
    "statio_pred_dic = open_json(f\"{results_dir}/statio_pred.json\")\n",
    "normal_pred_dic = open_json(f\"{results_dir}/normal_pred.json\")\n",
    "gt_bkps_dic = data_stats.pop('bkps')\n",
    "\n",
    "# output formatting\n",
    "metrics_dic = {}\n",
    "metrics_dic[\"pred_path\"] = results_dir\n",
    "metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in gt_bkps_dic.keys():\n",
    "    # compute metrics\n",
    "    normal_pred_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "    statio_pred_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "    gt_bkps = turn_str_of_list_into_list_of_int(gt_bkps_dic[exp_id])\n",
    "    compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "    compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "    # add time values\n",
    "    normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "    statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "full_results = compute_and_add_stat_on_metrics(full_results)\n",
    "full_results = turn_all_list_of_dict_into_str(full_results)\n",
    "create_parent_and_dump_json(pred_dir, 'metrics.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Robustness with respect to (spatially and temporaly) independent additive white noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_diagonal_white_noise(signal_rng:np.random.Generator, signal, sigma):\n",
    "    n_samples , n_dims = signal.shape\n",
    "    cov_mat = sigma * np.eye(n_dims)\n",
    "    white_noise = signal_rng.multivariate_normal(np.zeros(n_dims), cov_mat, size=n_samples)\n",
    "    return signal + white_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G, _ = generate_random_geographic_graph(graph_rng, min_n_nodes=10, max_n_nodes=30)\n",
    "bkps, s = generate_rd_signal_in_hyp(G, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "s_noise = add_diagonal_white_noise(signal_rng, s, sigma=3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,2))\n",
    "for i in range(5):\n",
    "    axes[0].plot(10*i+s[:, i])\n",
    "for i in range(5):\n",
    "    axes[1].plot(10*i+s_noise[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 10\n",
    "NX_GRAPH_SEED = 1\n",
    "GRAPH_SEED = 2\n",
    "SIGNAL_SEED = 3\n",
    "MIN_N_NODES = 10\n",
    "MAX_N_NODES = 25\n",
    "DIAG_COV_MAX = 1\n",
    "N_SAMPLES = 1000\n",
    "MAX_N_BKPS = 10\n",
    "SIGNAL_TO_NOISE_RATIO = 10\n",
    "BKPS_GAP_CONSTRAINT: seg_length = \"minimal\" \n",
    "\n",
    "###################################################################\n",
    "NAME = \"min_gap_cons_10_snr\"\n",
    "data_dir = \"data/synthetic_data/exp_D_in_hypo_diag_white_noise/D_2/\" + NAME\n",
    "desc = ['Minimal gap constraint, using geographical graphs, for testing with SNR equal 10 over only 10 samples.', 'The signal is generated withing the model hypothesis but we add temporally and spatially independent white noise, meaning that the white noise follows multivariate gaussian distribution with scalar covariance matrix', 'Purpose: check if the cost functions are sensitive to additive white noise with scalar matrix covariance (none of the nodes are linked).']\n",
    "## FUNCTIONS USED FOR GRAPH GENERATION\n",
    "###################################################################\n",
    "\n",
    "# initialization\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "data_metadata = {\"description\": desc, \"commit hash\":get_git_head_short_hash(), \"graph_func\": generate_random_geographic_graph.__name__, \"signal_func\": generate_rd_signal_in_hyp.__name__ + \" + \" + add_diagonal_white_noise.__name__, \"n_iter\": N_EXP, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": NX_GRAPH_SEED, \"signal_seed\": SIGNAL_SEED, \"n_samples\": N_SAMPLES, \"min_n_nodes\": MIN_N_NODES, \"max_n_nodes\": MAX_N_NODES, \"max_n_bkps\": MAX_N_BKPS, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"diag_cov_max\": DIAG_COV_MAX, \"signal_to_noise_ratio\": SIGNAL_TO_NOISE_RATIO, \"n_nodes\": [], 'n_bkps': []}\n",
    "\n",
    "# data generation and saving\n",
    "for exp_id in range(N_EXP):\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, _ = generate_random_geographic_graph(graph_rng, min_n_nodes=MIN_N_NODES, max_n_nodes=MAX_N_NODES)\n",
    "    n_nodes = int(G.number_of_nodes())\n",
    "    data_metadata[\"n_nodes\"].append(n_nodes)\n",
    "    gt_bkps, signal = generate_rd_signal_in_hyp(G, signal_rng, n_bkps_max=MAX_N_BKPS, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, diag_cov_max=DIAG_COV_MAX)\n",
    "    signal = add_diagonal_white_noise(signal_rng, signal, sigma=DIAG_COV_MAX/SIGNAL_TO_NOISE_RATIO)\n",
    "    data_metadata[\"n_bkps\"].append(len(gt_bkps)-1)\n",
    "    save_data(G, signal, gt_bkps, subdir)\n",
    "\n",
    "# storing metadata\n",
    "data_metadata = turn_all_list_of_dict_into_str(data_metadata)\n",
    "create_parent_and_dump_json(data_dir, \"metadata.json\", data_metadata, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "DATE = None\n",
    "if DATE :\n",
    "    date = \"2024-06-18_17-22-19\"\n",
    "    data_dir = \"data/synthetic_data/exp_B_totally_out_of_hypothesis/B_1/\" + date\n",
    "\n",
    "################################################################\n",
    "results_dir = \"results/synthetic/D_in_hypo_diag_white_noise/D_2/\" + NAME\n",
    "data_dir = data_dir\n",
    "################################################################\n",
    "\n",
    "data_stats = open_json(f\"{data_dir}/metadata.json\")\n",
    "data_stats[\"bkps\"] = {}\n",
    "data_stats[\"data_path\"] = data_dir\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "normal_results = {}\n",
    "\n",
    "subfold_list = [int(subdir) for subdir in os.listdir(data_dir) if '.' not in subdir]\n",
    "subfold_list.sort()\n",
    "for exp_id in tqdm(subfold_list, desc='Running experiment...'):\n",
    "\n",
    "    exp_id = str(exp_id) # just for compatibility\n",
    "\n",
    "    # loading data\n",
    "    subdir = f\"{data_dir}/{exp_id}\"\n",
    "    G, signal, gt_bkps = read_data(subdir)\n",
    "    min_size = get_min_size_for_hyp(G.number_of_nodes(), hyp=data_stats[\"bkps_gap_hyp\"])\n",
    "    data_stats[\"bkps\"][exp_id] = gt_bkps\n",
    "\n",
    "    # prediction with standard normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "\n",
    "    # prediction with stationary normal cost\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "\n",
    "# storing\n",
    "create_parent_and_dump_json(results_dir, \"data_stats.json\", turn_all_list_of_dict_into_str(data_stats), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"statio_pred.json\", turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "create_parent_and_dump_json(results_dir, \"normal_pred.json\", turn_all_list_of_dict_into_str(normal_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 2\n",
    "\n",
    "#############################################################\n",
    "pred_dir = results_dir\n",
    "#############################################################\n",
    "\n",
    "# initialization\n",
    "data_stats = open_json(f\"{results_dir}/data_stats.json\")\n",
    "statio_pred_dic = open_json(f\"{results_dir}/statio_pred.json\")\n",
    "normal_pred_dic = open_json(f\"{results_dir}/normal_pred.json\")\n",
    "gt_bkps_dic = data_stats.pop('bkps')\n",
    "\n",
    "# output formatting\n",
    "metrics_dic = {}\n",
    "metrics_dic[\"pred_path\"] = results_dir\n",
    "metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "for exp_id in gt_bkps_dic.keys():\n",
    "    # compute metrics\n",
    "    normal_pred_bkps = turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "    statio_pred_bkps = turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "    gt_bkps = turn_str_of_list_into_list_of_int(gt_bkps_dic[exp_id])\n",
    "    compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "    compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "    # add time values\n",
    "    normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "    statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "# results post-precessing and saving\n",
    "full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "full_results = compute_and_add_stat_on_metrics(full_results)\n",
    "full_results = turn_all_list_of_dict_into_str(full_results)\n",
    "create_parent_and_dump_json(pred_dir, 'metrics.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Robustness with respect to (temporaly) independent additive plain white noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Robustness with respect to the graph structure: modification of the connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_graph_connectivity(G:nx.Graph, edge_prop, graph_rng:np.random.Generator):\n",
    "    # initialization\n",
    "    edges = [e for e in G.edges()]\n",
    "    n_edges = len(edges)\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    n_modif_edges = int(n_edges * edge_prop)\n",
    "    # removing some random edges\n",
    "    edge_ids_to_remove = graph_rng.integers(low=0, high=n_edges, size=n_modif_edges//2)\n",
    "    edges_to_keep = [e for i, e in enumerate(edges) if i not in edge_ids_to_remove]\n",
    "    # adding some edges\n",
    "    new_edges_to_add = []\n",
    "    while len(new_edges_to_add) < n_modif_edges//2 :\n",
    "        new_edge = tuple(graph_rng.integers(low=0, high=n_nodes, size=2))\n",
    "        if new_edge[0] != new_edge[1]:\n",
    "            if new_edge not in new_edges_to_add:\n",
    "                new_edges_to_add.append(new_edge)\n",
    "    edges_new_graph = edges_to_keep + new_edges_to_add\n",
    "    return nx.from_edgelist(edges_new_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G, coord = generate_random_geographic_graph(graph_rng, min_n_nodes=10, max_n_nodes=30)\n",
    "G_modif = modify_graph_connectivity(G, 0.15, graph_rng)\n",
    "bkps, s = generate_rd_signal_in_hyp(G_modif, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "\n",
    "original_edges = set([(min(e), max(e)) for e in G.edges()])\n",
    "new_edges = set([(min(e), max(e)) for e in G_modif.edges()])\n",
    "\n",
    "added = new_edges.difference(original_edges)\n",
    "withdrawn  = original_edges.difference(new_edges)\n",
    "same = new_edges.intersection(original_edges)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# original graph\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=same, edge_color='k', ax=axes[0])\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=withdrawn, edge_color='r', width=3, ax=axes[0])\n",
    "nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[0], edgelist = [])\n",
    "# modified graph\n",
    "nx.draw_networkx_edges(G_modif, pos=coord, edgelist=same, edge_color='k')\n",
    "nx.draw_networkx_edges(G_modif, pos=coord, edgelist=added, width=3, edge_color='g')\n",
    "nx.draw_networkx(G_modif, with_labels=False, pos=coord, node_size=50, ax=axes[1], edgelist = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"Izenman2008\">[Izenman2008]</a>\n",
    "Izenman Alan J. (2008). Introduction to Random-Matrix Theory [asc.ohio-state.edu]\n",
    "\n",
    "<a id=\"Ryan2023\">[Ryan2023]</a>\n",
    "Sean Ryan and Rebecca Killick. Detecting Changes in Covariance via Random Matrix Theory. Technometrics, 65(4):480–491, October 2023. Publisher: Taylor & Francis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
