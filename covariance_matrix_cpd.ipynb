{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance matrix change-point detection under graph stationarity assumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "\n",
    "Let $y = (y_1, \\ldots, y_t, \\ldots, y_T), y_t \\in \\mathbb{R}^{N}$ a graph signal lying on the nodes of the graph $G = (V, E, W)$, with $N =|V|$.\n",
    "\n",
    "We aim at detecting changes of the (spatial) covariance matrix $\\Sigma_t$ of the graph signals $y_t$. We assume that there exits an unknown set of change-points $\\Tau = (t_1, \\ldots, t_K) \\subset [1, T]$ with unknown cardinality such that the covariance matrix of the graph signals is constant over any segment $[t_k, t_{k+1}]$. We do the following hypothesis:\n",
    "\n",
    "1. the signals $y_t$ follow a multivariate Gaussian distribution with fixed covariance matrix over each segment and known mean $\\mu$, i.e:\n",
    "$$\\forall k \\in [1, K] ~ \\forall t \\in [t_k, t_{k+1}] \\quad y_t \\sim \\mathcal{N}(\\mu, \\Sigma_k)$$\n",
    "\n",
    "2. over each segment, the signals $y_t$ verify the second order wide-sense graph stationarity:\n",
    "$$\\forall k \\in [1, K] \\quad \\Sigma_k = U \\text{diag}(\\gamma_k)U^T $$\n",
    "\n",
    "where the matrix $U$ contains the eigenvectors of the graph combinatorial Laplacian matrix $L = D - W$ in its columns. \n",
    "\n",
    "The Graph Fourier Transform $\\tilde{y}$ of a signal $y$ is defined by $\\tilde{y} = U^T y $.\n",
    "\n",
    "\n",
    "Based on the above assumptions, the cost derived from the maximum log-likelihood over a segment $[a, b-1]$ writes:\n",
    "\n",
    "\\begin{align*}\n",
    "    c_s(y_{a}, \\ldots, y_{b-1}) = ~ & (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + ~ \\sum_{t=a}^{b-1} \\sum_{n=1}^N \\frac{\\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2}{\\hat{\\gamma}_{a.b}^{(n)}} = ~ (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} ~ + N(b-a)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\hat{\\mu}_{T}$ is the empirical mean of the process over $[0, T]$\n",
    "- $\\hat{\\gamma}_{a..b}$ is the (empirical) biased correlogram/periodogram of the process over $[a, b-1]$: $\\hat{\\gamma}_{a..b} = \\frac{1}{(b-a)} \\sum_{t=a}^{b-1} \\left(\\tilde{y}_t^{(n)} - \\hat{\\tilde{\\mu}}_T^{(n)}\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on the minimum distance between consecutive change points (minimum segment length)\n",
    "\n",
    "We require that the different change points $(t_1, \\ldots, t_K)$ verify:\n",
    "\n",
    "$$|t_{k+1} - t_k| >= l ~ \\forall k \\in [1, K-1] $$\n",
    "\n",
    "where $l$ can be seen as the minimum admissible segment length. In this paragraph we give a meaningful lower bound of this parameter. Such lower bound is related to the computation of the cost functions over the segments $[a, b] \\subset [0, T]$, namely the graph stationary normal cost function $c_s$ described above and the standard normal cost function $c_n$:\n",
    "\n",
    "- $ c_n(y_{a}, \\ldots, y_{b-1}) = (b - a) \\log  \\left[ \\det \\left( \\hat \\Sigma_{a..b} \\right) \\right]$\n",
    "- $ c_s(y_{a}, \\ldots, y_{b-1}) = (b - a) \\sum_{n=1}^N \\log \\hat{\\gamma}_{a.b}^{(n)} $\n",
    "\n",
    "Based on the formula of the spectrogram $\\hat{\\gamma}_{a.b}$ given in the introduction, there is no numerical constraints for the feasibility of the computation. However, the $\\log [ \\det ( \\cdot ) ]$ function used in the formula for $c_n$ should be applied to invertible matrices $\\Sigma_{a..b}$ only. Therefore, we should focus on the conditions under which the matrix:\n",
    "\n",
    "$$ \\hat \\Sigma_{a..b} = \\frac{1}{b-a} \\sum_{t=a}^{b-1} (y_t - \\mu_T) (y_t - \\mu_T)^T \\quad \\text{ with } y_t \\sim \\mathcal{N}(\\mu, \\Sigma)  $$\n",
    "\n",
    "is invertible. Actually, such conditions have already been clearly stated in different works from Random Matrix Theory (RMT). For instance, it is shown in [[Izenman2008](#Izenman2008)] that $n \\hat \\Sigma_{a..b} \\sim \\mathcal{W}(b-a, \\Sigma)$ follows the Wishart distribution. In this framework, it is possible to study the distribution of the eigenvalues of $\\hat \\Sigma_{a..b}$ and to deduce that: \n",
    "\n",
    "$$ \\text{If } b-a > N \\text{ with } N  \\text{ the dimension of } y_t, \\text{ then } \\hat \\Sigma_{a..b} \\text{ is almost surely invertible }   $$\n",
    "\n",
    "Conversely, it is possible to show that if $ b-a < N $ (the number of observations is lower then the number of variables), the matrix $\\hat \\Sigma_{a..b}$ is nalmost surely not invertible. This can be done by considering the family the first $(N+1)$ columns of $\\hat \\Sigma_{a..b}$.\n",
    "\n",
    "Thus, the right lower-bound $l$ should be $\\boxed{l = N}$, which is consistent with statement from [[Ryan2023](#Ryan2023)].\n",
    "\n",
    "Note: with segments of length $l$, one is not guaranteed to compute good estimates of the covariance matrix, but at least such computations is almost surely admissible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data generation\n",
    "\n",
    "We design two graph generation scenarios.\n",
    "\n",
    "1. Erdős–Rényi (ER) graphs with random parameters\n",
    "\n",
    "In this scenario, we pick random number of nodes $N$ uniformly drawn in $[N_{\\min}, N_{\\max}]$, that may vary according to the experiment, and a random edge probability $p$ uniformly drawn in $[0.15, 0.5]$. The latter was chosen so that the generated graphs empirically have a realistic connectivity.\n",
    "\n",
    "2. Geographic-like graphs \n",
    "\n",
    "We pick a random number $N$ of nodes uniformly drawn in $[N_{\\min}, N_{\\max}]$. The nodes are randomly located in $[0, 1]^2$ using the uniform law $\\mathcal{U}([0, 1]^2)$. Eventually, we build the adjacency matrix of the graph by applying a threshold $\\rho$ to the distance separating the nodes. More formally, if we denote $(W_{ij})_{1 \\leq i, j \\leq N}$ the coefficients of the adjacency matrix, we explore the following two formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "    W_{ij} = \\mathbb{I} \\left( \\|p_i - p_j \\|_2 \\leq \\rho \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    W_{ij} = \\frac{\\rho}{\\|p_i - p_j \\|_2}  \\mathbb{I} \\left( \\|p_i - p_j \\|_2 \\leq \\rho \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $p_i$ denotes the 2D coordinates of node $i$. In the above formulas, the value of $\\rho$ is chosen empirically so that the resulting graphs visually exhibit connectivity patterns that are consistent to what one may expect when using a graph structure for signal analysis. More precisely, we use the following values:\n",
    "\n",
    "- for $N$ drawn in $[10, 50]$: $~$ $\\rho = 0.3$\n",
    "- for $N$ drawn in $[80, 110]$: $\\rho = 0.2$\n",
    "\n",
    "This scenario simulates 2D geographic graphs and is more likely to match realistic use-cases.\n",
    "\n",
    "\n",
    "### Signal generation\n",
    "\n",
    "Let $G$ be a graph randomly generated using one of the above scenarios. We recall that the laplacian matrix $L$ of the graph verifies $L = U \\Lambda U^T$, where the columns of $U$ are the eigenvectors of $L$. We now describe how to generate a signal $(y_t)_{1 \\leq t \\leq T}$ that verifies the hypothesis presented in the [problem formulation](#problem-formulation).\n",
    "\n",
    "We first pick an admissible number of change points $K$ depending on the signal length $T=1000$ and the minimum segment length $l = N$ by sampling $~  \\mathcal{U}([1, \\min(\\frac{T}{10}, \\frac{T}{l})])$. The change points $(t_k)_{1 \\leq k \\leq K}$ are uniformly drawn in $[l, T-l]$, by checking that a newly selected change point does not break the minimum segment length criteria. Therefore, it may happen that after a limit number of iterations, not $K$ change points were drawn, but we still use the same notations (SHOULD BE CHANGED).\n",
    "\n",
    "Finally, we apply the following formula to generate the signal $(y_t)_{1 \\leq t \\leq T}$:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\forall ~ k \\in [1, K-1] ~ \\forall  t \\in [t_k, t_{k+1}] \\quad  y_t \\sim \\mathcal{N}_N(0, \\Sigma_k) \\quad \\text{ with } \\quad \\Sigma_k = U \\text{diag}(\\gamma_k) U^T ~  \\text{ and } ~ \\gamma_k \\sim \\mathcal{U}niform_N([0, 1]) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setting v.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ruptures as rpt\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from numba import njit\n",
    "from math import floor\n",
    "from scipy.linalg import eigh\n",
    "from typing import List\n",
    "from ruptures.base import BaseCost\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import utils as my_ut\n",
    "import graph_related as my_gr\n",
    "import signal_related as my_sgn\n",
    "import result_related as my_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments description and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A. Data verifying the hypothesis of the model\n",
    "\n",
    "In this experiment, we generate data according to the hypothesis presented in the [problem formulation](#problem-formulation) and we compare our method to the cost function for standard covariance change detection in Gaussian models (that is supposed to cover our hypothesis). More precisely, we randomly generate a graph and a corresponding multivariate Gaussian signal, undergoing a (known) random number of covariance change points. The comparison between the two methods relies on the precision / recall and Hausdorff metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 3\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G, _ = my_gr.generate_random_geographic_graph_with_gauss_kernel(graph_rng, n_nodes=20, target_degree=10)\n",
    "bkps, s = my_sgn.generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_samples=1000, diag_cov_max=10, hyp='min', n_bkps_max=10)\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(mosaic='ABB', figsize=(14, 3))\n",
    "nx.draw_networkx(G, ax=axes['A'])\n",
    "for i in range(6):\n",
    "    axes['B'].plot(10*i+s[:, i])\n",
    "axes['B'].set_yticks([])\n",
    "for bkp in bkps[:-1]:\n",
    "    axes['B'].axvline(x=bkp, c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C. Data verifying the hypothesis of the models, with node dropping to simulate breakdowns\n",
    "\n",
    "In what follows, we work with signals verifying the two hypothesis from the [the problem formulation](#problem-formulation). Additionally, we will randomly select a (very) small number of nodes and simulate the breakdown of the corresponding sensors by setting the value of the signal lying on this node to 0 for a random time length. \n",
    "\n",
    "More formally, let denote $\\eta_{max}$ the hyper-parameter corresponding to the maximal proportion of nodes that undergo a breakdown. We denote $N^{max}_{broken} = \\lfloor \\eta_{max} * N \\rfloor$. For each signal $(y_t)_{1 \\leq t \\leq T} \\in \\mathbb{R}^{T \\times N}$ , we draw $N_{broken}$ number of nodes undergoing a breakdown in $ ~  \\mathcal{U}niform([0, N^{max}_{broken}])$. Then, for a node $i$ undergoing a breakdown we apply:\n",
    "\n",
    "\\begin{equation}\n",
    "    t_{start} ~ \\sim ~ \\mathcal{U}niform([0, T-1]), ~ t_{end} ~ \\sim ~ \\mathcal{U}niform([t_{start}, T]) \\qquad \\forall ~  t \\in [t_{start}, t_{end}] ~ y_t^i = 0\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, by increasing the value of $\\eta_{max}$ we evaluate the robustness of our cost function with respect to brutal, isolated and uncorrelated mean changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G, _ = my_gr.generate_random_er_graphs_fixed_nodes_nb(graph_rng, nx_graph_seed, n_nodes=30, target_deg=10, bandwidth_coef=0.4)\n",
    "bkps, s = my_sgn.generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "s, breakdowns = my_sgn.modify_signal_to_simulate_breakdown(s, signal_rng, n_breakdown_max=G.number_of_nodes()//10)\n",
    "\n",
    "print(\"The generated breakdowns are:\", breakdowns)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,3))\n",
    "for i in range(5):\n",
    "    ax.plot(10*i+s[:, i])\n",
    "for i, node_id in enumerate(breakdowns.keys()):\n",
    "    ax.plot(10*(i+5)+s[:, node_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D. Robustness with respect to (spatially and temporaly) independent additive white noise\n",
    "\n",
    "In this experiment, we keep using signals that verify our two hypothesis. Though we add a temporally independent white noise with scalar covariance matrix to such signal. More formally, we apply the change point detection algorithm to the signal $(y'_t)_{1 \\leq t \\leq T}$ defined by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\forall ~  t \\in [0, T] \\quad y'_t = y_t + e_t \\quad \\text{ with } \\quad e_t \\sim \\mathcal{N}_N(0, \\sigma)\n",
    "\\end{equation}\n",
    "\n",
    "We evaluate the performance of our cost function against increasing value of $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed)\n",
    "\n",
    "G, _ = my_gr.generate_random_geographic_graph_with_gauss_kernel(graph_rng, n_nodes=20, target_degree=10)\n",
    "bkps, s = my_sgn.generate_rd_signal_in_hyp_with_max_tries(G, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "s_noise = my_sgn.add_diagonal_white_noise(signal_rng, s, sigma=3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,2))\n",
    "for i in range(5):\n",
    "    axes[0].plot(10*i+s[:, i])\n",
    "for i in range(5):\n",
    "    axes[1].plot(10*i+s_noise[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F. Robustness with respect to the graph structure: modification of the connectivity\n",
    "\n",
    "In this experiment, we do not generate the signal $(y_t)_{1 \\leq t \\leq T}$ with the graph $G$ that is used to compute the cost function. Instead, we rather utilize the laplacian matrix $L_{noisy}$ of a noisy version $G_{noisy}$ of the original graph $G$.\n",
    "\n",
    "Let denote $M = |E|$ the number of edges in $G$ and $\\eta_{edge}$ the proportion of edges that we modify. We randomly remove $M_{remove} = \\lfloor M * \\eta_{edge} / 2 \\rfloor$ from the set E and we randomly add $M_{add} = \\lfloor M * \\eta_{edge} / 2 \\rfloor$ to $E$. In both cases, we select the edges randomly, but we always check that the resulting noisy graph $G_{noisy}$ still has the same number of nodes as $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed+3)\n",
    "\n",
    "G, coord = my_gr.generate_random_geographic_graph_with_gauss_kernel(graph_rng, n_nodes=20, target_degree=10)\n",
    "G_modif = my_gr.modify_graph_connectivity_from_binary_adj_mat_2(G, 0.2, graph_rng)\n",
    "bkps, s = my_sgn.generate_rd_signal_in_hyp_with_max_tries(G_modif, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "\n",
    "original_edges = set([(min(e), max(e)) for e in G.edges()])\n",
    "new_edges = set([(min(e), max(e)) for e in G_modif.edges()])\n",
    "\n",
    "\n",
    "added = new_edges.difference(original_edges)\n",
    "withdrawn  = original_edges.difference(new_edges)\n",
    "same = new_edges.intersection(original_edges)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# original graph\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=same, edge_color='k', ax=axes[0])\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=withdrawn, edge_color='r', width=3, ax=axes[0])\n",
    "nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[0], edgelist = [])\n",
    "# modified graph\n",
    "nx.draw_networkx_edges(G_modif, pos=coord, edgelist=same, edge_color='k', ax=axes[1])\n",
    "nx.draw_networkx_edges(G_modif, pos=coord, edgelist=added, width=3, edge_color='g', ax=axes[1])\n",
    "nx.draw_networkx(G_modif, with_labels=False, pos=coord, node_size=50, ax=axes[1], edgelist = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph_seed = 1\n",
    "graph_seed = 2\n",
    "signal_seed = 5\n",
    "\n",
    "signal_rng = np.random.default_rng(seed=signal_seed)\n",
    "graph_rng = np.random.default_rng(seed=graph_seed+3)\n",
    "\n",
    "# G, coord = generate_random_weighted_geographic_graph(graph_rng, min_n_nodes=19, max_n_nodes=20, dist_threshold=0.5)\n",
    "# # G_modif = modify_graph_connectivity(G, 0.05, graph_rng)\n",
    "# G_modif = modify_graph_connectivity_from_binary_adj_mat_2(G, 1, graph_rng)\n",
    "# bkps, s = generate_rd_signal_in_hyp_with_max_tries(G_modif, signal_rng, n_bkps_max=10, n_samples=G.number_of_nodes()**2, hyp='minimal', diag_cov_max=1)\n",
    "\n",
    "exp_id = 1\n",
    "\n",
    "ori_path = \"data_1/graphs/clean_ER_with_bandwidth/ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "adj_mat = np.load(f\"{ori_path}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "G = nx.from_numpy_array(adj_mat)\n",
    "coord = nx.spring_layout(G, seed=0)\n",
    "modif_path = \"data_1/graphs/ER_with_bd_edge_changed/ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.2\"\n",
    "modif_adj_mat = np.load(f\"{modif_path}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "modif_G = nx.from_numpy_array(modif_adj_mat)\n",
    "\n",
    "\n",
    "original_edges = set([(min(e), max(e)) for e in G.edges()])\n",
    "new_edges = set([(min(e), max(e)) for e in modif_G.edges()])\n",
    "\n",
    "\n",
    "added = new_edges.difference(original_edges)\n",
    "withdrawn  = original_edges.difference(new_edges)\n",
    "same = new_edges.intersection(original_edges)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# original graph\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=same, edge_color='k', ax=axes[0])\n",
    "nx.draw_networkx_edges(G, pos=coord, edgelist=withdrawn, edge_color='r', width=3, ax=axes[0])\n",
    "nx.draw_networkx(G, with_labels=False, pos=coord, node_size=50, ax=axes[0], edgelist = [])\n",
    "# modified graph\n",
    "nx.draw_networkx_edges(modif_G, pos=coord, edgelist=same, edge_color='k', ax=axes[1])\n",
    "nx.draw_networkx_edges(modif_G, pos=coord, edgelist=added, width=3, edge_color='g', ax=axes[1])\n",
    "nx.draw_networkx(modif_G, with_labels=False, pos=coord, node_size=50, ax=axes[1], edgelist = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin):\n",
    "    # initialization\n",
    "    stat_per_metric_per_cost_func = {}\n",
    "    for cost_func in cost_func_keys:\n",
    "        stat_per_metric_per_cost_func[cost_func] = {}\n",
    "        for metric in metrics_keys:\n",
    "            stat_per_metric_per_cost_func[cost_func][metric] = {\"mean\": [], \"std\": [], \"raw\": []}\n",
    "    # parsing metrics file to store results adequately\n",
    "    for folder_name in res_folder_list:\n",
    "        file_metric = my_ut.open_json(os.path.join(folder_name, f\"metrics_{preci_margin}.json\"))\n",
    "        for cost_func in cost_func_keys:\n",
    "            for metric in metrics_keys:\n",
    "                mean = file_metric[cost_func][metric][\"mean\"]\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"mean\"].append(mean)\n",
    "                std = file_metric[cost_func][metric][\"std\"]\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"std\"].append(std)\n",
    "                raw_str = file_metric[cost_func][metric][\"raw\"]\n",
    "                raw = my_ut.turn_str_of_list_into_list_of_float(raw_str)\n",
    "                stat_per_metric_per_cost_func[cost_func][metric][\"raw\"].append(raw)\n",
    "    return stat_per_metric_per_cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_raw_scatter_to_plot(x, res_dic, met_name, cost_func, ax):\n",
    "    # scatter plot based on the raw values\n",
    "    raw = np.array(res_dic[cost_func][met_name][\"raw\"])\n",
    "    raw.flatten()\n",
    "    x_scat_val = []\n",
    "    for x_id in range(len(x)):\n",
    "        x_scat_val = x_scat_val + [x[x_id]] * len(raw[x_id])\n",
    "    ax.scatter(x=x_scat_val, y=raw, alpha=0.3, c='k', s=10)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.4\n",
    "error_kw = {\"capsize\": 5, \"elinewidth\": 0.8, \"capthick\": 2}\n",
    "\n",
    "def plot_bar_and_scatter(cost_func_keys, res_dic, abscissa, colors_per_cost_func, met_name, ax, shift= 0.1, to_label=False):\n",
    "    # plot_x_coord = np.linspace(0, 1, num=(len(abscissa)+1))[:-1]\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        label = None\n",
    "        if to_label:\n",
    "            label = cost_func\n",
    "        color = colors_per_cost_func[cost_func]\n",
    "        # bar plot based on the statistics\n",
    "        x = np.asarray(abscissa) + shift*i\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.bar(x = x, height=y, yerr=y_err, label=label, width=bar_width, error_kw=error_kw, color=color)\n",
    "        # scatter plot based on the raw values\n",
    "        add_raw_scatter_to_plot(x, res_dic, met_name, cost_func, ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.02\n",
    "error_kw = {\"capsize\": 5, \"elinewidth\": 0.8, \"capthick\": 2}\n",
    "\n",
    "def plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, abscissa, plot_x_coord, colors_per_cost_func, met_name, ax, shift= 0.1, to_label=False):\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        label = None\n",
    "        if to_label:\n",
    "            label = cost_func\n",
    "        color = colors_per_cost_func[cost_func]\n",
    "        # bar plot based on the statistics\n",
    "        x = [plot_x_coord + shift*i]\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.bar(x = x, height=y, yerr=y_err, label=label, width=bar_width, error_kw=error_kw, color=color)\n",
    "        # scatter plot based on the raw values\n",
    "        add_raw_scatter_to_plot(x, res_dic, met_name, cost_func, ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_errorbar_manually(abscissa, y, yerr, marker, linestyle, linewidth, markersize, color, ax):\n",
    "    # create top and bottom lim\n",
    "    y_err_top = [y_pos + yerr_val for y_pos, yerr_val in zip(y, yerr)]\n",
    "    y_err_bottom = [y_pos - yerr_val for y_pos, yerr_val in zip(y, yerr)]\n",
    "    # add the top and bottom markers\n",
    "    ax.scatter(abscissa, y_err_top, s=markersize, c=color, marker=marker)\n",
    "    ax.scatter(abscissa, y_err_bottom, s=markersize, c=color, marker=marker)\n",
    "    # add the vertical lines\n",
    "    ax.vlines(x=abscissa, ymin=y_err_bottom, ymax=y_err_top, color=color, linewidth=linewidth, linestyle=linestyle)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_errorbar(abscissa_pos, cost_func_keys, color, markers, linestyles, met_name, res_dic, ax):\n",
    "    for i, cost_func in enumerate(cost_func_keys):\n",
    "        y = res_dic[cost_func][met_name][\"mean\"]\n",
    "        y_err = res_dic[cost_func][met_name][\"std\"]\n",
    "        ax.plot(abscissa_pos, y, c=color, marker=markers[i], markersize=7, linewidth=2, linestyle=linestyles[i])\n",
    "        add_errorbar_manually(abscissa_pos, y, y_err, marker=markers[i], linestyle=linestyles[i], linewidth=1, markersize=20, color=color, ax=ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostGraphStatioNormal(BaseCost):\n",
    "\n",
    "    \"\"\"\n",
    "    DISCLAIMER: in the current model, the mean in supposed to be known and constant over different segments,\n",
    "    so we compute its estimate over the whole available samples.\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"graph_sationary_normal_cost\"\n",
    "\n",
    "    def __init__(self, laplacian_mat) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            laplacian_mat (array): the discrete Laplacian matrix of the graph: D - W\n",
    "            where D is the diagonal matrix diag(d_i) of the node degrees and W the adjacency matrix\n",
    "        \"\"\"\n",
    "        self.graph_laplacian_mat = laplacian_mat\n",
    "        self.signal = None\n",
    "        self.gft_square_cumsum = None\n",
    "        self.gft_mean = None\n",
    "        self.min_size = laplacian_mat.shape[0]\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, signal):\n",
    "        \"\"\"Performs pre-computations for per-segment approximation cost.\n",
    "\n",
    "        NOTE: the number of dimensions of the signal and their ordering\n",
    "        must match those of the nodes of the graph.\n",
    "        The function eigh used below returns the eigenvector corresponding to \n",
    "        the ith eigenvalue in the ith column eigvect[:, i]\n",
    "\n",
    "        Args:\n",
    "            signal (array): of shape [n_samples, n_dim].\n",
    "        \"\"\"\n",
    "        self.signal = signal\n",
    "        # computation of the GFSS\n",
    "        _, eigvects = eigh(self.graph_laplacian_mat)\n",
    "        gft =  signal @ eigvects # equals signal.dot(eigvects) = eigvects.T.dot(signal.T).T\n",
    "        self.gft_mean = np.mean(gft, axis=0)\n",
    "        # computation of the per-segment cost utils\n",
    "        self.gft_square_cumsum = np.concatenate([np.zeros((1, signal.shape[1])), np.cumsum((gft - self.gft_mean[None, :])**2, axis=0)], axis=0)\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "\n",
    "        Returns:\n",
    "            float: segment cost\n",
    "        \"\"\"\n",
    "        if end - start < self.min_size:\n",
    "            raise ValueError(f'end - start shoud be higher than {self.min_size}')\n",
    "        sub_square_sum = self.gft_square_cumsum[end] - self.gft_square_cumsum[start]\n",
    "        return (end  - start) * np.sum(np.log(sub_square_sum / (end - start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import GraphicalLasso, log_likelihood\n",
    "\n",
    "class CostGraphLasso(BaseCost):\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"graph_lasso_mle_cost\"\n",
    "\n",
    "    def __init__(self, alpha, add_small_diag=True):\n",
    "        \"\"\"Initialize the object.\n",
    "\n",
    "        Args:\n",
    "            add_small_diag (bool, optional): For signals with truly constant\n",
    "                segments, the covariance matrix is badly conditioned, so we add\n",
    "                a small diagonal matrix. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.signal = None\n",
    "        self.min_size = 2\n",
    "        self.n_samples = None\n",
    "        self.alpha = alpha\n",
    "        self.add_small_diag = add_small_diag\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, signal) :\n",
    "        \"\"\"Set parameters of the instance.\n",
    "        Args:\n",
    "            signal (array): signal of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        if signal.ndim == 1:\n",
    "            self.signal = signal.reshape(-1, 1)\n",
    "        else:\n",
    "            self.signal = signal\n",
    "        self.n_samples, self.n_dims = self.signal.shape\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "\n",
    "        Returns:\n",
    "            float: segment cost\n",
    "        \"\"\"\n",
    "        sub_signal = self.signal[start:end, :]\n",
    "        emp_cov_mat= np.cov(sub_signal.T)\n",
    "        gl_estimator = GraphicalLasso(alpha=self.alpha, assume_centered=True, covariance='precomputed').fit(emp_cov_mat)\n",
    "        return log_likelihood(emp_cov_mat, gl_estimator.get_precision())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_station_normal_cost(signal, graph_laplacian_mat):\n",
    "    '''signal (array): of shape [n_samples, n_dim]'''\n",
    "    # computation of the graph fourier transform\n",
    "    _, eigvects = eigh(graph_laplacian_mat)\n",
    "    gft =  signal @ eigvects # equals signal.dot(eigvects) = eigvects.T.dot(signal.T).T\n",
    "    gft_mean = np.mean(gft, axis=0)\n",
    "    # computation of the per-segment cost utils\n",
    "    gft_square_cumsum = np.concatenate([np.zeros((1, signal.shape[1])), np.cumsum((gft - gft_mean[None, :])**2, axis=0)], axis=0)\n",
    "    return gft_square_cumsum.astype(np.float64)\n",
    "\n",
    "@njit\n",
    "def numba_statio_cost_func(start, end, gft_square_cumsum):\n",
    "    '''\n",
    "    Computes the cost over signal[start:end, :] where end is excluded\n",
    "\n",
    "    gft_square_cumsum (array): of shape [n_samples + 1, n_dim] \n",
    "    '''\n",
    "    sub_square_sum = gft_square_cumsum[end, :] - gft_square_cumsum[start, :]\n",
    "    return np.float64(end  - start) * np.sum(np.log(sub_square_sum / (end - start)), dtype=np.float64)\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_statio_cost(n_bkps:int, min_size:int, data: np.ndarray):\n",
    "    n_samples = data.shape[0]\n",
    "    # if no bkp to find\n",
    "    if n_bkps == 0:\n",
    "        return np.array([1000], dtype=np.int64)\n",
    "    # full partitions costs\n",
    "    full_part_cost = np.inf * np.ones((n_bkps, n_samples, n_samples), dtype=np.float64)\n",
    "    # compute the segment cost with no bpk, for admissible segment only\n",
    "    for start in range(0, n_samples-min_size):\n",
    "        # until n_samples + 1 because the call to cost_function(start, end, data) computes over [y_0, ... y_{n-1}] (remember data.shape[0] = n_samples + 1)\n",
    "        for end in range(start+min_size, n_samples):  \n",
    "            full_part_cost[0, start, end] = numba_statio_cost_func(start, end, data)\n",
    "    # compute the cost of the possible higher order partitions \n",
    "    for bkp_order in range(1, n_bkps):\n",
    "        min_multi_seg_length = (bkp_order + 1) * min_size\n",
    "        for start in range(0, n_samples-min_multi_seg_length):\n",
    "            for end in range(start + min_multi_seg_length, n_samples):\n",
    "                min_size_left_seg = min_multi_seg_length - min_size\n",
    "                full_part_cost[bkp_order, start, end] = np.min(full_part_cost[bkp_order-1, start, start+min_size_left_seg:end-min_size+1] + full_part_cost[0, start+min_size_left_seg:end-min_size+1, end])\n",
    "    # successively pick the bkps from the right-end of the whole signal\n",
    "    bkps = np.int64(n_samples-1) * np.ones(n_bkps+1, dtype=np.int64)\n",
    "    for bkp_id in range(n_bkps, 0, -1):\n",
    "        min_multi_seg_length = np.int64(bkp_id * min_size) \n",
    "        bkp_right = bkps[bkp_id]\n",
    "        bkp_left = min_multi_seg_length + np.argmin(full_part_cost[bkp_id-1, 0, min_multi_seg_length:bkp_right-min_size+1] + full_part_cost[0, min_multi_seg_length:bkp_right-min_size+1, bkp_right])\n",
    "        bkps[bkp_id-1] = bkp_left\n",
    "    return bkps\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_statio_cost_2_optim(n_bkps:int, min_size:int, data: np.ndarray):\n",
    "    # path_mat[n, K] avec n --> [y_0, ... y_{n-1}], K : n_bkps\n",
    "\n",
    "    # initialization \n",
    "    n_samples = data.shape[0] - 1\n",
    "    path_mat = np.empty((n_samples+1, n_bkps+1), dtype=np.int32)\n",
    "    path_mat[:, 0] = 0\n",
    "    path_mat[0, :] = -1\n",
    "    sum_of_cost_mat = np.full((n_samples+1, n_bkps+1),  fill_value=np.inf, dtype=np.float64)\n",
    "    sum_of_cost_mat[0, :] = 0\n",
    "\n",
    "    # forward computation\n",
    "    for end in range(min_size, n_samples+1):\n",
    "        sum_of_cost_mat[end, 0] = numba_statio_cost_func(0, end, data)\n",
    "        # consistent because our cost functions compute the costs over [start, end[\n",
    "        max_admissible_n_bkp = floor(end/min_size) - 1\n",
    "        for k_bkps in range(1, min(max_admissible_n_bkp+1, n_bkps+1)):\n",
    "            soc_optim = np.inf\n",
    "            soc_argmin = -1\n",
    "            for mid in range(min_size*k_bkps, end - min_size + 1): \n",
    "                soc = sum_of_cost_mat[mid, k_bkps-1] + numba_statio_cost_func(mid, end, data)\n",
    "                if soc < soc_optim:\n",
    "                    soc_argmin = mid\n",
    "                    soc_optim = soc\n",
    "            sum_of_cost_mat[end, k_bkps] = soc_optim\n",
    "            path_mat[end, k_bkps] = soc_argmin\n",
    "\n",
    "    # backtracking\n",
    "    bkps = np.full((n_bkps+1), fill_value=n_samples)\n",
    "    for k_bkps in range(n_bkps, 0, -1):\n",
    "        bkps[k_bkps-1] = path_mat[bkps[k_bkps], k_bkps]\n",
    "    \n",
    "    return bkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import slogdet\n",
    "\n",
    "@njit\n",
    "def standard_normal_cost_func(start, end, signal):\n",
    "    '''signal (array): of shape [n_samples, n_dim]'''\n",
    "    sub = signal[start:end, :]\n",
    "    cov = np.cov(sub.T)\n",
    "    cov += 1e-6 * np.eye(signal.shape[1])\n",
    "    _, val = slogdet(cov)\n",
    "    return np.float64(val * (end - start))\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_mle_standard_cost(n_bkps:int, min_size:int, signal: np.ndarray):\n",
    "    n_samples = signal.shape[0]\n",
    "    # if no bkp to find\n",
    "    if n_bkps == 0:\n",
    "        return np.array([1000], dtype=np.int64)\n",
    "    # full partitions costs\n",
    "    full_part_cost = np.inf * np.ones((n_bkps, n_samples, n_samples), dtype=np.float64)\n",
    "    # compute the segment cost with no bpk, for admissible segment only\n",
    "    for start in range(0, n_samples-min_size+1):\n",
    "        # until n_samples + 1 because the call to cost_function(start, end, data) computes over [y_0, ... y_{n-1}] \n",
    "        for end in range(start+min_size, n_samples+1):  \n",
    "            full_part_cost[0, start, end] = standard_normal_cost_func(start, end, signal)\n",
    "    # compute the cost of the possible higher order partitions \n",
    "    for bkp_order in range(1, n_bkps):\n",
    "        min_multi_seg_length = (bkp_order + 1) * min_size\n",
    "        for start in range(0, n_samples-min_multi_seg_length):\n",
    "            for end in range(start + min_multi_seg_length, n_samples):\n",
    "                min_size_left_seg = min_multi_seg_length - min_size\n",
    "                full_part_cost[bkp_order, start, end] = np.min(full_part_cost[bkp_order-1, start, start+min_size_left_seg:end-min_size+1] + full_part_cost[0, start+min_size_left_seg:end-min_size+1, end])\n",
    "    # successively pick the bkps from the right-end of the whole signal\n",
    "    bkps = np.int64(n_samples) * np.ones(n_bkps+1, dtype=np.int64)\n",
    "    for bkp_id in range(n_bkps, 0, -1):\n",
    "        min_multi_seg_length = np.int64(bkp_id * min_size) \n",
    "        bkp_right = bkps[bkp_id]\n",
    "        bkp_left = min_multi_seg_length + np.argmin(full_part_cost[bkp_id-1, 0, min_multi_seg_length:bkp_right-min_size+1] + full_part_cost[0, min_multi_seg_length:bkp_right-min_size+1, bkp_right])\n",
    "        bkps[bkp_id-1] = bkp_left\n",
    "    return bkps\n",
    "\n",
    "@njit\n",
    "def numba_cpd_dynprog_mle_standard_cost_2_optim(n_bkps:int, min_size:int, signal):\n",
    "    # path_mat[n, K] avec n --> [y_0, ... y_{n-1}] (very important to understand indexing) , K : n_bkps\n",
    "    # sum_of_cost_mat[n, K]: best cost for signal until sample n with K bkps\n",
    "\n",
    "    # initialization \n",
    "    n_samples = signal.shape[0]\n",
    "    path_mat = np.empty((n_samples+1, n_bkps+1), dtype=np.int32)\n",
    "    path_mat[:, 0] = 0\n",
    "    path_mat[0, :] = -1\n",
    "    sum_of_cost_mat = np.full((n_samples+1, n_bkps+1),  fill_value=np.inf, dtype=np.float64)\n",
    "    sum_of_cost_mat[0, :] = 0\n",
    "\n",
    "    # pre-computation, to optimize jit processing\n",
    "    statio_segment_cost = np.full((n_samples+1, n_samples+1), fill_value=np.inf, dtype=np.float64)\n",
    "    for start in range(0, n_samples-min_size+1):\n",
    "        for end in range(start+min_size, n_samples+1):  \n",
    "            statio_segment_cost[start, end] = standard_normal_cost_func(start, end, signal)\n",
    "\n",
    "    # forward computation\n",
    "    for end in range(min_size, n_samples+1):\n",
    "        sum_of_cost_mat[end, 0] = statio_segment_cost[0, end]\n",
    "        # consistent because our cost functions compute the costs over [start, end[\n",
    "        max_admissible_n_bkp = floor(end/min_size) - 1\n",
    "        for k_bkps in range(1, min(max_admissible_n_bkp+1, n_bkps+1)):\n",
    "            soc_optim = np.inf\n",
    "            soc_argmin = -1\n",
    "            for mid in range(min_size*k_bkps, end - min_size + 1):\n",
    "                soc = sum_of_cost_mat[mid, k_bkps-1] + statio_segment_cost[mid, end]\n",
    "                if soc < soc_optim:\n",
    "                    soc_argmin = mid\n",
    "                    soc_optim = soc\n",
    "            sum_of_cost_mat[end, k_bkps] = soc_optim\n",
    "            path_mat[end, k_bkps] = soc_argmin\n",
    "\n",
    "    # backtracking\n",
    "    bkps = np.full((n_bkps+1), fill_value=n_samples)\n",
    "    for k_bkps in range(n_bkps, 0, -1):\n",
    "        bkps[k_bkps-1] = path_mat[bkps[k_bkps], k_bkps]\n",
    "    \n",
    "    return bkps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph and signals generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 10 # 1000\n",
    "GRAPH_SEED = 1\n",
    "N_NODES = 20\n",
    "TARGET_DEGREE = 10\n",
    "K_NEIGHBOUR = 8\n",
    "ER_BANDWIDTH = 0.4\n",
    "EDGE_PROP_TO_MODIF = 0.6\n",
    "INITIAL_GRAPH_PATH =  'data_1/test/graphs/clean_exp_geo'  # \"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "INIT_NAME = 'exp_geo_20_nodes_av_deg_10_3' # \"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "graph_rng = np.random.default_rng(seed=GRAPH_SEED)\n",
    "to_modify_graph_path = os.path.join(INITIAL_GRAPH_PATH, INIT_NAME)\n",
    "\n",
    "# logging\n",
    "NAME =  INIT_NAME  #+ '_' + f\"edge_prop_{EDGE_PROP_TO_MODIF}\"  #f\"ER_{N_NODES}_nodes_deg_{TARGET_DEGREE}_bandwidth_{ER_BANDWIDTH}_edge_prop_{EDGE_PROP_TO_MODIF}\"\n",
    "data_dir = os.path.join(INITIAL_GRAPH_PATH, NAME) #  \"data_1/graphs/ER_with_bd_edge_changed/\" + NAME\n",
    "graphs_desc = f\"Standard exp geo graphs for testing purpose before rectorization.\"\n",
    "# graphs_desc = f\"Graphs fetched from {to_modify_graph_path}. ER graphs with fixed number of nodes. The edge probability is randomly drawn based on the target degree but also using a bandwidth parameter to allow for more diversity. Otherwise, for a given number of nodes and edge probability, the generated graphs would always be the same based on the networkx implementation. The connectivity of the graphs is modified: we randomly remove half of the edges obtained when randomly selecting some with the given edge proportion, and we randomly add the same amount of edges.\"\n",
    "graph_gen_func = lambda rng : my_gr.generate_random_geographic_graph_with_gauss_kernel(rng, n_nodes=N_NODES, target_degree=TARGET_DEGREE)\n",
    "# graph_modif_func = lambda exp_id : load_modify_connec_and_store_graph(to_modify_graph_path, exp_id, EDGE_PROP_TO_MODIF, graph_rng, data_dir)\n",
    "graphs_metadata = {\"datetime\": now, \"description\": graphs_desc, \"commit hash\": my_ut.get_git_head_short_hash(), \"graph_func\": my_gr.generate_random_geographic_graph_with_gauss_kernel.__name__, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": \"index\", \"n_nodes\": N_NODES, \"target_degree\": TARGET_DEGREE} #, \"bandwidth coefficient\": ER_BANDWIDTH}\n",
    "# graphs_metadata = {\"datetime\": now, \"description\": graphs_desc, \"commit hash\": get_git_head_short_hash(), \"graph_modif_func\": load_modify_connec_and_store_graph.__name__, \"graph_seed\": GRAPH_SEED, \"nx_graph_seed\": \"index\", \"n_nodes\": N_NODES, \"target_degree\": TARGET_DEGREE, \"bandwidth coefficient\": ER_BANDWIDTH, \"modified edge proportion\": EDGE_PROP_TO_MODIF}\n",
    "\n",
    "# output formatting\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=False)\n",
    "graphs_metadata = my_ut.turn_all_list_of_dict_into_str(graphs_metadata)\n",
    "my_ut.create_parent_and_dump_json(data_dir, \"00_graphs_metadata.json\", graphs_metadata, indent=4)\n",
    "\n",
    "# graph generation\n",
    "for exp_id in range(N_EXP):\n",
    "    # graph_modif_func(exp_id)\n",
    "    G, coords = graph_gen_func(graph_rng)\n",
    "    my_ut.save_graph(G, f\"{data_dir}/{exp_id}_mat_adj.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal and bkps generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_FOLDER =  \"data_1/test/graphs/clean_exp_geo\" #\"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "GRAPH_FOLDER_NAME =  \"exp_geo_20_nodes_av_deg_10_3\" #\"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "GRAPH_PATH = os.path.join(GRAPH_FOLDER, GRAPH_FOLDER_NAME)\n",
    "SIGNAL_SEED = 3\n",
    "DIAG_COV_MAX = 1\n",
    "MIN_SEGMENT_LENGTH_COEF = 0.1\n",
    "SNR = 10\n",
    "N_SAMPLES = 1000\n",
    "BKPS_GAP_CONSTRAINT: my_ut.seg_length = \"minimal\" \n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "signal_rng = np.random.default_rng(seed=SIGNAL_SEED)\n",
    "sigma_noise = DIAG_COV_MAX / ( 10**(SNR / 10) )\n",
    "\n",
    "\n",
    "# logging\n",
    "NAME =  GRAPH_FOLDER_NAME # f\"{BKPS_GAP_CONSTRAINT}_x{MIN_SEGMENT_LENGTH_COEF}_SNR_{SNR}\" + \"_\" + GRAPH_FOLDER_NAME \n",
    "data_dir =  f\"data_1/test/signals/within_hyp/segment_length_minimal/{NAME}\" #f\"data_1/signal/within_hyp/noisy_varying_segment_length/\" + NAME\n",
    "signal_desc = \"For testing purpose before refactorization. The goal is to reproduce results from results_1/synthetic/within_hypothesis/test/exp_geo_20_nodes_av_deg_10_NUMBA_2_optim_pre_comp_TEST\"  #\"Data verifying the two hypothesis, with a given number of bkps fixed by a coefficient applied to the large segment length. We add a white noise to the observed signal.\"\n",
    "# signal_gen_func = lambda G : generate_rd_signal_in_hyp_with_fixed_min_size(G, signal_rng, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, min_size_coef=MIN_SEGMENT_LENGTH_COEF, diag_cov_max=DIAG_COV_MAX)\n",
    "signal_gen_func = lambda G : my_sgn.generate_rd_signal_in_hyp(G, signal_rng, hyp=BKPS_GAP_CONSTRAINT, n_samples=N_SAMPLES, diag_cov_max=DIAG_COV_MAX)\n",
    "# signal_modif_func = lambda s : add_diagonal_white_noise(signal_rng, s, sigma=sigma_noise)\n",
    "# signal_metadata = {\"datetime\": now, \"description\": signal_desc, \"commit hash\": get_git_head_short_hash(), \"graph_folder\": GRAPH_PATH, \"signal_seed\": SIGNAL_SEED, \"signal_gen_function\": generate_rd_signal_in_hyp.__name__, \"signal_modif_func\": add_diagonal_white_noise.__name__, \"n_samples\": N_SAMPLES, \"diag_cov_max\": DIAG_COV_MAX, \"min_size_coef\": MIN_SEGMENT_LENGTH_COEF, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"SNR\": SNR}\n",
    "signal_metadata = {\"datetime\": now, \"description\": signal_desc, \"commit hash\": my_ut.get_git_head_short_hash(), \"graph_folder\": GRAPH_PATH, \"signal_seed\": SIGNAL_SEED, \"signal_gen_function\": my_sgn.generate_rd_signal_in_hyp.__name__, \"n_samples\": N_SAMPLES, \"diag_cov_max\": DIAG_COV_MAX, \"bkps_gap_hyp\": BKPS_GAP_CONSTRAINT, \"SNR\": SNR}\n",
    "\n",
    "# output formatting\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=False)\n",
    "signal_metadata = my_ut.turn_all_list_of_dict_into_str(signal_metadata)\n",
    "my_ut.create_parent_and_dump_json(data_dir, \"00_signal_metadata.json\", signal_metadata, indent=4)\n",
    "\n",
    "# signal generation\n",
    "for exp_id in range(len(os.listdir(GRAPH_PATH)) - 1):\n",
    "    adj_mat = np.load(f\"{GRAPH_PATH}/{exp_id}_mat_adj.npy\", allow_pickle=False)\n",
    "    G = nx.from_numpy_array(adj_mat)\n",
    "    bkps, signal = signal_gen_func(G)\n",
    "    # signal = signal_modif_func(signal)\n",
    "    my_ut.save_signal_and_bkps(signal, bkps, data_dir, str(exp_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search algorithms: running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_numba_statio_normal_cost(G: nx.Graph, signal: np.ndarray, gt_bkps: List[int], statio_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    graph_lapl_mat = nx.laplacian_matrix(G).toarray().astype(np.float64)\n",
    "    ###############################################################\n",
    "    # graph_lapl_mat = np.eye(signal.shape[1])\n",
    "    ###############################################################\n",
    "    gft_square_cumsum = init_station_normal_cost(signal, graph_lapl_mat)\n",
    "    statio_bkps = numba_cpd_dynprog_statio_cost_2_optim(len(gt_bkps)-1, signal.shape[1], gft_square_cumsum)\n",
    "    statio_bkps = [int(bkp) for bkp in statio_bkps]\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "    statio_results[exp_id][\"gt\"] = gt_bkps\n",
    "    statio_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_numba_standard_mle_normal_cost(signal: np.ndarray, gt_bkps: List[int], normal_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    normal_bkps = numba_cpd_dynprog_mle_standard_cost_2_optim(len(gt_bkps) - 1, signal.shape[1], signal)\n",
    "    normal_bkps = [int(bkp) for bkp in normal_bkps]\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "    normal_results[exp_id][\"gt\"] = gt_bkps\n",
    "    normal_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statio_normal_cost(G: nx.Graph, signal: np.ndarray, gt_bkps: List[int], statio_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    statio_cost = CostGraphStatioNormal(nx.laplacian_matrix(G).toarray())\n",
    "    algo_statio = rpt.Dynp(custom_cost=statio_cost, jump=1, min_size=min_size).fit(signal)\n",
    "    statio_bkps = algo_statio.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    statio_results[exp_id] = {}\n",
    "    statio_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    statio_results[exp_id][\"pred\"] = statio_bkps\n",
    "    statio_results[exp_id][\"gt\"] = gt_bkps\n",
    "    statio_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_standard_mle_normal_cost(signal: np.ndarray, gt_bkps: List[int], normal_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        normal_cost = rpt.costs.CostNormal()\n",
    "        algo_normal = rpt.Dynp(custom_cost=normal_cost, jump=1, min_size=min_size).fit(signal)\n",
    "        normal_bkps = algo_normal.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    normal_results[exp_id] = {}\n",
    "    normal_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    normal_results[exp_id][\"pred\"] = normal_bkps\n",
    "    normal_results[exp_id][\"gt\"] = gt_bkps\n",
    "    normal_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph_lasso_mle_cost(signal: np.ndarray, gt_bkps: List[int], alpha:float, graph_lasso_results: dict):\n",
    "    # running CPD algorithm\n",
    "    t1 = time.perf_counter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        lasso_cost = CostGraphLasso(alpha=alpha)\n",
    "        algo_lasso = rpt.Dynp(custom_cost=lasso_cost, jump=1, min_size=signal.shape[1]).fit(signal)\n",
    "        lasso_bkps = algo_lasso.predict(n_bkps=len(gt_bkps)-1)\n",
    "    t2 = time.perf_counter()\n",
    "    # logging\n",
    "    graph_lasso_results[exp_id] = {}\n",
    "    graph_lasso_results[exp_id][\"time\"] = round(t2 - t1, ndigits=3)\n",
    "    graph_lasso_results[exp_id][\"pred\"] = lasso_bkps\n",
    "    graph_lasso_results[exp_id][\"gt\"] = gt_bkps\n",
    "    graph_lasso_results[exp_id][\"n_bkps\"] = len(gt_bkps)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME =  \"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "GRAPH_PATH = \"data_1/graphs/ER_with_bd_edge_changed\"\n",
    "SIGNAL_PATH = \"data_1/signal/within_hyp/noisy_varying_segment_length\"\n",
    "SIGNAL_NAME = \"large_x0.1_SNR_10\" + '_' + NAME\n",
    "MAX_ID_SUBSET = 1000\n",
    "RESULT_DIR = \"results_1/synthetic/within_hypo_graph_connec_modif/large_x0.1\"\n",
    "LASSO_ALPHA = 0.1\n",
    "EDGE_PROP_MODIF_LIST = [0.03, 0.05, 0.08, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "OTHER_GRAPH_SEED = 1\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# logging\n",
    "signal_path = os.path.join(SIGNAL_PATH, SIGNAL_NAME)\n",
    "signal_metadata = my_ut.open_json(f\"{signal_path}/00_signal_metadata.json\")\n",
    "seg_length_hyp = \"minimal\"\n",
    "graph_rng = np.random.default_rng(OTHER_GRAPH_SEED)\n",
    "\n",
    "for EDGE_PROP in EDGE_PROP_MODIF_LIST:\n",
    "    \n",
    "    GRAPH_NAME =  NAME + f\"_edge_prop_{EDGE_PROP}\" #\"exp_geo_20_nodes_av_deg_10\" #\"ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.05\" \n",
    "    graph_path = os.path.join(GRAPH_PATH, GRAPH_NAME)\n",
    "    graph_metadata = my_ut.open_json(f\"{graph_path}/00_graphs_metadata.json\")\n",
    "    \n",
    "    RESULT_NAME = f\"edge_prop_{EDGE_PROP}\"\n",
    "    final_name = SIGNAL_NAME + \"_\" + RESULT_NAME\n",
    "    results_dir = os.path.join(RESULT_DIR, final_name)\n",
    "\n",
    "    exp_desc = \"Study of the robustness with respect to noisy graph observation. More precisely, the binary connectivity of the graph is modified, and a proportion of edges is added and removed.\"\n",
    "    experiment_metadata = {\"datetime\": now, \"description\": exp_desc, \"commit hash\": my_ut.get_git_head_short_hash(), \"graph folder\": graph_path, \"graph metadata\": graph_metadata, \"signal folder\": SIGNAL_PATH + '/' + SIGNAL_NAME, \"signal metadata\": signal_metadata, \"min segment length hypothesis\": seg_length_hyp, \"max id experiment subset\": MAX_ID_SUBSET, \"edge_prop\": EDGE_PROP}\n",
    "\n",
    "    # output formatting\n",
    "    statio_results = {}\n",
    "    # normal_results = {}\n",
    "    # lasso_results = {}\n",
    "\n",
    "    # running CPD algorithms\n",
    "    for exp_id in tqdm(range(MAX_ID_SUBSET), desc='Running experiment...'):\n",
    "        exp_id = str(exp_id)\n",
    "        G, signal, gt_bkps, min_size = my_ut.load_data(graph_path, signal_path, exp_id, seg_length_hyp)\n",
    "        run_numba_statio_normal_cost(G, signal, gt_bkps, statio_results)\n",
    "        # run_numba_standard_mle_normal_cost(signal, gt_bkps, normal_results)\n",
    "        # run_graph_lasso_mle_cost(signal, gt_bkps, LASSO_ALPHA, lasso_results)\n",
    "\n",
    "    my_ut.create_parent_and_dump_json(results_dir, \"experiment_metadata.json\", my_ut.turn_all_list_of_dict_into_str(experiment_metadata), indent=4)\n",
    "    my_ut.create_parent_and_dump_json(results_dir, F\"statio_pred.json\", my_ut.turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "# my_ut.create_parent_and_dump_json(results_dir, \"normal_pred.json\", my_ut.turn_all_list_of_dict_into_str(normal_results), indent=4)\n",
    "# my_ut.create_parent_and_dump_json(results_dir, F\"lasso_pred_alpha_{LASSO_ALPHA}.json\", my_ut.turn_all_list_of_dict_into_str(lasso_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME =  \"exp_geo_20_nodes_av_deg_10_3\" #\"ER_20_nodes_deg_10_bandwidth_0.4\"\n",
    "GRAPH_NAME =  NAME #\"exp_geo_20_nodes_av_deg_10\" #\"ER_20_nodes_deg_10_bandwidth_0.4_edge_prop_0.05\" \n",
    "GRAPH_PATH =   \"data_1/test/graphs/clean_exp_geo\"  #\"data_1/graphs/clean_ER_with_bandwidth\"\n",
    "SIGNAL_PATH = \"data_1/test/signals/within_hyp/segment_length_minimal\" #\"data_1/signal/within_hyp/noisy_varying_segment_length\"\n",
    "SIGNAL_NAME =  NAME #\"large_x0.1_SNR_10\" + '_' + NAME\n",
    "MAX_ID_SUBSET = 3 #1000\n",
    "RESULT_DIR =  \"results_1/synthetic/within_hypothesis/test\" #\"results_1/synthetic/ablation_study/large_x0.1\"\n",
    "RESULT_NAME = \"test_before_refacto\"  #\"er_other_graph_same_folder_2\"\n",
    "LASSO_ALPHA = 0.1\n",
    "OTHER_GRAPH_SEED = 1\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "final_name = SIGNAL_NAME + \"_\" + RESULT_NAME\n",
    "results_dir = os.path.join(RESULT_DIR, final_name)\n",
    "\n",
    "# logging\n",
    "graph_path = os.path.join(GRAPH_PATH, GRAPH_NAME)\n",
    "signal_path = os.path.join(SIGNAL_PATH, SIGNAL_NAME)\n",
    "graph_metadata = my_ut.open_json(f\"{graph_path}/00_graphs_metadata.json\")\n",
    "signal_metadata = my_ut.open_json(f\"{signal_path}/00_signal_metadata.json\")\n",
    "seg_length_hyp = \"minimal\"\n",
    "graph_rng = np.random.default_rng(OTHER_GRAPH_SEED)\n",
    "\n",
    "\n",
    "exp_desc = \"Test before refactorization, the goal is to reproduce the results from results_1/synthetic/within_hypothesis/test/exp_geo_20_nodes_av_deg_10_NUMBA_2_optim_pre_comp_TEST\"  #\"Ablation study: another ER graph from the same folder is used to instantiate the cost function.\"\n",
    "experiment_metadata = {\"datetime\": now, \"description\": exp_desc, \"commit hash\": my_ut.get_git_head_short_hash(), \"graph folder\": graph_path, \"graph metadata\": graph_metadata, \"signal folder\": SIGNAL_PATH + '/' + SIGNAL_NAME, \"signal metadata\": signal_metadata, \"min segment length hypothesis\": seg_length_hyp, \"max id experiment subset\": MAX_ID_SUBSET}\n",
    "\n",
    "# output formatting\n",
    "statio_results = {}\n",
    "normal_results = {}\n",
    "# lasso_results = {}\n",
    "\n",
    "# running CPD algorithms\n",
    "for exp_id in tqdm(range(MAX_ID_SUBSET), desc='Running experiment...'):\n",
    "    exp_id = str(exp_id)\n",
    "    G, signal, gt_bkps, min_size = my_ut.load_data(graph_path, signal_path, exp_id, seg_length_hyp)\n",
    "    run_numba_statio_normal_cost(G, signal, gt_bkps, statio_results)\n",
    "    run_numba_standard_mle_normal_cost(signal, gt_bkps, normal_results)\n",
    "    # run_graph_lasso_mle_cost(signal, gt_bkps, LASSO_ALPHA, lasso_results)\n",
    "\n",
    "my_ut.create_parent_and_dump_json(results_dir, \"experiment_metadata.json\", my_ut.turn_all_list_of_dict_into_str(experiment_metadata), indent=4)\n",
    "my_ut.create_parent_and_dump_json(results_dir, \"statio_pred.json\", my_ut.turn_all_list_of_dict_into_str(statio_results), indent=4)\n",
    "my_ut.create_parent_and_dump_json(results_dir, \"normal_pred.json\", my_ut.turn_all_list_of_dict_into_str(normal_results), indent=4)\n",
    "# create_parent_and_dump_json(results_dir, F\"lasso_pred_alpha_{LASSO_ALPHA}.json\", turn_all_list_of_dict_into_str(lasso_results), indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECI_RECALL_MARGIN = 2\n",
    "res_folder_root = \"results_1/synthetic/within_hypothesis/test/exp_geo_20_nodes_av_deg_10_3_test_before_refacto\"\n",
    "# file_names = os.listdir(res_folder_root)\n",
    "# PRED_FOLDER = [os.path.join(res_folder_root, file_name) for file_name in file_names]\n",
    "PRED_FOLDER = [res_folder_root]\n",
    "\n",
    "for pred_dir in PRED_FOLDER:\n",
    "\n",
    "    # fetching predictions\n",
    "    data_stats = my_ut.open_json(f\"{pred_dir}/experiment_metadata.json\")\n",
    "    statio_pred_dic = my_ut.open_json(f\"{pred_dir}/statio_pred.json\")\n",
    "    normal_pred_dic = my_ut.open_json(f\"{pred_dir}/normal_pred.json\")\n",
    "    assert list(normal_pred_dic.keys()) == list(statio_pred_dic.keys())\n",
    "\n",
    "    # output formatting\n",
    "    metrics_dic = {}\n",
    "    metrics_dic[\"pred_path\"] = pred_dir\n",
    "    metrics_dic[\"hyper-parameters\"] = data_stats\n",
    "    metrics_dic[\"hyper-parameters\"][\"metrics_margin\"] = PRECI_RECALL_MARGIN\n",
    "\n",
    "    statio_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "    normal_results = {\"recall\": {'raw': []}, \"precision\": {'raw': []}, \"f1_score\": {'raw': []}, \"hausdorff\": {'raw': []}, \"time\": {\"raw\": []}}\n",
    "\n",
    "    for exp_id in statio_pred_dic.keys():\n",
    "        # compute metrics\n",
    "        statio_pred_bkps = my_ut.turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"pred\"])\n",
    "        normal_pred_bkps = my_ut.turn_str_of_list_into_list_of_int(normal_pred_dic[exp_id][\"pred\"])\n",
    "        gt_bkps = my_ut.turn_str_of_list_into_list_of_int(statio_pred_dic[exp_id][\"gt\"])\n",
    "        my_res.compute_and_update_metrics(gt_bkps, statio_pred_bkps, statio_results, PRECI_RECALL_MARGIN)\n",
    "        my_res.compute_and_update_metrics(gt_bkps, normal_pred_bkps, normal_results, PRECI_RECALL_MARGIN)\n",
    "        # add time values\n",
    "        statio_results[\"time\"][\"raw\"].append(statio_pred_dic[exp_id][\"time\"])\n",
    "        normal_results[\"time\"][\"raw\"].append(normal_pred_dic[exp_id][\"time\"])\n",
    "\n",
    "    # results post-precessing and saving\n",
    "    full_results = {\"statio normal cost\": statio_results, \"normal cost\": normal_results}\n",
    "    full_results = my_res.compute_and_add_stat_on_metrics(full_results)\n",
    "    full_results[\"metadata\"] = metrics_dic\n",
    "    full_results = my_ut.turn_all_list_of_dict_into_str(full_results)\n",
    "    my_ut.create_parent_and_dump_json(pred_dir, f'metrics_{PRECI_RECALL_MARGIN}.json', full_results, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION 1\n",
    "####################\n",
    "preci_margin = 2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "legend_elements = []\n",
    "\n",
    "\n",
    "res_fold_root = \"results_1/synthetic/within_hypothesis_noisy/varying_segment_length/large_x0.1_SNR_10ER_20_nodes_deg_10_bandwidth_0.4_80exp\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\", \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "\n",
    "cost_func_keys = [\"normal cost\"]\n",
    "res_dic = get_res_per_metric_per_cost_func([res_fold_root], cost_func_keys, metrics_keys, preci_margin)\n",
    "x_coords = [0.1]\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "legend_elements.append(Line2D([0], [0], color=colors_per_cost_func[\"normal cost\"], lw=3, label=\"A. normal mle cost\"))\n",
    "\n",
    "cost_func_keys = [\"statio normal cost\"]\n",
    "res_dic = get_res_per_metric_per_cost_func([res_fold_root], cost_func_keys, metrics_keys, preci_margin)\n",
    "x_coords = [0.2]\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "legend_elements.append(Line2D([0], [0], color=colors_per_cost_func[\"statio normal cost\"], lw=3, label=\"B. statio cost\"))\n",
    "\n",
    "\n",
    "##### INITIALIZATION 2\n",
    "####################\n",
    "\n",
    "res_folder_root = \"results_1/synthetic/ablation_study/large_x0.1\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "res_file_name_list.sort()\n",
    "res_folder_list = [os.path.join(res_folder_root, file_name) for file_name in res_file_name_list]\n",
    "cost_func_keys = [\"statio normal cost\"]\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "colors_per_file = [\"lightcoral\", \"darkorchid\", \"peru\", \"teal\"]\n",
    "x_coords_top = [0.3, 0.4, 0.5, 0.6]\n",
    "labels = [\"C. connectivity modified 0.05\", \"D. other ER graph\", \"E. exp sim geographical graphs\", \"F. diagonal in canonical basis\"]\n",
    "\n",
    "for i, file_name in enumerate(res_folder_list):\n",
    "\n",
    "    colors_per_cost_func = {\"statio normal cost\": colors_per_file[i]}\n",
    "    res_dic = get_res_per_metric_per_cost_func([file_name], cost_func_keys, metrics_keys, preci_margin)\n",
    "    x_coords = [x_coords_top[i]]\n",
    "    # precision\n",
    "    plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "    # haussdorff\n",
    "    plot_bar_and_scatter_ablation_study(cost_func_keys, res_dic, x_coords, x_coords[0], colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "    legend_elements.append(Line2D([0], [0], color=colors_per_file[i], lw=3, label=labels[i]))\n",
    "\n",
    "\n",
    "axes[0].set_title(F'F1 Score (margin = {preci_margin})')\n",
    "axes[0].set_xticks([i/10 for i in range(1, 7)], [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n",
    "axes[0].set_xlabel(\"Different settings\")\n",
    "axes[1].set_title('Haussdorff')\n",
    "axes[1].set_xticks([i/10 for i in range(1, 7)], [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n",
    "axes[1].set_xlabel(\"Different settings\")\n",
    "axes[1].set_ylim(bottom=0, top=400)\n",
    "\n",
    "fig.suptitle(\"ABLATION STUDY \\nER graphs; N_nodes=N=20; SNR=10; segment_length=0.1*N(N-1)/2; N_exp=1000\")\n",
    "fig.subplots_adjust(bottom=0.17, top=0.85)\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncols=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "####################\n",
    "PRECI_RECALL_MARGIN = 5\n",
    "pred_path = \"results_1/synthetic/within_hypothesis/varying_segment_length\"\n",
    "PRED_FOLDER = ['results_1/synthetic/within_hypothesis/varying_segment_length/minimal_x1_ER_20_nodes_deg_10_bandwidth_0.4_80_exp',\n",
    "                'results_1/synthetic/within_hypothesis/varying_segment_length/minimal_x1.5_ER_20_nodes_deg_10_bandwidth_0.4_80_exp',\n",
    "                'results_1/synthetic/within_hypothesis/varying_segment_length/minimal_x2_ER_20_nodes_deg_10_bandwidth_0.4_80_exp',\n",
    "                'results_1/synthetic/within_hypothesis/varying_segment_length/minimal_x2.5_ER_20_nodes_deg_10_bandwidth_0.4_80_exp',\n",
    "                'results_1/synthetic/within_hypothesis/varying_segment_length/minimal_x4_ER_20_nodes_deg_10_bandwidth_0.4_80_exp'\n",
    "            ]\n",
    "# file_names = os.listdir(pred_path)\n",
    "# PRED_FOLDER = [os.path.join(pred_path, file_name) for file_name in file_names]\n",
    "\n",
    "\n",
    "cost_func_keys = [\"statio normal cost\", \"normal cost\"]\n",
    "markers = ['o', \"s\"]\n",
    "linestyles = [\"dashed\", \"dotted\"]\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "abscissa_pos =  [\"x1\", \"x1.5\", \"x2\", \"x2.5\", \"x4\"]\n",
    "graph_types = [\"ER\"] # \"exp_geo\", \"KNN_geo\", \n",
    "graph_colors = ['darkorange'] #, 'dodgerblue', 'darkorchid']\n",
    "shift = 3\n",
    "\n",
    "\n",
    "##### PLOTTING\n",
    "##############\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(7*len(metrics_keys), 5)) #, layout='constrained')\n",
    "\n",
    "for i in range(len(graph_types)):\n",
    "\n",
    "    # re-arrange the folder name so they have the right position for meaningful plotting\n",
    "    graph_name = graph_types[i]\n",
    "    res_folder_list = [folder_name for folder_name in PRED_FOLDER if graph_name in folder_name]\n",
    "    print(res_folder_list)\n",
    "    res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, PRECI_RECALL_MARGIN)\n",
    "\n",
    "    x_coords = [float(x[1:]) + i*shift for x in abscissa_pos]\n",
    "    plot_scatter_errorbar(x_coords, cost_func_keys, graph_colors[i], markers, linestyles, \"f1_score\", res_dic, ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Number of nodes\")\n",
    "    axes[0].set_title(F'F1 SCORE (margin = {PRECI_RECALL_MARGIN})')\n",
    "    plot_scatter_errorbar(x_coords, cost_func_keys, graph_colors[i], markers, linestyles, \"hausdorff\", res_dic, ax=axes[1])\n",
    "    axes[1].set_xlabel(\"Number of nodes\")\n",
    "    axes[1].set_title('Haussdorff')\n",
    "    axes[1].set_ylim(bottom=0)\n",
    "\n",
    "x_tick_loc = [float(x[1:]) + 0* shift for x in abscissa_pos]\n",
    "axes[0].set_xticks(x_tick_loc, abscissa_pos)\n",
    "axes[1].set_xticks(x_tick_loc, abscissa_pos)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='k', lw=0.01, marker=markers[0], linestyle=linestyles[0], label=cost_func_keys[0]),\n",
    "    Line2D([0], [0], color='k', lw=0.01, marker=markers[1], linestyle=linestyles[1], label=cost_func_keys[1]),\n",
    "    Line2D([0], [0], color=graph_colors[0], lw=3, label=graph_types[0]),\n",
    "    # Line2D([0], [0], color=graph_colors[1], lw=3, label=graph_types[1]),\n",
    "    # Line2D([0], [0], color=graph_colors[2], lw=3, label=graph_types[2]),\n",
    "]               \n",
    "fig.suptitle(\"Comparison of cost functions with respect to the minimum segment length\")\n",
    "fig.subplots_adjust(bottom=0.17)\n",
    "fig.legend(handles=legend_elements, loc=\"lower center\", ncols=len(legend_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIALIZATION\n",
    "####################\n",
    "preci_margin = 5\n",
    "\n",
    "res_folder_root = \"results_1/synthetic/within_hypo_graph_connec_modif/large_x0.1\"\n",
    "res_file_name_list = os.listdir(res_folder_root)\n",
    "res_file_name_list.sort()\n",
    "res_folder_list = [os.path.join(res_folder_root, file_name) for file_name in res_file_name_list]\n",
    "\n",
    "abscissa_pos = range(len(res_folder_list))  #[\"x\" + str(i/10) for i in range(1, 11)]\n",
    "x_tick_labels = [file_name[61:] for file_name in res_file_name_list]\n",
    "cost_func_keys = [\"statio normal cost\"] #, \"normal cost\"]\n",
    "colors_per_cost_func = {\"statio normal cost\": \"darkorange\"} #, \"normal cost\": \"dodgerblue\"}\n",
    "metrics_keys = [\"f1_score\", \"hausdorff\"]\n",
    "\n",
    "res_dic = get_res_per_metric_per_cost_func(res_folder_list, cost_func_keys, metrics_keys, preci_margin)\n",
    "\n",
    "\n",
    "\n",
    "##### PLOTTING\n",
    "##############\n",
    "fig, axes = plt.subplots(1, (len(metrics_keys)), figsize=(8*len(metrics_keys), 6))\n",
    "\n",
    "x_coords = abscissa_pos  #[float(x[1:]) for x in abscissa_pos]\n",
    "# precision\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"f1_score\", axes[0], shift=0.03, to_label=True)\n",
    "axes[0].set_title(F'F1 SCORE (margin = {preci_margin})')\n",
    "axes[0].set_xlabel(\"Proportion of edges modified\")\n",
    "axes[0].set_xticks(x_coords, x_tick_labels)\n",
    "# haussdorff\n",
    "plot_bar_and_scatter(cost_func_keys, res_dic, x_coords, colors_per_cost_func, \"hausdorff\", axes[1], shift=0.03)\n",
    "axes[1].set_title('Haussdorff')\n",
    "axes[1].set_xlabel(\"Proportion of edges modified\")\n",
    "axes[1].set_ylim(bottom=0)\n",
    "axes[1].set_xticks(x_coords, x_tick_labels)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=list(colors_per_cost_func.values())[0], lw=3, label=list(colors_per_cost_func.keys())[0]),\n",
    "    # Line2D([0], [0], color=list(colors_per_cost_func.values())[1], lw=3, label=list(colors_per_cost_func.keys())[1]),\n",
    "    # Line2D([0], [0], color=graph_colors[1], lw=3, label=graph_types[1]),\n",
    "    # Line2D([0], [0], color=graph_colors[2], lw=3, label=graph_types[2]),\n",
    "] \n",
    "\n",
    "fig.suptitle(\"Evaluation of the robustness with respect to the quality of the graph observation \\n graphs:ER, N=nb_nodes=20, N_exp=1000, SNR=10, min segment length= 0.1 N(N-1)/2\")\n",
    "fig.subplots_adjust(bottom=0.17, top=0.85)\n",
    "fig.legend(handles=legend_elements, loc=\"lower center\", ncols=len(legend_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"Izenman2008\">[Izenman2008]</a>\n",
    "Izenman Alan J. (2008). Introduction to Random-Matrix Theory [asc.ohio-state.edu]\n",
    "\n",
    "<a id=\"Ryan2023\">[Ryan2023]</a>\n",
    "Sean Ryan and Rebecca Killick. Detecting Changes in Covariance via Random Matrix Theory. Technometrics, 65(4):480–491, October 2023. Publisher: Taylor & Francis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
